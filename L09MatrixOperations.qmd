---
jupyter: python3
---
```{python}
#| echo: false
qr_setting = None
#
import numpy as np
import matplotlib as mp
import pandas as pd
import matplotlib.pyplot as plt
import laUtilities as ut
import slideUtilities as sl
import demoUtilities as dm
import pandas as pd
from matplotlib import animation
from IPython.display import HTML
from IPython.display import Latex

```
<!--
This comment somehow suppresses the title page
-->
## Matrix Algebra

::: {.content-visible when-profile="slides"}
![](images/Arthur_Cayley.jpg){width=250}
:::

:::: {.content-hidden when-profile="slides"}
::: {.column-margin}
Many parts of this page are based on _Linear Algebra and its
Applications,_ by David C. Lay
:::
::::

::: {.content-visible when-profile="slides"}
##
:::

Today we will talk about multiplying matrices:

* How do you multiply matrices?
* What does the product of two matrices mean?
* What algebraic rules apply to matrix multiplication?
* What is the computational cost of matrix multiplication?

::: {.content-visible when-profile="slides"}
##
:::

<a title="Herbert Beraud (1845–1896) / Public domain" href="https://commons.wikimedia.org/wiki/File:Arthur_Cayley.jpg"><img width="256" alt="Arthur Cayley" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Arthur_Cayley.jpg/256px-Arthur_Cayley.jpg"></a>

Early in his life, Arthur Cayley practiced law to support his passion for mathematics. During his time as a practicing lawyer, he published over 200 papers on mathematics. Finally at the age of 42 he got a position at Cambridge University and took a big pay cut so he could become a full-time mathematician. He said he never regretted the choice. Cayley often said, “I love my subject.”

In 1855-1857 Cayley formed the theory of matrices, and came up with a way to multiply matrices. As we’ll see, it is not the most obvious thing to do, but it was quickly realized that it was the “right” way. Cayley came about this idea by first thinking about linear transformations, and how to compose linear transformations. So that’s where we’ll start.

## Composing Linear Transformations

Let's start our discussion by recalling the linear transformation we called __reflection through the origin.__

::: {.fragment}
Here is a picture of what this transformation does to a shape:

```{python}
#| echo: false
note = dm.mnote()
A = np.array(
    [[-1, 0],
     [ 0,-1]])
dm.plotSetup()
dm.plotShape(note)
dm.plotShape(A @ note,'r')
plt.annotate("",
            xy=(-0.75, -0.75), xycoords='data',
            xytext=(0.75, 0.75), textcoords='data',
            size=20, va="center", ha="center",
            arrowprops=dict(arrowstyle="simple",
                            connectionstyle="arc3,rad=0.3"),
            );
```
:::

::: {.fragment}
We determined that the matrix $C = \left[\begin{array}{rr}-1&0\\0&-1\end{array}\right]$ implements this linear transformation.  
:::

::: {.fragment}
But notice that we could have accomplished this another way:

* First reflect through the $x_1$ axis 
* Then reflect through the $x_2$ axis
:::

::: {.fragment}
```{python}
#| echo: false
A = np.array(
    [[ 1, 0],
     [ 0,-1]])
B = np.array(
    [[-1, 0],
     [ 0, 1]])
dm.plotSetup()
dm.plotShape(note)
dm.plotShape(A @ note,'g')
dm.plotShape(B @ (A @ note),'r')
plt.annotate("",
            xy=(0.75, -0.65), xycoords='data',
            xytext=(0.75, 0.75), textcoords='data',
            size=20, va="center", ha="center",
            arrowprops=dict(arrowstyle="simple",
                            connectionstyle="arc3,rad=0.3"),
            );
plt.annotate("",
            xy=(-0.75, -0.75), xycoords='data',
            xytext=(0.75, -0.85), textcoords='data',
            size=20, va="center", ha="center",
            arrowprops=dict(arrowstyle="simple",
                            connectionstyle="arc3,rad=0.3"),
            );
```
:::

::: {.fragment}
As we saw, to reflect a point through the $x_1$ axis, we multiply it by matrix $A = \left[\begin{array}{rr}1&0\\0&-1\end{array}\right]$.

Likewise, to reflect a point through the $x_2$ axis, we multiply it by matrix $B = \left[\begin{array}{rr}-1&0\\0&1\end{array}\right]$.
:::

::: {.fragment}
So, another way to reflect point ${\bf u}$ through the origin would be:

* ${\bf v} = A{\bf u}$
* Followed by ${\bf w} = B{\bf v}.$
:::

::: {.fragment}
In other words, ${\bf w} = B(A{\bf u}).$
:::

::: {.fragment}
What we are doing here is called __composing__ transformations.

In a composition of transformations, we take the output of one transformation as input for another transformation.
:::

::: {.fragment}
Now, here is the key point:

<font color=blue>It is clear that $B(A{\bf x})$ and $C{\bf x}$ are the __same__ linear transformation.</font>  

So, using $C$ we can go directly to the solution using one multiplication, rather than having to multiply twice (once for $A$ and once for $B$).
:::

::: {.content-visible when-profile="slides"}
##
:::

So a natural question is: given $A$ and $B$, could we find $C$ directly?

In other words, for any $A$ and $B$, could we find $C$ such that:

$$ A(B{\bf x}) = C{\bf x}? $$

::: {.fragment}
Let's determine how to find $C$ given $A$ and $B.$
:::

::: {.fragment}
If $A$ is $m \times n$, $B$ is $n \times p$, and ${\bf x} \in \mathbb{R}^p,$ denote the columns of $B$ by ${\bf b_1},\dots,{\bf b_p},$ and the entries in ${\bf x}$ by $x_1, \dots, x_p.$
:::

::: {.fragment}
Then:

$$ B{\bf x} = x_1{\bf b_1} + \dots + x_p {\bf b_p}. $$
:::

::: {.fragment}
and:

$$A(B{\bf x}) = A(x_1{\bf b_1} + \dots + x_p {\bf b_p})$$
:::

::: {.fragment}
Since matrix-vector multiplication is a linear transformation:

$$ = x_1A{\bf b_1} + \dots + x_pA{\bf b_p}. $$
:::

::: {.fragment}
So the vector $A(B{\bf x})$ is a linear combination of the vectors $A{\bf b_1}, \dots, A{\bf b_p},$ using the entries in ${\bf x}$ as weights.
:::

::: {.fragment}
A linear combination of vectors is the same as a matrix-vector multiplication.   In matrix terms, this linear combination is written:

$$ A(B{\bf x}) = [A{\bf b_1} \; \dots \; A{\bf b_p}] {\bf x}.$$
:::

::: {.fragment}
So this matrix <font color=blue>$[A{\bf b_1} \; \dots \; A{\bf b_p}]$</font> is what we are looking for!
:::

## Matrix Multiplication

::: {.fragment}
__Definition.__  If $A$ is an $m \times n$ matrix and $B$ is $n \times p$ matrix with columns ${\bf b_1},\dots,{\bf b_p},$ then the product $AB$ is defined as the $m \times p$ matrix whose columns are $A{\bf b_1}, \dots, A{\bf b_p}.$  That is,

$$ AB = A[{\bf b_1} \; \dots \; {\bf b_p}] = [A{\bf b_1} \; \dots \; A{\bf b_p}]. $$
:::

::: {.fragment}
This definition means that for any $A$ and $B$ for which $AB$ is defined, then if $C$ = $AB$,

$$ C{\bf x} = A(B{\bf x}). $$
:::

::: {.fragment}
That is: <font color=blue>_multiplication of matrices_ </font> corresponds to <font color=blue>_composition of linear transformations._ </font>
:::

::: {.fragment}
Note that when $C = AB$, $C{\bf x}$ is a vector _in the span of the columns of_ $A.$
:::

::: {.content-visible when-profile="slides"}
##
:::

__Example.__  Compute $AB$ where $A = \left[\begin{array}{rr}2&3\\1&-5\end{array}\right]$ and $B = \left[\begin{array}{rrr}4&3&6\\1&-2&3\end{array}\right].$


::: {.fragment}
__Solution.__ Write $B = \left[{\bf b_1}\;{\bf b_2}\;{\bf b_3}\right],$ and compute:

$$ A{\bf b_1} = \left[\begin{array}{rr}2&3\\1&-5\end{array}\right]\left[\begin{array}{r}4\\1\end{array}\right],\;\;\;
A{\bf b_2} = \left[\begin{array}{rr}2&3\\1&-5\end{array}\right]\left[\begin{array}{r}3\\-2\end{array}\right],\;\;\;
A{\bf b_3} = \left[\begin{array}{rr}2&3\\1&-5\end{array}\right]\left[\begin{array}{r}6\\3\end{array}\right],$$
:::

::: {.fragment}
$$ = \left[\begin{array}{r}11\\-1\end{array}\right]\;\;\;\left[\begin{array}{r}0\\13\end{array}\right]\;\;\;\left[\begin{array}{r}21\\-9\end{array}\right].$$
:::

::: {.fragment}
So:

$$ AB = \left[A{\bf b_1}\;A{\bf b_2}\;A{\bf b_3}\right] = \left[\begin{array}{rrr}11&0&21\\-1&13&-9\end{array}\right].$$
:::

::: {.content-visible when-profile="slides"}
##
:::

__Example.__ Verify that reflection through the $x_1$ axis followed by reflection through the $x_2$ axis is the same as reflection through the origin.

::: {.fragment}
$$\left[\begin{array}{rr}-1&0\\0&1\end{array}\right]\left[\begin{array}{rr}1&0\\0&-1\end{array}\right] = \left[\begin{array}{rr}~&~\\~&~\end{array}\right].$$
:::

::: {.fragment}
$$\left[\begin{array}{rr}-1&0\\0&1\end{array}\right]\left[\begin{array}{rr}1&0\\0&-1\end{array}\right] = \left[\begin{array}{rr}-1&0\\0&-1\end{array}\right].$$
:::

::: {.fragment}
Note that this is a valid proof because __every linear transformation of vectors is defined by its standard matrix.__
:::

::: {.content-visible when-profile="slides"}
##
:::

__Example.__  If $A$ is a $3 \times 5$ matrix, and $B$ is a $5 \times 2$ matrix, what are the sizes of $AB$ and $BA$, if they are defined?

::: {.fragment}
$$\begin{array}{cccc}A&B&=&AB\\
3\times 5&5 \times 2&& 3 \times 2\\
\left[\begin{array}{rrrrr}*&*&*&*&*\\ *&*&*&*&*\\ *&*&*&*&*\end{array}\right] & 
\left[\begin{array}{rr}*&*\\ *&*\\ *&*\\ *&*\\ *&*\end{array}\right] & 
= &
\left[\begin{array}{rr}*&*\\ *&*\\ *&*\end{array}\right]
\end{array}$$
:::

::: {.fragment}
What about $BA$?
:::

::: {.fragment}
It is <font color=blue>not defined, </font> because the number of columns of $B$ does not match the number of rows of $A$.
:::

::: {.fragment}
__Facts.__

If $A$ is $m\times n$, and $B$ is $p \times q$, then $AB$ is defined if and only if $n = p$.   If $AB$ is defined, then it is $m \times q$.

$$\begin{array}{cccc}A&B&=&AB\\
3\times \fbox{5}&\fbox{5} \times 2&& 3 \times 2\\
\end{array}$$
:::

## The Inner Product View of Matrix Multiplication

Recall that the inner product of two vectors ${\bf u}$ and ${\bf v}$ is $\sum_k u_k v_k.$   

Also recall that one way to define the matrix vector product is $(A{\bf x})_i =$ inner product of ${\bf x}$ and row $i$ of $A$.

::: {.fragment}
This immediately shows another way to think of matrix multiplication:

$(AB)_{ij} =$ inner product of row $i$ of $A$ and column $j$ of $B$

$(AB)_{ij} = \sum_k A_{ik}B_{kj}.$
:::

::: {.fragment}
__Example.__ Start with the same matrices as the last example, $A = \left[\begin{array}{rr}2&3\\1&-5\end{array}\right]$ and $B = \left[\begin{array}{rrr}4&3&6\\1&-2&3\end{array}\right].$  Compute the entry in row 1 and column 3 of $C$.
:::

::: {.fragment}
$$AB = \left[\begin{array}{rr}\fbox{2} & \fbox{3}\\1&-5\end{array}\right]
\left[\begin{array}{rrr}4&3&\fbox{6}\\1&-2&\fbox{3}\end{array}\right] = 
\left[\begin{array}{rrc}*&*&2(6)+3(3)\\ *&*&*\end{array}\right] = 
\left[\begin{array}{rrr}*&*&21\\ *&*&*\end{array}\right].$$

This agrees with the result of the last example, and we could reproduce the whole solution by repeating this for each element of the result matrix.
:::

::: {.content-visible when-profile="slides"}
##
::::: {.fragment .fade-down .r-fit-text}
Question Time!   Q9.1
:::::
:::

## Matrix Algebra

We've defined multiplication of two matrices.   What about addition of two matrices?

::: {.fragment}
This is straightfoward: if $A$ and $B$ are the same shape, we get $A + B$ by adding the corresponding elements.  (Just like adding vectors.)

That is, 

$$(A + B)_{ij} = A_{ij} + B_{ij}.$$

If $A$ and $B$ are not the same shape, $A + B$ is undefined.
:::

::: {.fragment}
Furthermore, we define scalar-matrix multiplication just as for vectors:

$$ (rA)_{ij} = r(A_{ij}).$$
:::

::: {.fragment}
So, just as we did for vectors, we can show that the standard properties of addition apply, and that scalar multiplication distributes over addition:

1. $A +  B = B + A$
2. $(A + B) + C = A + (B + C)$
3. $A + 0 = A$
4. $r(A + B) = rA + rB$
5. $(r + s)A = rA + sA$
6. $r(sA) = (rs)A$
:::

::: {.fragment}
Furthermore, we find that <font color=blue>__some__ </font> (but not all!) of the familiar properties of multiplication apply to matrix multiplication (assume that all sums and products are defined):

1. $A(BC) = (AB)C$  
    * multiplication of matrices is associative
2. $A(B+C) = AB + AC$ 
    * multiplication on the left distributes over addition
3. $(B+C)A = BA + CA$ 
    * multiplication on the right distributes over addition
4. $r(AB) = (rA)B = A(rB)$ 
    * for any scalar $r$
5. $I A = A = AI$ 
:::

::: {.fragment}
Note that property 1 means that we can write $ABC$ without bothering about parentheses. 
:::

::: {.content-visible when-profile="slides"}
##
:::

__Now, here is where things get different!__

::: {.fragment}
* In general, $AB$ is __not__ equal to $BA$.  Multiplication is <font color="blue"><B>not commutative!</B></font>

    * Consider $A = \left[\begin{array}{rr}1 & 1\\1&1\end{array}\right]$ and $B = \left[\begin{array}{rr}1 & 1\\1&2\end{array}\right].$
:::

::: {.fragment}
* In fact, even if $AB$ is defined, $BA$ may not be defined.  
:::

::: {.fragment}
* On the other hand, sometimes $A$ and $B$ __do__ commute.
    * Consider $A$ and $B$ as the reflections through the $x_1$ and $x_2$ axis. Then $AB$ and $BA$ both implement reflection through the origin (i.e., the same transformation.) So in this case $AB = BA$.
:::

::: {.fragment}
* You cannot, in general, cancel out matrices in a multiplication.   That is, <font color=blue>if $AC = AB$, it does not follow that $C = B$.  </font>
    * Consider the case where $A$ is the projection onto one of the axes.
:::

::: {.fragment}
* If $AB$ is the zero matrix, you cannot in general conclude that either $A$ or $B$ must be a zero matrix.
    * Consider $A = \left[\begin{array}{rr}1 & 0\\0&0\end{array}\right]$ and $B = \left[\begin{array}{rr}0 & 0\\0&1\end{array}\right].$
:::

::: {.fragment}
__Study and remember these rules.  You will use them!__
:::

## Powers of a Matrix

Equipped now with matrix-matrix multiplication, we can define the powers of a matrix in a straightforward way.  For an integer $k > 0$:

$$ A^k = \overbrace{A\cdots A}^k.$$

Obviously, $A$ must be a square matrix for $A^k$ to be defined.

::: {.fragment}
What should $A^0$ be?

$A^0{\bf x}$ should be the result of multiplying ${\bf x}$ with $A$ zero times.   So we define $A^0 = I$.
:::

::: {.content-visible when-profile="slides"}
##
::::: {.fragment .fade-down .r-fit-text}
Question Time!   Q9.2
:::::
:::

## The Transpose of a Matrix

Given an $m \times n$ matrix $A,$ the _transpose_ of $A$ is the matrix we get by interchanging its rows and columns.

It is denoted $A^T$.   Its shape is $n \times m$.

::: {.fragment}
For example, if:

$$
\begin{array}{ccc}
A = \left[\begin{array}{rr}a&b\\c&d\end{array}\right],&
B = \left[\begin{array}{rr}-5&2\\1&-3\\0&4\end{array}\right],&
C = \left[\begin{array}{rrrr}1&1&1&1\\-3&5&-2&7\end{array}\right]
\end{array}
$$

Then:

$$
\begin{array}{ccc}
A^T = \left[\begin{array}{rr}a&c\\b&d\end{array}\right],&
B^T = \left[\begin{array}{rrr}-5&1&0\\2&-3&4\end{array}\right],&
C^T = \left[\begin{array}{rr}1&-3\\1&5\\1&-2\\1&7\end{array}\right]
\end{array}
$$
:::

::: {.fragment}
The definition can be stated succinctly:

$$A^T_{ij} = A_{ji}.$$
:::

::: {.fragment}
__Rules for Transposes:__

1. $(A^T)^T = A$
2. $(A + B)^T = A^T + B^T$
3. For any scalar $r$, $(rA)^T = r(A^T)$
4. $(AB)^T = B^TA^T$

The first three are pretty obvious.  

The last one is a bit different. __Memorize it.__ You will use it: the transpose of a product is the product of the transposes __in reverse order__.
:::

::: {.content-visible when-profile="slides"}
##
::::: {.fragment .fade-down .r-fit-text}
Question Time!   Q9.3
:::::
:::

::: {.content-visible when-profile="slides"}
##
:::

__Question:__ For a vector in ${\bf x} \in \mathbb{R}^n$, what is ${\bf x}^T$?   

::: {.fragment}
__Answer:__ For the purposes of the definition, we treat ${\bf x}$ as a $n \times 1$ matrix.  So its transpose is an $1\times n$ matrix, i.e., a matrix with a single row.
:::

::: {.fragment}
__Question:__ For two vectors ${\bf x}$ and ${\bf y}$, what is ${\bf x}^T {\bf y}$?
:::

::: {.fragment}
__Answer:__ By the definition of matrix-vector multiplication,  ${\bf x}^T {\bf y} = \sum_{i=1}^n x_i y_i.$

That is, ${\bf x}^T {\bf y}$ is the __inner product__ of ${\bf x}$ and ${\bf y}$.  This simple construction is a very useful one to remember.
:::

## The Computational Viewpoint

::: {.fragment}
You recall in the last lecture I said that in Python/numpy:

    C = A @ B
    
was the same as:

    for i in range(k):
        C[:,k] = AxIP(A, B[:,k])
        
So now you know: `A @ B` is really _matrix multiplication_ of `A` and `B.` :)
:::

::: {.fragment}
Matrix multiplication is a mainstay of computing.  Thousands of applications rely heavily on matrix multiplication.  

Some examples include:

* Computer graphics and animation
* Google's algorithm for ranking search results
* Modeling mechanical structures such as aircraft and buildings
* Compressing and decompressing audio signals
* Weather modeling and prediction
* Modeling quantum computing

So minimizing the time required to do matrix multiplication is immensely important.
:::

::: {.content-visible when-profile="slides"}
##
:::

### Complexity

What is the computational complexity of matrix multiplication?

::: {.fragment}
For two $n \times n$ matrices, consider the definition that uses inner product:

$$ (AB)_{ij} = \sum_{k=1}^n A_{ik}B_{kj}.$$
:::

::: {.fragment}
So each element of the product $AB$ requires $n$ multiplications and $n$ additions.

There are $n^2$ elements of $AB$, so the overall computation requires 

$$2n \cdot n^2 = 2n^3$$ 

operations.
:::

::: {.fragment}
That's not particularly good news; for two matrices of size 10,000 $\times$ 10,000 (which is not particularly large in practice), this is 2 trillion operations (2 teraflops).
:::

::: {.fragment}
What is the computational complexity of matrix-vector multiplication?

We know that matrix-vector multiplication requires $n$ inner products, each of size $n$.  

So, matrix-vector multiplication requires 

$$2n^2$$

operations.
:::

::: {.content-visible when-profile="slides"}
##
:::

### Order matters!

When you look at a mathematical expression involving matrices, think __carefully__ about what it means and how you might efficiently compute it.

::: {.fragment}
__For example:__

What is the most efficient way to compute $A^2{\bf x}$?

Here are your choices:
:::

::::: {.columns}
:::: {.column width="50%"}
::: {.fragment}
First compute $A^2$, then compute $(A^2){\bf x}$
:::
::::

:::: {.column width="50%"}
::: {.fragment}
First compute $A{\bf x}$, then compute $A(A{\bf x})$
:::
::::
:::::

::::: {.columns}
:::: {.column width="50%"}
::: {.fragment}
$$2n^3 + 2n^2$$
:::
::::

:::: {.column width="50%"}
::: {.fragment}
$$2 \cdot 2n^2 = 4n^2$$
:::
::::
:::::

::: {.fragment}
Again, if we are working with a square matrix with 10,000 rows, then
:::

::::: {.columns}
:::: {.column width="50%"}
::: {.fragment}
$(A^2){\bf x}$ requires 2 Trillion flops
:::
::::

:::: {.column width="50%"}
::: {.fragment}
 $A(A{\bf x})$ requires 400 Million flops
:::
::::
:::::

::: {.fragment .ctrd}
Which would you choose? :)
:::

::::: {.columns}
:::: {.column width="50%"}
::: {.fragment}
![](images/one_trillion_a-300x217.jpg){height=250}

One Trillion Pennies
:::
::::

:::: {.column width="50%"}
::: {.fragment}
![](images/one_mill_a-295x300.jpg){height=250}

One Million Pennies
:::
::::

:::::

::: {.content-visible when-profile="slides"}
##
:::

### Parallelization

Although matrix multiplication is computationally demanding, it has a wonderful property: it is _highly parallel_.  

That is, the computation needed for each element does not require computing the other elements.  

(This is not true, for example, for Gaussian elimination; think about the role of a pivot.)

::: {.fragment}
This means that if we have multiple processors, and each has access to $A$ and $B$, the work can be divided up very cleanly.

For example, let's say you have $n$ processors.   Then each processor can independently compute one column of the result, without needing to know anything about what the other processors are doing.  

Specifically, processor $i$ can compute its column as $A{\bf b_i}$.  

In that case, since all processors are working in parallel, the elapsed time is reduced from $2n^3$ down to $2n^2.$
:::

::: {.content-visible when-profile="slides"}
##
:::

### The Importance of Libraries

The pervasive use of matrix multiplication in science and engineering means that very efficient and carefully constructed libraries have been developed for it.

Important issues for high performance include:

* how are the matrix elements actually laid out in memory?
* what is the order in which matrix elements are accessed?
* what are the architectural details of the computer you are using?  
    * memories, caches, number of processors, etc

The premier library is called __LAPACK.__

LAPACK has been developed over the past 40 years and is updated frequently to tune it for new computer hardware.  

::: {.fragment}
Python's "numpy" uses LAPACK under the hood for its matrix computations.

Hence, even though Python is an interpreted language, for doing intensive matrix computations it is very fast, just as fast as compiled code.
:::

