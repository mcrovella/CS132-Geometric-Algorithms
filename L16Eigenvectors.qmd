---
jupyter: python3
---
```{python}
#| echo: false
qr_setting = None
#
import numpy as np
import matplotlib as mp
import pandas as pd
import matplotlib.pyplot as plt
import laUtilities as ut
import slideUtilities as sl
import demoUtilities as dm
import pandas as pd
from datetime import datetime
from matplotlib import animation
from IPython.display import HTML
from IPython.display import Latex

```
<!--
This comment somehow suppresses the title page
-->
## Eigenvectors and Eigenvalues

::: {.content-visible when-profile="slides"}
::: {.ctrd}
![](images/eigshow-fig.png){width=300}
:::
:::

::: {.content-visible when-profile="slides"}
##
:::

:::: {.content-hidden when-profile="slides"}
::: {.column-margin}
Many parts of this page are based on _Linear Algebra and its
Applications,_ by David C. Lay
:::
::::

```{python}
#| echo: false
cohortMatrix = np.array([
[56, 42, 49, 39, 40, 31, 33, 46, 55, 91, 83, 129, 153, 178],
[43, 36, 27, 34, 24, 29, 39, 56, 74, 69, 111, 136, 168, 171],
[32, 24, 22, 21, 26, 25, 44, 64, 52, 77, 105, 130, 139, 175],
[25, 16, 19, 28, 24, 30, 37, 40, 49, 56, 79, 93, 90, 126]])
nyears = np.shape(cohortMatrix)[1]
# index rows by time, and columns by cohort    
Year = pd.DateOffset(years=1)
# need to fliplr because spreadsheet rows are in decreasing cohort order
datestrs=['09-2004','09-2005','09-2006','09-2007','09-2008','09-2009','09-2010','09-2011','09-2012','09-2013','09-2014','09-2015','09-2016','09-2017']
dates = [datetime.strptime(x, '%m-%Y') for x in datestrs[:nyears]]
cohorts = pd.DataFrame(np.fliplr(cohortMatrix.T),index=dates,columns=pd.Index(['U1','U2','U3','U4']))
# learning the model
b = np.array(cohorts[1:])
a = np.array(cohorts[:-1])
x, resid, rank, s = np.linalg.lstsq(a,b,rcond=None)
A = x.T
#
cohorts.sum(axis=1).plot(figsize = (6,4))
plt.ylim(ymin=0)
# plt.legend(['Model','Actual'],loc='best')
plt.title('Number of BU CS Majors',size=20);
```


::: {.fragment}
During the time period shown, the number of CS majors was growing very rapidly.  I needed to have some estimate of where the numbers might be going.

Modeling the number of students in the CS major is challenging because students enter and leave the major at various points during their undergraduate degree.
:::

::: {.fragment}
We will use a linear difference model that we will train on historical data. 

Our state variable is 

$$\mathbf{x}_t = \begin{bmatrix}x_{1,t}\\ x_{2,t}\\ x_{3,t}\\ x_{4,t}\end{bmatrix}$$

where $x_{i,t}$ is the number of students who are declared CS majors and in their $i$th year at BU, in the fall of year $t$.   

Our model is a linear dynamical system:

$$ \mathbf{x}_{t+1} = A \mathbf{x}_t.$$
:::

::: {.fragment}
Given historical data which are measurements of our state variable from past years, we can estimate $A$ by a method called _least squares_.

(We will cover least squares later in the semester.)
:::

::: {.fragment}
Here is the $A$ that we get when look at historical data:

```{python}
print(A)
```
:::

::: {.fragment}
Now, using $A$, we can predict growth into the future via $\mathbf{x}_{t+1} = A \mathbf{x}_t.$
:::

::: {.fragment}
```{python}
#| echo: false
OneYearAhead = cohorts.dot(A.T).shift(1,freq=Year)
OneYearAhead.columns=pd.Index(['U1','U2','U3','U4'])
OneYearAhead.sum(axis=1).plot(figsize = (6, 4))
cohorts.sum(axis=1).plot()
plt.ylim(ymin=0)
plt.legend(['Model','Actual'],loc='best');
```
:::

::: {.fragment}
Notice how the matrix $A$ captures the complex way that one year's student population relates to the next year's population.  

Because of this complexity, can be hard to understand what is going on just by looking at the values of $A$.

But, when we look at the plot above, it appears that there is a relatively simple kind of exponential growth going on.

Can we "extract" this exponential growth model from $A$?    How?
:::

::: {.fragment}
Today's lecture will start to develop the tools to answer such questions (and more).
:::

## Eigenvectors and Eigenvalues

::: {.fragment}
As we've seen, linear transformations (thinking geometrically) can "move" a vector to a new location.
:::

::: {.fragment}
For example, a linear transformation $A\mathbf{x}$ can do the following to $\mathbf{x}$:

* rotate $\mathbf{x}$
* reflect $\mathbf{x}$
* shear $\mathbf{x}$
* project $\mathbf{x}$
* scale $\mathbf{x}$
:::

::: {.fragment}
Of the above transformations, one is a bit different: __scaling__.
    
That is because, if a matrix $A$ scales $\mathbf{x}$, 

that transformation could also have been expressed __without__ a matrix-vector multiplication
:::

::: {.fragment}
that is, a scalinbg can be simply written as $\lambda\mathbf{x}$ for some scalar value $\lambda$.
:::

::: {.fragment}
In many ways, life would be simpler if we could avoid all of the complexities of rotation, reflection, and projection ... and only work in terms of scaling.

That is the idea we will develop today.
:::

::: {.content-visible when-profile="slides"}
##

Demo: eigshow

##
:::

::: {.content-visible when-profile="web"}
Let's start with a simple example: a __shear__ matrix:

$$ A = \begin{bmatrix}1&0.1\\0&1\end{bmatrix} $$

Let's look at what this matrix does to each point in the plane.

```{python}
#| echo: False
A = np.array([[1, 0.1],
              [0,   1]])
ax = ut.plotSetup(-7,7,-7, 7, size=(8,6))
ut.centerAxes(ax)
for x in range(-6,7):
    for y in range(-6,7):
        v = np.array([x,y])
        ut.plotVec(ax, v)
        ut.plotVec(ax, A.dot(v),'b')
```

We can use arrows to show how each red point is moved to the corresponding blue point.

```{python}
#| echo: False
ax = ut.plotSetup(-7,7,-7, 7, size=(8,6))
ut.centerAxes(ax)
A = np.array([[1,0.1],[0,1]])
for x in range(-6,7):
    for y in range(-6,7):
        ut.plotArrowVec(ax, A.dot(np.array([x,y])), [x,y])
```

Now let's look at a more complicated situation.

Consider the matrix $A = \begin{bmatrix}1.0&0.2\\0.1&0.9\end{bmatrix}$.

```{python}
#| echo: False
V = np.array([[2,-1],[1,1]])
L = np.array([[1.1,0],
              [0,0.8]])
A = V.dot(L).dot(np.linalg.inv(V))
ax = ut.plotSetup(-7,7,-7, 7, size=(12,8))
ut.centerAxes(ax)
for x in range(-6,7):
    for y in range(-6,7):
        v = np.array([x,y])
        ut.plotArrowVec(ax, A.dot(v), v)
```
:::

The starting point for understanding what $A$ does is to look for special vectors which __do not change their direction__ when multiplied by $A$.

::: {.fragment}
__Example.__

Let $A = \begin{bmatrix}3&-2\\1&0\end{bmatrix}, \mathbf{u} = \begin{bmatrix}-1\\1\end{bmatrix}, \mathbf{v}=\begin{bmatrix}2\\1\end{bmatrix}.$  The images of $\mathbf{u}$ and $\mathbf{v}$ under multiplication by $A$ are shown here:

```{python}
#| echo: false
ax = ut.plotSetup(size=(12,8))
ut.centerAxes(ax)
A = np.array([[3,-2],[1,0]])
u = np.array([-1,1])
v = np.array([2,1])
#
ut.plotArrowVec(ax, v, [0,0], color='Blue')
ut.plotArrowVec(ax, A.dot(v), [0,0], color='Blue')
ax.text(v[0],v[1]+0.2,r'${\bf v}$',size=20)
ax.text(A.dot(v)[0],A.dot(v)[1]+0.2,r'$A{\bf v}$',size=20)
#
ut.plotArrowVec(ax, u, [0,0], color='Red')
ut.plotArrowVec(ax, A.dot(u), [0,0], color='Red')
ax.text(u[0]-0.5,u[1]+0.1,r'${\bf u}$',size=20)
ax.text(A.dot(u)[0]-0.7,A.dot(u)[1]+0.3,r'$A{\bf u}$',size=20);
```
:::

::: {.fragment}
For a "typical" vector (like $\mathbf{u}$) the action of $A$ is to scale, reflect, and/or rotate it.  

But for some special vectors (like $\mathbf{v}$) the action of $A$ is only to "stretch" it (without any rotation or reflection).

In fact, $A\mathbf{v}$ is simply $2\mathbf{v}.$
:::

::: {.fragment}
Today we will by studying equations such as 

$$ A\mathbf{x} = 2\mathbf{x}\;\;\;\;\text{or}\;\;\;\;A\mathbf{x} = -4\mathbf{x}$$

where special vectors are transformed by $A$ into scalar multiples of themselves.
:::

::: {.content-visible when-profile="slides"}
##
::: 

__Definitions.__  

::: {.fragment}
An __eigenvector__ of an $n\times n$ matrix $A$ is a nonzero vector $\mathbf{x}$ such that $A\mathbf{x} = \lambda\mathbf{x}$ for some scalar $\lambda.$  
:::

::: {.fragment}
A scalar $\lambda$ is called an __eigenvalue__ of $A$ if there is a nontrivial solution $\mathbf{x}$ of $A\mathbf{x} = \lambda\mathbf{x}.$  
:::

::: {.fragment}
Such an $\mathbf{x}$ is called an _eigenvector corresponding to $\lambda.$_
:::

## Checking Eigenvectors and Eigenvalues

::: {.fragment}
Before considering how to compute eigenvectors, let's look at simply how to determine if a given vector is an eigenvector of a matrix.
:::

::: {.fragment}
__Example.__  Let $A = \begin{bmatrix}1&6\\5&2\end{bmatrix}, \mathbf{u} = \begin{bmatrix}6\\-5\end{bmatrix}, \mathbf{v}=\begin{bmatrix}3\\-2\end{bmatrix}.$  

Are $\mathbf{u}$ and $\mathbf{v}$ eigenvectors of $A$?
:::

::: {.fragment}
__Solution.__

$$A\mathbf{u} = \begin{bmatrix}1&6\\5&2\end{bmatrix}\begin{bmatrix}6\\-5\end{bmatrix} = \begin{bmatrix}-24\\20\end{bmatrix} = -4\begin{bmatrix}6\\-5\end{bmatrix} = -4\mathbf{u}.$$
:::

::: {.fragment}
$$A\mathbf{v} = \begin{bmatrix}1&6\\5&2\end{bmatrix}\begin{bmatrix}3\\-2\end{bmatrix} = \begin{bmatrix}-9\\11\end{bmatrix} \neq \lambda\begin{bmatrix}3\\-2\end{bmatrix}.$$
:::

::: {.fragment}
So $\mathbf{u}$ is an eigenvector corresponding to the eigenvalue $-4$, 

but $\mathbf{v}$ is not an eigenvector of $A$.
:::

## Finding Eigenvectors

Our first task will be to find eigenvectors.  We'll start by working through an example.

::: {.fragment}
__Example.__   Let's use the same matrix $A = \begin{bmatrix}1&6\\5&2\end{bmatrix}.$

We'll now show that 7 is an eigenvalue of the matrix $A$, and we will find the corresponding eigenvectors.
:::

::: {.fragment}
For 7 to be an eigenvalue of $A$, it must be the case that 

$$ A\mathbf{x}=7\mathbf{x}$$

has a nontrivial solution.
:::

::: {.fragment}
This is equivalent to:

$$ A\mathbf{x} - 7\mathbf{x} = {\bf 0}$$
:::

::: {.fragment}
or 

$$ (A - 7I)\mathbf{x} = {\bf 0}.$$
:::

::: {.fragment}
To solve this homogeneous equation, form the matrix

$$A - 7I = \begin{bmatrix}1&6\\5&2\end{bmatrix}-\begin{bmatrix}7&0\\0&7\end{bmatrix} = \begin{bmatrix}-6&6\\5&-5\end{bmatrix}.$$
:::

::: {.fragment}
To see that $(A - 7I)\mathbf{x} = {\bf 0}$ has a nontrivial solution, observe that the columns of $A-7I$ are obviously linearly dependent. 

So there are nonzero vectors that are solutions of $(A - 7I)\mathbf{x} = {\bf 0}$.

<font color = "blue">So 7 is an eigenvalue of $A$.</font>
:::

::: {.fragment}
To find the corresponding eigenvectors, we solve $(A - 7I)\mathbf{x} = {\bf 0}$ using row operations:

$$\begin{bmatrix}-6&6&0\\5&-5&0\end{bmatrix}\sim\begin{bmatrix}1&-1&0\\0&0&0\end{bmatrix}.$$
:::

::: {.fragment}
This says that $x_1 = x_2,$ and $x_2$ is free.

So the general solution has the form $x_2\begin{bmatrix}1\\1\end{bmatrix}.$  

<font color = "blue">Each vector of this form with $x_2 \neq 0$ is an eigenvector corresponding to $\lambda = 7.$</font>
:::

::: {.content-visible when-profile="slides"}
##
::::: {.fragment .fade-down .r-fit-text}
Question Time!   Q16.1
:::::
:::

## Eigenspace

::: {.fragment}
Now, let's generalize what we just did.

From the previous example, we can conclude that $\lambda$ is an eigenvalue of an $n\times n$ matrix $A$ if and only if the equation

$$(A-\lambda I)\mathbf{x} = {\bf 0}$$

has a nontrivial solution.
:::

::: {.fragment}
Notice that the set of _all_ solutions of that equation is just 

__the null space of the matrix $A-\lambda I.$__
:::

::: {.fragment}
So the set of all eigenvectors corresponding to a particular $\lambda$ (along with the trivial solution, that is the $\mathbf{0}$ vector) forms a _subspace_ of $\mathbb{R}^n$, 

and is called the __eigenspace__ of $A$ corresponding to $\lambda$.
:::

::: {.content-visible when-profile="slides"}
##
::: 

__Example.__  For matrix $A = \begin{bmatrix}1&6\\5&2\end{bmatrix},$ we found that the general solution for the eigenvector corresponding to $\lambda = 7$ is the expression $x_2\begin{bmatrix}1\\1\end{bmatrix}.$  

::: {.fragment}
This means that the eigenspace corresponding to $\lambda = 7$ consist|s of all multiples of $\begin{bmatrix}1\\1\end{bmatrix},$ 

... in other words $\operatorname{Span}\left\{\begin{bmatrix}1\\1\end{bmatrix}\right\}$.
:::

::: {.fragment}
So we would also say that $\left\{\begin{bmatrix}1\\1\end{bmatrix}\right\}$ is a basis for the eigenspace corresponding to $\lambda = 7$.
:::

::: {.fragment}
We could also show that the eigenspace corresponding to $\lambda = -4$ is $\operatorname{Span}\left\{\begin{bmatrix}-6\\5\end{bmatrix}\right\}$
:::

::: {.fragment}
```{python}
#| echo: false
ax = ut.plotSetup(-10,10,-6,10,size=(7,7))
ut.centerAxes(ax)
A = np.array([[1,6],[5,2]])
u = np.array([1,1])
v = np.array([3./2,-5./4])
#
ax.plot(v[0],v[1],'bo')
ax.plot(A.dot(v)[0],A.dot(v)[1],'bo')
ax.text(v[0],v[1]+0.2,r'${\bf v}$',size=20)
ax.text(A.dot(v)[0]+0.2,A.dot(v)[1]+0.2,r'$A{\bf v}$',size=20)
ax.plot([-10,10],[5*10/6.0,-5*10/6.0],'b-',lw=2)
#
ax.plot(u[0],u[1],'ro')
ax.plot(A.dot(u)[0],A.dot(u)[1],'ro')
ax.text(u[0]+0.7,u[1],r'${\bf u}$',size=20)
ax.text(A.dot(u)[0]+0.7,A.dot(u)[1],r'$A{\bf u}$',size=20)
ax.plot([-6,10],[-6,10],'r-',lw=2)
# 
ax.annotate('', xy=(A.dot(u)[0], A.dot(u)[1]),  xycoords='data',
                xytext=(u[0], u[1]), textcoords='data',
                size=18,
                #bbox=dict(boxstyle="round", fc="0.8"),
                arrowprops={'arrowstyle': 'simple',
                                'fc': '0.5', 
                                'ec': 'none',
                                'connectionstyle' : 'arc3,rad=0.5'},
                )
ax.annotate('', xy=(A.dot(v)[0], A.dot(v)[1]),  xycoords='data',
                xytext=(v[0], v[1]), textcoords='data',
                size=18,
                #bbox=dict(boxstyle="round", fc="0.8"),
                arrowprops={'arrowstyle': 'simple',
                                'fc': '0.5', 
                                'ec': 'none',
                                'connectionstyle' : 'arc3,rad=-0.5'},
                )
ax.text(3,9,r'Eigenspace for $\lambda$ = 7',size=16)
ax.text(-9.5,8,r'Eigenspace for $\lambda = -4$',size=16);
```
:::

::: {.content-visible when-profile="slides"}
##
::: 

__Example.__  Let $A = \begin{bmatrix}4&-1&6\\2&1&6\\2&-1&8\end{bmatrix}.$  An eigenvalue of $A$ is $2.$  Find a basis for the the corresponding eigenspace.

::: {.fragment}
__Solution.__  The eigenspace we are looking for is the nullspace of $A - 2I.$  

We will use the strategy for finding a basis for a nullspace that we learned in the last lecture.
:::

::: {.fragment}
Form

$$A - 2I = \begin{bmatrix}4&-1&6\\2&1&6\\2&-1&8\end{bmatrix}-\begin{bmatrix}2&0&0\\0&2&0\\0&0&2\end{bmatrix} = \begin{bmatrix}2&-1&6\\2&-1&6\\2&-1&6\end{bmatrix}$$
:::

::: {.fragment}
and row reduce the augmented matrix for $(A-2I)\mathbf{x}={\bf 0}$:

$$\begin{bmatrix}2&-1&6&0\\2&-1&6&0\\2&-1&6&0\end{bmatrix} \sim \begin{bmatrix}2&-1&6&0\\0&0&0&0\\0&0&0&0\end{bmatrix}$$
:::

::: {.fragment}
At this point, it is clear that 2 is indeed an eigenvalue of $A$, 

because the equation $(A-2I)\mathbf{x} = {\bf 0}$ has free variables.  
:::

::: {.fragment}
The general solution is $x_1 = \frac{1}{2}x_2 -3x_3.$

Expressed as a vector, the general solution is:

$$\begin{bmatrix}\frac{1}{2}x_2 - 3x_3\\x_2\\x_3\end{bmatrix}.$$
:::

::: {.fragment}
Expressed as a vector sum, this is:

$$x_2\begin{bmatrix}1/2\\1\\0\end{bmatrix} + x_3\begin{bmatrix}-3\\0\\1\end{bmatrix}\;\;\;\;\text{with $x_2$ and $x_3$ free.}$$
:::

::: {.fragment}
So this eigenspace is a two-dimensional subspace of $\mathbb{R}^3,$ and a basis for this subspace is

$$\left\{\begin{bmatrix}1/2\\1\\0\end{bmatrix},\begin{bmatrix}-3\\0\\1\end{bmatrix}\right\}.$$
:::

::: {.fragment}
```{python}
#| echo: false
fig = ut.three_d_figure((16, 1), fig_desc = '2D Eigenspace for lambda = 2',
                        xmin = -3, xmax = 3, ymin = -3, ymax = 3, zmin = -3, zmax = 3,
                        qr = qr_setting, figsize = (7, 7))
plt.close()
v = np.array([1.,2.,0])
v = v/np.sqrt(np.sum(v*v))
u = np.array([-3.0,0,1.0])
u = u/np.sqrt(np.sum(u*u))
w = -v-2.0*u
w = w/np.sqrt(np.sum(w*w))
pts = np.array([v,u,w])
#ax.text(v[0],v[1],v[2],r'$\bf v$',size=20)
#ax.text(u[0],u[1],u[2],r'$\bf u$',size=20)
fig.text(-2, -2, -3, r'Eigenspace for $\lambda=2$', 'Eigenspace for lambda = 2', size=20)
#ax.text(0.2,0.2,-4,r'$\bf 0$',size=20)
# plotting the span of v
fig.plotSpan(u, v, 'Green')
fig.plotPoint(u[0], u[1], u[2], 'r')
fig.plotPoint(v[0], v[1], v[2], 'r')
fig.plotPoint(w[0], w[1], w[2], 'r')
fig.plotPoint(0, 0, 0, 'k')
fig.ax.quiver(pts[:,0],pts[:,1],pts[:,2],pts[:,0],pts[:,1],pts[:,2],length=1.0,color='Red',lw=2)
pts = 2.0*pts
fig.ax.quiver(pts[:,0],pts[:,1],pts[:,2],pts[:,0],pts[:,1],pts[:,2],length=1.0,color='Blue',lw=2)
u = 2*u
v = 2*v
w = 2*w
fig.plotPoint(u[0], u[1], u[2], 'b')
fig.plotPoint(v[0], v[1], v[2], 'b')
fig.plotPoint(w[0], w[1], w[2], 'b')
# plotting the axes
# ut.plotIntersection3d(ax,[0,0,1,0],[0,1,0,0])
# ut.plotIntersection3d(ax,[0,0,1,0],[1,0,0,0])
fig.plotIntersection([0,1,0,0], [1,0,0,0])
fig.ax.view_init(azim = 0, elev = 22)
fig.save()
#
def anim(frame):
    fig.ax.view_init(azim = frame, elev = 22)
    # fig.canvas.draw()
#
# create and display the animation 
HTML(animation.FuncAnimation(fig.fig, anim,
                       frames = 5 * np.arange(72),
                       fargs = None,
                       interval = 100).to_jshtml(default_mode = 'loop'))
```
:::

::: {.content-visible when-profile="slides"}
##
::::: {.fragment .fade-down .r-fit-text}
Question Time!   Q16.2
:::::
:::

## Eigenvalues of a Triangular Matrix

::: {.fragment}
__Theorem.__  The eigenvalues of a triangular matrix are the entries on its main diagonal.
:::

::: {.fragment}
__Proof.__  We'll consider the $3\times 3$ case.  If $A$ is upper triangular, then $A-\lambda I$ has the form

$$A - \lambda I = \begin{bmatrix}a_{11}&a_{12}&a_{13}\\0&a_{22}&a_{23}\\0&0&a_{33}\end{bmatrix} - \begin{bmatrix}\lambda&0&0\\0&\lambda&0\\0&0&\lambda\end{bmatrix}$$
:::

::: {.fragment}
$$ = \begin{bmatrix}a_{11}-\lambda&a_{12}&a_{13}\\0&a_{22}-\lambda&a_{23}\\0&0&a_{33}-\lambda\end{bmatrix}$$
:::

::: {.fragment}
Now, $\lambda$ is an eigenvalue of $A$ if and only if the equation $(A-\lambda I)\mathbf{x} = {\bf 0}$ has a nontrivial solution -- 

in other words, if and only if the equation has a free variable.
:::

::: {.fragment}
The special pattern that the zero entries have in $A$ means that there will be a free variable if any diagonal entry is also zero, because then there will be a column without a pivot.
:::

::: {.fragment}
That is, $(A-\lambda I)\mathbf{x} = {\bf 0}$ has a free variable if and only if at least one of the entries on the diagonal of $(A-\lambda I)$ is zero.
:::

::: {.fragment}
This happens if and only if $\lambda$ equals one of the entries $a_{11}, a_{22},$ or $a_{33}$ on the diagonal of $A$.
:::

::: {.fragment}
__Example.__  Let $A = \begin{bmatrix}3&6&-8\\0&0&6\\0&0&2\end{bmatrix}$ and $B = \begin{bmatrix}4&0&0\\-2&1&0\\5&3&4\end{bmatrix}$.
:::

::: {.fragment}
Then the eigenvalues of $A$ are 3, 0, and 2.   The eigenvalues of $B$ are 4 and 1.
:::

## Invertibility and Eigenvalues

::: {.fragment}
So far we haven't probed what it means for a matrix to have an eigenvalue of 0.
:::

::: {.fragment}
This happens if and only if the equation $A\mathbf{x} = 0\mathbf{x}$ has a nontrivial solution.
:::

::: {.fragment}
But that equation is equivalent to $A\mathbf{x} = {\bf 0}$ which has a nontrivial solution if and only if $A$ is not invertible.
:::

::: {.fragment}
This means that 

<center><font color = "blue">0 is an eigenvalue of <em>A</em> if and only if <em>A</em> is not invertible.</font></center>
:::

::: {.fragment}
This draws an important connection between invertibility and zero eigenvalues.
:::

::: {.fragment}
So we have __yet another__ addition to the Invertible Matrix Theorem!
:::

::: {.content-visible when-profile="slides"}
##
::::: {.fragment .fade-down .r-fit-text}
Question Time!   Q16.3
:::::
:::

## Eigenvectors Solve Difference Equations

::: {.fragment}
Let's return to the problem we considered at the outset: predicting future values of $\mathbf{x}_t$ (the number of CS majors of each class in year $t$).
:::

::: {.fragment}
We modeled the future as $\mathbf{x}_{t+1} = A\mathbf{x}_t\;\;\;(t = 0,1,2,\dots).$

So then $\mathbf{x}_{t} = A^t\mathbf{x}_0\;\;\;(t = 0,1,2,\dots).$
:::

::: {.fragment}
A __solution__ of such an equation is an explicit description of $\{\mathbf{x}_t\}$ whose formula for each $\mathbf{x}_t$ does not depend directly on $A$ or on the preceding terms in the sequence other than the initial term $\mathbf{x}_0.$
:::

::: {.fragment}
The simplest way to build a solution is to take an eigenvector $\mathbf{x}_0$ and its corresponding eigenvalue $\lambda$.

Then 

$$ \mathbf{x}_t = \lambda^t \mathbf{x}_0 \;\;\;(t = 0,1,2,\dots)$$
:::

::: {.fragment}
This sequence is a solution because 

$$A\mathbf{x}_t = A(\lambda^t\mathbf{x}_0) = \lambda^t(A\mathbf{x}_0) = \lambda^t(\lambda\mathbf{x}_0) = \lambda^{t+1}\mathbf{x}_0 = \mathbf{x}_{t+1}.$$
:::

::: {.content-visible when-profile="slides"}
##
:::

We can extend this to combinations of eigenvectors.  

::: {.fragment}
If $A$ has two eigenvectors $\mathbf{u}$ and $\mathbf{v}$ corresponding to eigenvalues $\lambda$ and $\mu$,
then another solution is 

$$\mathbf{x}_t = \lambda^t\mathbf{u} + \mu^t \mathbf{v}$$
:::

::: {.fragment}
To address the problem of predicting the CS major population, 

we assume that we can express the state vector $\mathbf{x}$ as a linear combination of eigenvectors $\mathbf{u}_1, \mathbf{u}_2, \dots,\mathbf{u}_p$.  

That is, 

$$\mathbf{x}_0 = a_1\mathbf{u}_1 + a_2\mathbf{u}_2+\dots+a_p\mathbf{u}_p.$$
:::

::: {.fragment}
Then at time $t$, 

$$\mathbf{x}_t = \lambda_1^ta_1\mathbf{u}_1 + \lambda_2^ta_2\mathbf{u}_2 + \dots + \lambda_p^ta_p\mathbf{u}_p.$$

Which gives us an explicit solution for the number of students in the major in any future year.
:::

::: {.fragment}
Note that the expression above is exponential in each of the $\lambda_1,\dots,\lambda_p$. 
:::

::: {.fragment}
The resulting value will be essentially determined by the largest $\lambda_i$.   

To see this, let's say that $\lambda_1$ is the largest eigenvalue, and $\lambda_2$ is the second largest.   
:::

::: {.fragment}
Then the proportion of $\mathbf{u}_1$ to $\mathbf{u}_2$ contained in $\mathbf{x}_t$ will be given by $(\lambda_1/\lambda_2)^t$.   

Since this ratio is larger than 1, the relative proportion of $\mathbf{u}_1$ will grow exponentially.  
:::

::: {.fragment}
This shows the importance of the largest eigenvalue in the value of $\mathbf{x}_t$ for large $t$.  

In the long run, $\mathbf{x}_t$ will be very close to $\lambda_1^t\mathbf{u}_1.$
:::

::: {.fragment}
The largest eigenvalue shows the asymptotic rate of growth (or shrinkage) of the state vector $\mathbf{x}_t$.
:::

::: {.fragment}
For the matrix $A$ that describes CS major population growth, the largest eigenvalue is 1.19.  

In other words, this models the population growth rate of CS majors at that time as increasing about 19% per year.
:::

