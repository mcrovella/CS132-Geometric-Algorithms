

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Linear Models &#8212; Linear Algebra, Geometry, and Computation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'L23LinearModels';</script>
    <link rel="shortcut icon" href="_static/DiagramAR-icon.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Symmetric Matrices" href="L24SymmetricMatrices.html" />
    <link rel="prev" title="Least Squares" href="L22LeastSquares.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="landing-page.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/DiagramAR-icon.png" class="logo__image only-light" alt="Linear Algebra, Geometry, and Computation - Home"/>
    <script>document.write(`<img src="_static/DiagramAR-icon.png" class="logo__image only-dark" alt="Linear Algebra, Geometry, and Computation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="landing-page.html">
                    Preface
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="L01LinearEquations.html">Linear Equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="L02Numerics.html">(Getting Serious About) Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="L03RowReductions.html">Gaussian Elimination</a></li>
<li class="toctree-l1"><a class="reference internal" href="L04VectorEquations.html">Vector Equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="L05Axb.html"><span class="math notranslate nohighlight">\(A{\bf x} = {\bf b}\)</span></a></li>
<li class="toctree-l1"><a class="reference internal" href="L06LinearIndependence.html">Linear Independence</a></li>
<li class="toctree-l1"><a class="reference internal" href="L07LinearTransformations.html">Linear Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="L08MatrixofLinearTranformation.html">The Matrix of a Linear Transformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="L09MatrixOperations.html">Matrix Algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="L10MatrixInverse.html">The Inverse of a Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="L11MarkovChains.html">Markov Chains</a></li>
<li class="toctree-l1"><a class="reference internal" href="L12MatrixFactorizations.html">Matrix Factorizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="L13ComputerGraphics.html">Computer Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="L14Subspaces.html">Subspaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="L15DimensionRank.html">Dimension and Rank</a></li>
<li class="toctree-l1"><a class="reference internal" href="L16Eigenvectors.html">Eigenvectors and Eigenvalues</a></li>
<li class="toctree-l1"><a class="reference internal" href="L17CharacteristicEqn.html">The Characteristic Equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="L18Diagonalization.html">Diagonalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="L19PageRank.html">PageRank</a></li>
<li class="toctree-l1"><a class="reference internal" href="L20Orthogonality.html">Analytic Geometry in <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span></a></li>
<li class="toctree-l1"><a class="reference internal" href="L21OrthogonalSets.html">Orthogonal Sets and Projection</a></li>
<li class="toctree-l1"><a class="reference internal" href="L22LeastSquares.html">Least Squares</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="L24SymmetricMatrices.html">Symmetric Matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="L25SVD.html">The Singular Value Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="L26ApplicationsOfSVD.html">Applications of the SVD</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/mcrovella/CS132-Geometric-Algorithms" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/L23LinearModels.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-problems">Regression Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-framework-of-linear-models">The Framework of Linear Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-a-line-to-data">Fitting a Line to Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-least-squares-problem">A Least-Squares Problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-general-linear-model">The General Linear Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-fitting-of-other-models">Least-Squares Fitting of Other Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-regression">Multiple Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-regression-in-practice">Multiple Regression in Practice</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="cell tag_remove-input docutils container">
</div>
<section class="tex2jax_ignore mathjax_ignore" id="linear-models">
<h1>Linear Models<a class="headerlink" href="#linear-models" title="Permalink to this heading">#</a></h1>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/35e832b06982cf24d5993c3b565832fa38fa9d155ae1b666d1b5dc072c504ebb.jpg" src="_images/35e832b06982cf24d5993c3b565832fa38fa9d155ae1b666d1b5dc072c504ebb.jpg" />
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><a href="https://commons.wikimedia.org/wiki/File:Sir_Francis_Galton_by_Charles_Wellington_Furse.jpg#/media/File:Sir_Francis_Galton_by_Charles_Wellington_Furse.jpg">Sir Francis Galton by Charles Wellington Furse</a> by Charles Wellington Furse (died 1904) - <a href="//en.wikipedia.org/wiki/National_Portrait_Gallery,_London" class="extiw" title="en:National Portrait Gallery, London">National Portrait Gallery</a>: <a rel="nofollow" class="external text" href="http://www.npg.org.uk/collections/search/portrait.php?search=ap&amp;npgno=3916&amp;eDate=&amp;lDate=">NPG 3916</a></div></div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/0740bae3b41006baf591754cd6efff297efa0e7574bf2e26f4a260dd3ceebf54.png" src="_images/0740bae3b41006baf591754cd6efff297efa0e7574bf2e26f4a260dd3ceebf54.png" />
</div>
</div>
<p>In 1886 Francis Galton published his observations about how random factors affect outliers.</p>
<p>This notion has come to be called “regression to the mean” because unusually large or small phenomena, after the influence of random events, become closer to their mean values (less extreme).</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/3720f936d0e42166d3c8611c270a41946dfeea0cba67c15a723296b6c45c16f6.png" src="_images/3720f936d0e42166d3c8611c270a41946dfeea0cba67c15a723296b6c45c16f6.png" />
</div>
</div>
<p>One of the most fundamental kinds of machine learning is the construction of a model that can be used to summarize a set of data.</p>
<p>A model is a concise description of a dataset or a real-world phenomenon.</p>
<p>For example, an equation can be a model if we use the equation to describe something in the real world.</p>
<p>The most common form of modeling is <strong>regression</strong>, which means constructing an equation that describes the relationships among variables.</p>
<section id="regression-problems">
<h2>Regression Problems<a class="headerlink" href="#regression-problems" title="Permalink to this heading">#</a></h2>
<p>For example, we may look at these points and observe that they approximately lie on a line.</p>
<p>So we could decide to <strong>model</strong> this data using a line.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/e8fb14005eb6a2b654c26e68a90c3721dd4f1ade4453dc9a270f5405ba9f1b0d.png" src="_images/e8fb14005eb6a2b654c26e68a90c3721dd4f1ade4453dc9a270f5405ba9f1b0d.png" />
</div>
</div>
<p>We may look at these points and decide to model them using a quadratic function.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/ddbfd958a6196410c384fef6f26928c96e5680d9b01b476a0f01d07882b9daab.png" src="_images/ddbfd958a6196410c384fef6f26928c96e5680d9b01b476a0f01d07882b9daab.png" />
</div>
</div>
<p>And we may look at these points and decide to model them using a logarithmic function.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/47b6b03162906cf8bfdf5f9d72aed2547f0d811a60f9f8ac5f8eb4345f558e1b.png" src="_images/47b6b03162906cf8bfdf5f9d72aed2547f0d811a60f9f8ac5f8eb4345f558e1b.png" />
</div>
</div>
<p>Clearly, none of these datasets agrees perfectly with the proposed model.   So the question arises:</p>
<p>How do we find the <strong>best</strong> linear function (or quadratic function, or logarithmic function) given the data?</p>
</section>
<section id="the-framework-of-linear-models">
<h2>The Framework of Linear Models<a class="headerlink" href="#the-framework-of-linear-models" title="Permalink to this heading">#</a></h2>
<p>The regression problem has been studied extensively in the field of statistics and machine learning.</p>
<p>Certain terminology is used:</p>
<ul class="simple">
<li><p>Some values are referred to as “independent,” and</p></li>
<li><p>Some values are referred to as “dependent.”</p></li>
</ul>
<p>The basic regression task is:</p>
<ul class="simple">
<li><p>given a set of independent variables</p></li>
<li><p>and the associated dependent variables,</p></li>
<li><p>estimate the parameters of a model (such as a line, parabola, etc) that describes how the dependent variables are related to the independent variables.</p></li>
</ul>
<p>The independent variables are collected into a matrix <span class="math notranslate nohighlight">\(X,\)</span> which is called the <strong>design matrix.</strong></p>
<p>The dependent variables are collected into an <strong>observation</strong> vector <span class="math notranslate nohighlight">\(\mathbf{y}.\)</span></p>
<p>The parameters of the model (for any kind of model) are collected into a <strong>parameter</strong> vector <span class="math notranslate nohighlight">\(\beta.\)</span></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/18fa750fcc203761e7cd658350f76aa77d8306da6e40d593030da0f1c49cac96.png" src="_images/18fa750fcc203761e7cd658350f76aa77d8306da6e40d593030da0f1c49cac96.png" />
</div>
</div>
</section>
<section id="fitting-a-line-to-data">
<h2>Fitting a Line to Data<a class="headerlink" href="#fitting-a-line-to-data" title="Permalink to this heading">#</a></h2>
<p>The first kind of model we’ll study is a linear equation:</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1 x.\]</div>
<p>This is the most commonly used type of model, particularly in fields like economics, psychology, biology, etc.</p>
<p>The reason it is so commonly used is that, like Galton’s data, experimental data often produce points <span class="math notranslate nohighlight">\((x_1, y_1), \dots, (x_n,y_n)\)</span> that seem to lie close to a line.</p>
<p>The question we must confront is: given a set of data, how should we “fit” the equation of the line to the data?</p>
<p>Our intuition is this: we want to determine the parameters <span class="math notranslate nohighlight">\(\beta_0, \beta_1\)</span> that define a line that is as “close” to the points as possible.</p>
<p>Let’s develop some terminology for evaluating a model.</p>
<p>Suppose we have a line <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1 x\)</span>.   For each data point <span class="math notranslate nohighlight">\((x_j, y_j),\)</span> there is a point <span class="math notranslate nohighlight">\((x_j, \beta_0 + \beta_1 x_j)\)</span> that is the point on the line with the same <span class="math notranslate nohighlight">\(x\)</span>-coordinate.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/48f6f40940ff62e227b595d38d3e89c5b7770e9d3afe06aabec9e95f2483b09e.jpg" src="_images/48f6f40940ff62e227b595d38d3e89c5b7770e9d3afe06aabec9e95f2483b09e.jpg" />
</div>
</div>
<p>We call <span class="math notranslate nohighlight">\(y_j\)</span> the <strong>observed</strong> value of <span class="math notranslate nohighlight">\(y\)</span></p>
<p>and we call <span class="math notranslate nohighlight">\(\beta_0 + \beta_1 x_j\)</span> the <strong>predicted</strong> <span class="math notranslate nohighlight">\(y\)</span>-value.</p>
<p>The difference between an observed <span class="math notranslate nohighlight">\(y\)</span>-value and a predicted <span class="math notranslate nohighlight">\(y\)</span>-value is called a <strong>residual</strong>.</p>
<p>There are several ways to measure how “close” the line is to the data.</p>
<p>The usual choice is to sum the squares of the residuals.</p>
<p>(Note that the residuals themselves may be positive or negative – by squaring them, we ensure that our error measures don’t cancel out.)</p>
<p>The <strong>least-squares line</strong> is the line <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1x\)</span> that minimizes the sum of squares of the residuals.</p>
<p>The coefficients <span class="math notranslate nohighlight">\(\beta_0, \beta_1\)</span> of the line are called <strong>regression coefficients.</strong></p>
</section>
<section id="a-least-squares-problem">
<h2>A Least-Squares Problem<a class="headerlink" href="#a-least-squares-problem" title="Permalink to this heading">#</a></h2>
<p>Let’s imagine for a moment that the data fit a line perfectly.</p>
<p>Then, if each of the data points happened to fall exactly on the line, the parameters <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> would satisfy the equations</p>
<div class="math notranslate nohighlight">
\[\beta_0 + \beta_1 x_1 = y_1 \]</div>
<div class="math notranslate nohighlight">
\[\beta_0 + \beta_1 x_2 = y_2 \]</div>
<div class="math notranslate nohighlight">
\[\beta_0 + \beta_1 x_3 = y_3 \]</div>
<div class="math notranslate nohighlight">
\[ \vdots\]</div>
<div class="math notranslate nohighlight">
\[\beta_0 + \beta_1 x_n = y_n \]</div>
<p>We can write this system as</p>
<div class="math notranslate nohighlight">
\[X\mathbf{\beta} = \mathbf{y}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}X=\begin{bmatrix}1&amp;x_1\\1&amp;x_2\\\vdots&amp;\vdots\\1&amp;x_n\end{bmatrix},\;\;\mathbf{\beta} = \begin{bmatrix}\beta_0\\\beta_1\end{bmatrix},\;\;\mathbf{y}=\begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix}\end{split}\]</div>
<p>Of course, if the data points don’t actually lie exactly on a line,</p>
<p>… then there are no parameters <span class="math notranslate nohighlight">\(\beta_0, \beta_1\)</span> for which the predicted <span class="math notranslate nohighlight">\(y\)</span>-values in <span class="math notranslate nohighlight">\(X\mathbf{\beta}\)</span> equal the observed <span class="math notranslate nohighlight">\(y\)</span>-values in <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>,</p>
<p>… and <span class="math notranslate nohighlight">\(X\mathbf{\beta}=\mathbf{y}\)</span> has no solution.</p>
<p>Now, since the data doesn’t fall exactly on a line, we have decided to seek the <span class="math notranslate nohighlight">\(\beta\)</span> that minimizes the sum of squared residuals, ie,</p>
<div class="math notranslate nohighlight">
\[\sum_i (\beta_0 + \beta_1 x_i - y_i)^2\]</div>
<div class="math notranslate nohighlight">
\[=\Vert X\beta -\mathbf{y}\Vert^2\]</div>
<p>This is key: <strong>the sum of squares of the residuals</strong> is <strong>exactly</strong> the <strong>square of the distance between the vectors <span class="math notranslate nohighlight">\(X\mathbf{\beta}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}.\)</span></strong></p>
<p>This is a least-squares problem, <span class="math notranslate nohighlight">\(A\mathbf{x} = \mathbf{b},\)</span> with different notation.</p>
<p><em>Computing the least-squares solution of <span class="math notranslate nohighlight">\(X\mathbf{\beta} = \mathbf{y}\)</span> is equivalent to finding the <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> that determines the least-squares line.</em></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/bd3dd7ad76b43d6e80b4ab098f499982ef2b185227e9d2a9a73f2c182868e8a8.png" src="_images/bd3dd7ad76b43d6e80b4ab098f499982ef2b185227e9d2a9a73f2c182868e8a8.png" />
</div>
</div>
<div class="toggle docutils container">
<p>Question Time!  Q23.1</p>
</div>
<p><strong>Example 1.</strong>  Find the equation <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1 x\)</span> of the least-squares line that best fits the data points</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>x</p></th>
<th class="head"><p>y</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>2</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>3</p></td>
</tr>
</tbody>
</table>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/6629321405c462336ece9c8f77b5144945df819b29240dbedd781f785827aa94.png" src="_images/6629321405c462336ece9c8f77b5144945df819b29240dbedd781f785827aa94.png" />
</div>
</div>
<p><strong>Solution.</strong> Use the <span class="math notranslate nohighlight">\(x\)</span>-coordinates of the data to build the design matrix <span class="math notranslate nohighlight">\(X\)</span>, and the <span class="math notranslate nohighlight">\(y\)</span>-coordinates to build the observation vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X = \begin{bmatrix}1&amp;2\\1&amp;5\\1&amp;7\\1&amp;8\end{bmatrix},\;\;\;\mathbf{y}=\begin{bmatrix}1\\2\\3\\3\end{bmatrix}\end{split}\]</div>
<p>Now, to obtain the least-squares line, find the least-squares solution to <span class="math notranslate nohighlight">\(X\mathbf{\beta} = \mathbf{y}.\)</span></p>
<p>We do this via the method we learned last lecture (just with new notation):</p>
<div class="math notranslate nohighlight">
\[X^TX\mathbf{\beta} = X^T\mathbf{y}\]</div>
<p>So, we compute:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X^TX = \begin{bmatrix}1&amp;1&amp;1&amp;1\\2&amp;5&amp;7&amp;8\end{bmatrix}\begin{bmatrix}1&amp;2\\1&amp;5\\1&amp;7\\1&amp;8\end{bmatrix} = \begin{bmatrix}4&amp;22\\22&amp;142\end{bmatrix}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}X^T\mathbf{y} =  \begin{bmatrix}1&amp;1&amp;1&amp;1\\2&amp;5&amp;7&amp;8\end{bmatrix}\begin{bmatrix}1\\2\\3\\3\end{bmatrix} = \begin{bmatrix}9\\57\end{bmatrix}\end{split}\]</div>
<p>So the normal equations are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}4&amp;22\\22&amp;142\end{bmatrix}\begin{bmatrix}\beta_0\\\beta_1\end{bmatrix} = \begin{bmatrix}9\\57\end{bmatrix}\end{split}\]</div>
<p>Solving, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}\beta_0\\\beta_1\end{bmatrix}=\begin{bmatrix}4&amp;22\\22&amp;142\end{bmatrix}^{-1}\begin{bmatrix}9\\57\end{bmatrix}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split} = \frac{1}{84}\begin{bmatrix}142&amp;-22\\-22&amp;4\end{bmatrix}\begin{bmatrix}9\\57\end{bmatrix}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split} = \begin{bmatrix}2/7\\5/14\end{bmatrix}\end{split}\]</div>
<p>So the least-squares line has the equation</p>
<div class="math notranslate nohighlight">
\[y = \frac{2}{7} + \frac{5}{14}x.\]</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/bffdf2b1d078928e5fb96f8f11e3c16ce5179a8ad08f4d32ef7507a0f8e9ba51.png" src="_images/bffdf2b1d078928e5fb96f8f11e3c16ce5179a8ad08f4d32ef7507a0f8e9ba51.png" />
</div>
</div>
</section>
<section id="the-general-linear-model">
<h2>The General Linear Model<a class="headerlink" href="#the-general-linear-model" title="Permalink to this heading">#</a></h2>
<p>Another way that the inconsistent linear system is often written is to collect all the residuals into a <strong>residual vector.</strong></p>
<p>Then an exact equation is</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = X\mathbf{\beta} + {\mathbf\epsilon}\]</div>
<p>Any equation of this form is referred to as a <strong>linear model.</strong></p>
<p>In this formulation, the goal is to minimize the length of <span class="math notranslate nohighlight">\(\epsilon\)</span>, ie, <span class="math notranslate nohighlight">\(\Vert\epsilon\Vert.\)</span></p>
<p>In some cases, one would like to fit data points with something other than a straight line.</p>
<p>For example, think of Gauss trying to find the equation for the orbit of Ceres.</p>
<p>In cases like this, the matrix equation is still <span class="math notranslate nohighlight">\(X\mathbf{\beta} = \mathbf{y}\)</span>, but the specific form of <span class="math notranslate nohighlight">\(X\)</span> changes from one problem to the next.</p>
<p>The least-squares solution <span class="math notranslate nohighlight">\(\hat{\mathbf{\beta}}\)</span> is a solution of the normal equations</p>
<div class="math notranslate nohighlight">
\[X^TX\mathbf{\beta} = X^T\mathbf{y}.\]</div>
</section>
<section id="least-squares-fitting-of-other-models">
<h2>Least-Squares Fitting of Other Models<a class="headerlink" href="#least-squares-fitting-of-other-models" title="Permalink to this heading">#</a></h2>
<p>Most models have parameters, and the object of <strong>model fitting</strong> is to to fix those parameters.   Let’s talk about model parameters.</p>
<p>In model fitting, the parameters are the unknown.  A central question for us is whether the model is <em>linear</em> in its parameters.</p>
<p>For example, the model <span class="math notranslate nohighlight">\(y = \beta_0 e^{-\beta_1 x}\)</span> is <strong>not</strong> linear in its parameters.</p>
<p>The model <span class="math notranslate nohighlight">\(y = \beta_0 e^{-2 x}\)</span> <strong>is</strong> linear in its parameters.</p>
<p>For a model that is linear in its parameters, an observation is a linear combination of (arbitrary) known functions.</p>
<p>In other words, a model that is linear in its parameters is</p>
<div class="math notranslate nohighlight">
\[y = \beta_0f_0(x) + \beta_1f_1(x) + \dots + \beta_nf_n(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(f_0, \dots, f_n\)</span> are known functions and <span class="math notranslate nohighlight">\(\beta_0,\dots,\beta_k\)</span> are parameters.</p>
<p><strong>Example.</strong>  Suppose data points <span class="math notranslate nohighlight">\((x_1, y_1), \dots, (x_n, y_n)\)</span> appear to lie along some sort of parabola instead of a straight line.  Suppose we wish to approximate the data by an equation of the form</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1x + \beta_2x^2.\]</div>
<p>Describe the linear model that produces a “least squares fit” of the data by the equation.</p>
<p><strong>Solution.</strong>  The ideal relationship is <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1x + \beta_2x^2.\)</span></p>
<p>Suppose the actual values of the parameters are <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \beta_2.\)</span>  Then the coordinates of the first data point satisfy the equation</p>
<div class="math notranslate nohighlight">
\[y_1 = \beta_0 + \beta_1x_1 + \beta_2x_1^2 + \epsilon_1\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon_1\)</span> is the residual error between the observed value <span class="math notranslate nohighlight">\(y_1\)</span> and the predicted <span class="math notranslate nohighlight">\(y\)</span>-value.</p>
<p>Each data point determines a similar equation:</p>
<div class="math notranslate nohighlight">
\[y_1 = \beta_0 + \beta_1x_1 + \beta_2x_1^2 + \epsilon_1\]</div>
<div class="math notranslate nohighlight">
\[y_2 = \beta_0 + \beta_1x_2 + \beta_2x_2^2 + \epsilon_2\]</div>
<div class="math notranslate nohighlight">
\[\vdots\]</div>
<div class="math notranslate nohighlight">
\[y_n = \beta_0 + \beta_1x_n + \beta_2x_n^2 + \epsilon_n\]</div>
<p>Clearly, this system can be written as <span class="math notranslate nohighlight">\(\mathbf{y} = X\mathbf{\beta} + \mathbf{\epsilon}.\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix} = \begin{bmatrix}1&amp;x_1&amp;x_1^2\\1&amp;x_2&amp;x_2^2\\\vdots&amp;\vdots&amp;\vdots\\1&amp;x_n&amp;x_n^2\end{bmatrix} \begin{bmatrix}\beta_0\\\beta_1\\\beta_2\end{bmatrix} + \begin{bmatrix}\epsilon_1\\\epsilon_2\\\vdots\\\epsilon_n\end{bmatrix}\end{split}\]</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/9441c66fc9f7a46458f8d1842dc80a93f32f041ce40aef51b63e66d59141862f.png" src="_images/9441c66fc9f7a46458f8d1842dc80a93f32f041ce40aef51b63e66d59141862f.png" />
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/35b0284c5d07e1dbd66316cdca236fd7561b6909e1515c53afb165dc4b9180f0.png" src="_images/35b0284c5d07e1dbd66316cdca236fd7561b6909e1515c53afb165dc4b9180f0.png" />
</div>
</div>
<div class="toggle docutils container">
<p>Question Time!  Q23.2</p>
</div>
</section>
<section id="multiple-regression">
<h2>Multiple Regression<a class="headerlink" href="#multiple-regression" title="Permalink to this heading">#</a></h2>
<p>Suppose an experiment involves two independent variables – say, <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span>, – and one dependent variable, <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>A linear equation for predicting <span class="math notranslate nohighlight">\(y\)</span> from <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span> has the form</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1 u + \beta_2 v\]</div>
<p>Since there is more than one independent variable, this is called <strong>multiple regression.</strong></p>
<p>A more general prediction equation might have the form</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1 u + \beta_2 v + \beta_3u^2 + \beta_4 uv + \beta_5 v^2\]</div>
<p>A least squares fit to equations like this is called a <strong>trend surface.</strong></p>
<p>In general, a linear model will arise whenever <span class="math notranslate nohighlight">\(y\)</span> is to be predicted by an equation of the form</p>
<div class="math notranslate nohighlight">
\[y = \beta_0f_0(u,v) + \beta_1f_1(u,v) + \cdots + \beta_kf_k(u,v)\]</div>
<p>with <span class="math notranslate nohighlight">\(f_0,\dots,f_k\)</span> any sort of known functions and <span class="math notranslate nohighlight">\(\beta_0,...,\beta_k\)</span> unknown weights.</p>
<p><strong>Example.</strong>  In geography, local models of terrain are constructed from data <span class="math notranslate nohighlight">\((u_1, v_1, y_1), \dots, (u_n, v_n, y_n)\)</span> where <span class="math notranslate nohighlight">\(u_j, v_j\)</span>, and <span class="math notranslate nohighlight">\(y_j\)</span> are latitude, longitude, and altitude, respectively.</p>
<p>Let’s take an example.   Here are a set of points in <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/40798f8f29b1217d24bed5cca443828b60d0a409672b2df9ce7d8fe09c203b68.png" src="_images/40798f8f29b1217d24bed5cca443828b60d0a409672b2df9ce7d8fe09c203b68.png" />
</div>
</div>
<p>Let’s describe the linear models that gives a least-squares fit to such data.  The solution is called the least-squares <em>plane.</em></p>
<p><strong>Solution.</strong>  We expect the data to satisfy these equations:</p>
<div class="math notranslate nohighlight">
\[y_1 = \beta_0 + \beta_1 u_1 + \beta_2 v_1 + \epsilon_1\]</div>
<div class="math notranslate nohighlight">
\[y_1 = \beta_0 + \beta_1 u_2 + \beta_2 v_2 + \epsilon_2\]</div>
<div class="math notranslate nohighlight">
\[\vdots\]</div>
<div class="math notranslate nohighlight">
\[y_1 = \beta_0 + \beta_1 u_n + \beta_2 v_n + \epsilon_n\]</div>
<p>This system has the matrix for <span class="math notranslate nohighlight">\(\mathbf{y} = X\mathbf{\beta} + \epsilon,\)</span> where</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{y} = \begin{bmatrix}y_1\\y_1\\\vdots\\y_n\end{bmatrix},\;\;X = \begin{bmatrix}1&amp;u_1&amp;v_1\\1&amp;u_2&amp;v_2\\\vdots&amp;\vdots&amp;\vdots\\1&amp;u_n&amp;v_n\end{bmatrix},\;\;\mathbf{\beta}=\begin{bmatrix}\beta_0\\\beta_1\\\beta_2\end{bmatrix},\;\;\epsilon = \begin{bmatrix}\epsilon_1\\\epsilon_2\\\vdots\\\epsilon_n\end{bmatrix}\end{split}\]</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/d03ecf396d86bdc77d31f5b466eaef3a62f0720b49339d8a8b02bac2ada488b2.png" src="_images/d03ecf396d86bdc77d31f5b466eaef3a62f0720b49339d8a8b02bac2ada488b2.png" />
</div>
</div>
<p>This example shows that the linear model for multiple regression has the same abstract form as the model for the simple regression in the earlier examples.</p>
<p>We can see that there the general principle is the same across all the different kinds of linear models.</p>
<p>Once <span class="math notranslate nohighlight">\(X\)</span> is defined properly, the normal equations for <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> have the same matrix form, no matter how many variables are involved.</p>
<p>Thus, for any linear model where <span class="math notranslate nohighlight">\(X^TX\)</span> is invertible, the least squares <span class="math notranslate nohighlight">\(\hat{\mathbf{\beta}}\)</span> is given by <span class="math notranslate nohighlight">\((X^TX)^{-1}X^T\mathbf{y}\)</span>.</p>
</section>
<section id="multiple-regression-in-practice">
<h2>Multiple Regression in Practice<a class="headerlink" href="#multiple-regression-in-practice" title="Permalink to this heading">#</a></h2>
<p>Let’s see how powerful multiple regression can be on a real-world example.</p>
<p>A typical application of linear models is predicting house prices.   Linear models have been used for this problem for decades, and when a municipality does a value assessment on your house, they typically use a linear model.</p>
<p>We can consider various measurable attributes of a house (its “features”) as the independent variables, and the most recent sale price of the house as the dependent variable.</p>
<p>For our case study, we will use the features:</p>
<ul class="simple">
<li><p>Lot Area (sq ft),</p></li>
<li><p>Gross Living Area (sq ft),</p></li>
<li><p>Number of Fireplaces,</p></li>
<li><p>Number of Full Baths,</p></li>
<li><p>Number of Half Baths,</p></li>
<li><p>Garage Area (sq ft),</p></li>
<li><p>Basement Area (sq ft)</p></li>
</ul>
<p>So our design matrix will have 8 columns (including the constant for the intercept):</p>
<div class="math notranslate nohighlight">
\[ X\beta = \mathbf{y}\]</div>
<p>and it will have one row for each house in the data set, with <span class="math notranslate nohighlight">\(y\)</span> the sale price of the house.</p>
<p>We will use data from housing sales in Ames, Iowa from 2006 to 2009:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/ames-housing-data/train.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;LotArea&#39;</span><span class="p">,</span> <span class="s1">&#39;GrLivArea&#39;</span><span class="p">,</span> <span class="s1">&#39;Fireplaces&#39;</span><span class="p">,</span> <span class="s1">&#39;FullBath&#39;</span><span class="p">,</span> <span class="s1">&#39;HalfBath&#39;</span><span class="p">,</span> <span class="s1">&#39;GarageArea&#39;</span><span class="p">,</span> <span class="s1">&#39;TotalBsmtSF&#39;</span><span class="p">,</span> <span class="s1">&#39;SalePrice&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LotArea</th>
      <th>GrLivArea</th>
      <th>Fireplaces</th>
      <th>FullBath</th>
      <th>HalfBath</th>
      <th>GarageArea</th>
      <th>TotalBsmtSF</th>
      <th>SalePrice</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8450</td>
      <td>1710</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>548</td>
      <td>856</td>
      <td>208500</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9600</td>
      <td>1262</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>460</td>
      <td>1262</td>
      <td>181500</td>
    </tr>
    <tr>
      <th>2</th>
      <td>11250</td>
      <td>1786</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>608</td>
      <td>920</td>
      <td>223500</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9550</td>
      <td>1717</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>642</td>
      <td>756</td>
      <td>140000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>14260</td>
      <td>2198</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>836</td>
      <td>1145</td>
      <td>250000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_no_intercept</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;LotArea&#39;</span><span class="p">,</span> <span class="s1">&#39;GrLivArea&#39;</span><span class="p">,</span> <span class="s1">&#39;Fireplaces&#39;</span><span class="p">,</span> <span class="s1">&#39;FullBath&#39;</span><span class="p">,</span> <span class="s1">&#39;HalfBath&#39;</span><span class="p">,</span> <span class="s1">&#39;GarageArea&#39;</span><span class="p">,</span> <span class="s1">&#39;TotalBsmtSF&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;SalePrice&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
<p>Next we add a column of 1s to the design matrix, which adds a constant intercept to the model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X_no_intercept</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="s1">&#39;int&#39;</span><span class="p">),</span> <span class="n">X_no_intercept</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[    1,  8450,  1710, ...,     1,   548,   856],
       [    1,  9600,  1262, ...,     0,   460,  1262],
       [    1, 11250,  1786, ...,     1,   608,   920],
       ...,
       [    1,  9042,  2340, ...,     0,   252,  1152],
       [    1,  9717,  1078, ...,     0,   240,  1078],
       [    1,  9937,  1256, ...,     1,   276,  1256]])
</pre></div>
</div>
</div>
</div>
<p>Now let’s peform the least-squares regression:</p>
<div class="math notranslate nohighlight">
\[\hat{\beta} = (X^TX)^{-1}X^T \mathbf{y}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<p>What does our model tell us?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">beta_hat</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-2.92338280e+04,  1.87444579e-01,  3.94185205e+01,  1.45698657e+04,
        2.29695596e+04,  1.62834807e+04,  9.14770980e+01,  5.11282216e+01])
</pre></div>
</div>
</div>
</div>
<p>We see that we have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span>: Intercept of -$29,233</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1\)</span>: Marginal value of one square foot of Lot Area: $18</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_2\)</span>: Marginal value of one square foot of Gross Living Area: $39</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_3\)</span>: Marginal value of one additional fireplace: $14,570</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_4\)</span>: Marginal value of one additional full bath: $22,970</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_5\)</span>: Marginal value of one additional half bath: $16,283</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_6\)</span>: Marginal value of one square foot of Garage Area: $91</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_7\)</span>: Marginal value of one square foot of Basement Area: $51</p></li>
</ul>
<p>Is our model doing a good job?</p>
<p>There are many statistics for testing this question, but we’ll just look at the predictions versus the ground truth.</p>
<p>For each house we compute its predicted sale value according to our model:</p>
<div class="math notranslate nohighlight">
\[\hat{\mathbf{y}} = X\hat{\beta}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
</pre></div>
</div>
</div>
</div>
<p>And for each house, we’ll plot its predicted versus actual sale value:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/a58c13127eb911683cedb1e51deb15ec8f62c25f3c20b495357f52a1e63e5e94.png" src="_images/a58c13127eb911683cedb1e51deb15ec8f62c25f3c20b495357f52a1e63e5e94.png" />
</div>
</div>
<p>We see that the model does a reasonable job for house values less than about $250,000.</p>
<p>For a better model, we’d want to consider more features of each house, and perhaps some additional functions such as polynomials as components of our model.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="L22LeastSquares.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Least Squares</p>
      </div>
    </a>
    <a class="right-next"
       href="L24SymmetricMatrices.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Symmetric Matrices</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-problems">Regression Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-framework-of-linear-models">The Framework of Linear Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-a-line-to-data">Fitting a Line to Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-least-squares-problem">A Least-Squares Problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-general-linear-model">The General Linear Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-fitting-of-other-models">Least-Squares Fitting of Other Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-regression">Multiple Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-regression-in-practice">Multiple Regression in Practice</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mark Crovella
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2020-2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>