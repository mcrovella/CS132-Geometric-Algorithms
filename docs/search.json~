[
  {
    "objectID": "L08MatrixofLinearTransformations.html",
    "href": "L08MatrixofLinearTransformations.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nIn the last lecture we introduced the idea of a linear transformation:\n\n\nWe have seen that every matrix multiplication is a linear transformation from vectors to vectors.\n\n\nBut, are there any other possible linear transformations from vectors to vectors?\n\n\nNo. \nIn other words, the reverse statement is also true:\n\n every linear transformation from vectors to vectors is a matrix multiplication. \n\n\nWe’ll now prove this fact.\nWe’ll do it constructively, meaning we’ll actually show how to find the matrix corresponding to any given linear transformation \\(T\\).\n\nTheorem. Let \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) be a linear transformation. Then there is (always) a unique matrix \\(A\\) such that:\n\\[ T({\\bf x}) = A{\\bf x} \\;\\;\\; \\text{for all}\\; {\\bf x} \\in \\mathbb{R}^n.\\]\n\n\nIn fact, \\(A\\) is the \\(m \\times n\\) matrix whose \\(j\\)th column is the vector \\(T({\\bf e_j})\\), where \\({\\bf e_j}\\) is the \\(j\\)th column of the identity matrix in \\(\\mathbb{R}^n\\):\n\\[A = \\left[T({\\bf e_1}) \\dots T({\\bf e_n})\\right].\\]\n\\(A\\) is called the standard matrix of \\(T\\).\n\n\nProof. Write\n\\[{\\bf x} = I{\\bf x} = \\left[{\\bf e_1} \\dots {\\bf e_n}\\right]\\bf x\\]\n\n\n\\[ = x_1{\\bf e_1} + \\dots + x_n{\\bf e_n}.\\]\n\n\nIn other words, for any \\(\\mathbf{x}\\), we can always expand it as:\n\\[ \\mathbf{x}\n\\;= \\;\\;\\;\\;\\;\n\\begin{bmatrix}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & \\dots & 1\n\\end{bmatrix} \\; \\begin{bmatrix}x_1\\\\x_2\\\\ \\vdots \\\\ x_n\\end{bmatrix}\n\\;\\;\\;\\;\\;= \\;\\;\\;\\;\\;\n\\begin{bmatrix} x_1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} +\n\\begin{bmatrix} 0 \\\\ x_2 \\\\ \\vdots \\\\ 0 \\end{bmatrix} +\n\\dots +\n\\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\]\n\n\nBecause \\(T\\) is linear, we have:\n\\[ T({\\bf x}) = T(x_1{\\bf e_1} + \\dots + x_n{\\bf e_n})\\]\n\n\n\\[ = x_1T({\\bf e_1}) + \\dots + x_nT({\\bf e_n})\\]\n\n\n\\[ = \\left[T({\\bf e_1}) \\dots T({\\bf e_n})\\right] \\, \\left[\\begin{array}{r}x_1\\\\\\vdots\\\\x_n\\end{array}\\right] = A{\\bf x}.\\]\n\n\nSo … we see that the ideas of matrix multiplication and linear transformation are essentially equivalent when applied to vectors.\nEvery matrix multiplication is a linear transformation, and every linear transformation from vectors to vectors is a matrix multiplication.\nHowever, term linear transformation focuses on a property of the mapping, while the term matrix multiplication focuses on how such a mapping is implemented.\n\nThis proof shows us an important idea:\n To find the standard matrix of a linear transformation, ask what the transformation does to the columns of \\(I\\).\nIn other words, if \\(T(\\mathbf{x}) = A\\mathbf{x}\\), then:\n\\[A = \\left[T({\\bf e_1}) \\dots T({\\bf e_n})\\right].\\]\nThis gives us a way to compute the standard matrix of a transformation.\nNow, in \\(\\mathbb{R}^2\\), \\(I = \\left[\\begin{array}{cc}1&0\\\\0&1\\end{array}\\right]\\). So:\n\\[\\mathbf{e_1} = \\left[\\begin{array}{c}1\\\\0\\end{array}\\right]\\;\\;\\text{and}\\;\\;\\mathbf{e_2} = \\left[\\begin{array}{c}0\\\\1\\end{array}\\right].\\]\nSo to find the matrix of any given linear transformation of vectors in \\(\\mathbb{R}^2\\), we only have to know what that transformation does to these two points:\n\n\n\n\n\n\n\n\n\nThis is a hugely powerful tool.\nLet’s say we start from some given linear transformation; we can use this idea to find the matrix that implements that linear transformation.\n\nFor example, let’s consider rotation about the origin as a kind of transformation.\n\n\n\n\n\n\n\n\n\n\n\nFirst things first: Is rotation a linear transformation?\n\n\nRecall that a for a transformation to be linear, it must be true that \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v}).\\)\n\n\nI’m going to show you a “geometric proof.”\nThis figure shows that “the rotation of \\(\\mathbf{u+v}\\) is the sum of the rotation of \\(\\mathbf{u}\\) and the rotation of \\(\\mathbf{v}\\)”.\n\n\n\n\n\n\n\n\n\n\nOK, so rotation is a linear transformation.\nLet’s see how to compute the linear transformation that is a rotation.\n\nSpecifically: Let \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) be the transformation that rotates each point in \\(\\mathbb{R}^2\\) about the origin through an angle \\(\\theta\\), with counterclockwise rotation for a positive angle.\nLet’s find the standard matrix \\(A\\) of this transformation.\n\n\nSolution. The columns of \\(I\\) are \\({\\bf e_1} = \\left[\\begin{array}{r}1\\\\0\\end{array}\\right]\\) and \\({\\bf e_2} = \\left[\\begin{array}{r}0\\\\1\\end{array}\\right].\\)\n\n\nReferring to the diagram below, we can see that \\(\\left[\\begin{array}{r}1\\\\0\\end{array}\\right]\\) rotates into \\(\\left[\\begin{array}{r}\\cos\\theta\\\\\\sin\\theta\\end{array}\\right],\\) and \\(\\left[\\begin{array}{r}0\\\\1\\end{array}\\right]\\) rotates into \\(\\left[\\begin{array}{r}-\\sin\\theta\\\\\\cos\\theta\\end{array}\\right].\\)\n\n\n\n\n\n\n\n\n\n\n\nSo by the Theorem above,\n\\[ A = \\left[\\begin{array}{rr}\\cos\\theta&-\\sin\\theta\\\\\\sin\\theta&\\cos\\theta\\end{array}\\right].\\]\n\nTo demonstrate the use of a rotation matrix, let’s rotate the following shape:\n\ndm.plotSetup()\nnote = dm.mnote()\ndm.plotShape(note)\n\n\n\n\n\n\n\n\n\nThe variable note is a array of 26 vectors in \\(\\mathbb{R}^2\\) that define its shape.\nIn other words, it is a 2 \\(\\times\\) 26 matrix.\n\n\nTo rotate note we need to multiply each column of note by the rotation matrix \\(A\\).\nIn Python this can be performed using the @ operator.\nThat is, if A and B are matrices,\nA @ B\nwill multiply A by every column of B, and the resulting vectors will be formed into a matrix.\n\ndm.plotSetup()\nangle = 90\ntheta = (angle/180) * np.pi\nA = np.array(\n    [[np.cos(theta), -np.sin(theta)],\n     [np.sin(theta), np.cos(theta)]])\nrnote = A @ note\ndm.plotShape(rnote)",
    "crumbs": [
      "The Matrix of a Linear Transformation"
    ]
  },
  {
    "objectID": "L08MatrixofLinearTransformations.html#the-matrix-of-a-linear-transformation",
    "href": "L08MatrixofLinearTransformations.html#the-matrix-of-a-linear-transformation",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nIn the last lecture we introduced the idea of a linear transformation:\n\n\nWe have seen that every matrix multiplication is a linear transformation from vectors to vectors.\n\n\nBut, are there any other possible linear transformations from vectors to vectors?\n\n\nNo. \nIn other words, the reverse statement is also true:\n\n every linear transformation from vectors to vectors is a matrix multiplication. \n\n\nWe’ll now prove this fact.\nWe’ll do it constructively, meaning we’ll actually show how to find the matrix corresponding to any given linear transformation \\(T\\).\n\nTheorem. Let \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) be a linear transformation. Then there is (always) a unique matrix \\(A\\) such that:\n\\[ T({\\bf x}) = A{\\bf x} \\;\\;\\; \\text{for all}\\; {\\bf x} \\in \\mathbb{R}^n.\\]\n\n\nIn fact, \\(A\\) is the \\(m \\times n\\) matrix whose \\(j\\)th column is the vector \\(T({\\bf e_j})\\), where \\({\\bf e_j}\\) is the \\(j\\)th column of the identity matrix in \\(\\mathbb{R}^n\\):\n\\[A = \\left[T({\\bf e_1}) \\dots T({\\bf e_n})\\right].\\]\n\\(A\\) is called the standard matrix of \\(T\\).\n\n\nProof. Write\n\\[{\\bf x} = I{\\bf x} = \\left[{\\bf e_1} \\dots {\\bf e_n}\\right]\\bf x\\]\n\n\n\\[ = x_1{\\bf e_1} + \\dots + x_n{\\bf e_n}.\\]\n\n\nIn other words, for any \\(\\mathbf{x}\\), we can always expand it as:\n\\[ \\mathbf{x}\n\\;= \\;\\;\\;\\;\\;\n\\begin{bmatrix}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & \\dots & 1\n\\end{bmatrix} \\; \\begin{bmatrix}x_1\\\\x_2\\\\ \\vdots \\\\ x_n\\end{bmatrix}\n\\;\\;\\;\\;\\;= \\;\\;\\;\\;\\;\n\\begin{bmatrix} x_1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} +\n\\begin{bmatrix} 0 \\\\ x_2 \\\\ \\vdots \\\\ 0 \\end{bmatrix} +\n\\dots +\n\\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\]\n\n\nBecause \\(T\\) is linear, we have:\n\\[ T({\\bf x}) = T(x_1{\\bf e_1} + \\dots + x_n{\\bf e_n})\\]\n\n\n\\[ = x_1T({\\bf e_1}) + \\dots + x_nT({\\bf e_n})\\]\n\n\n\\[ = \\left[T({\\bf e_1}) \\dots T({\\bf e_n})\\right] \\, \\left[\\begin{array}{r}x_1\\\\\\vdots\\\\x_n\\end{array}\\right] = A{\\bf x}.\\]\n\n\nSo … we see that the ideas of matrix multiplication and linear transformation are essentially equivalent when applied to vectors.\nEvery matrix multiplication is a linear transformation, and every linear transformation from vectors to vectors is a matrix multiplication.\nHowever, term linear transformation focuses on a property of the mapping, while the term matrix multiplication focuses on how such a mapping is implemented.\n\nThis proof shows us an important idea:\n To find the standard matrix of a linear transformation, ask what the transformation does to the columns of \\(I\\).\nIn other words, if \\(T(\\mathbf{x}) = A\\mathbf{x}\\), then:\n\\[A = \\left[T({\\bf e_1}) \\dots T({\\bf e_n})\\right].\\]\nThis gives us a way to compute the standard matrix of a transformation.\nNow, in \\(\\mathbb{R}^2\\), \\(I = \\left[\\begin{array}{cc}1&0\\\\0&1\\end{array}\\right]\\). So:\n\\[\\mathbf{e_1} = \\left[\\begin{array}{c}1\\\\0\\end{array}\\right]\\;\\;\\text{and}\\;\\;\\mathbf{e_2} = \\left[\\begin{array}{c}0\\\\1\\end{array}\\right].\\]\nSo to find the matrix of any given linear transformation of vectors in \\(\\mathbb{R}^2\\), we only have to know what that transformation does to these two points:\n\n\n\n\n\n\n\n\n\nThis is a hugely powerful tool.\nLet’s say we start from some given linear transformation; we can use this idea to find the matrix that implements that linear transformation.\n\nFor example, let’s consider rotation about the origin as a kind of transformation.\n\n\n\n\n\n\n\n\n\n\n\nFirst things first: Is rotation a linear transformation?\n\n\nRecall that a for a transformation to be linear, it must be true that \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v}).\\)\n\n\nI’m going to show you a “geometric proof.”\nThis figure shows that “the rotation of \\(\\mathbf{u+v}\\) is the sum of the rotation of \\(\\mathbf{u}\\) and the rotation of \\(\\mathbf{v}\\)”.\n\n\n\n\n\n\n\n\n\n\nOK, so rotation is a linear transformation.\nLet’s see how to compute the linear transformation that is a rotation.\n\nSpecifically: Let \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) be the transformation that rotates each point in \\(\\mathbb{R}^2\\) about the origin through an angle \\(\\theta\\), with counterclockwise rotation for a positive angle.\nLet’s find the standard matrix \\(A\\) of this transformation.\n\n\nSolution. The columns of \\(I\\) are \\({\\bf e_1} = \\left[\\begin{array}{r}1\\\\0\\end{array}\\right]\\) and \\({\\bf e_2} = \\left[\\begin{array}{r}0\\\\1\\end{array}\\right].\\)\n\n\nReferring to the diagram below, we can see that \\(\\left[\\begin{array}{r}1\\\\0\\end{array}\\right]\\) rotates into \\(\\left[\\begin{array}{r}\\cos\\theta\\\\\\sin\\theta\\end{array}\\right],\\) and \\(\\left[\\begin{array}{r}0\\\\1\\end{array}\\right]\\) rotates into \\(\\left[\\begin{array}{r}-\\sin\\theta\\\\\\cos\\theta\\end{array}\\right].\\)\n\n\n\n\n\n\n\n\n\n\n\nSo by the Theorem above,\n\\[ A = \\left[\\begin{array}{rr}\\cos\\theta&-\\sin\\theta\\\\\\sin\\theta&\\cos\\theta\\end{array}\\right].\\]\n\nTo demonstrate the use of a rotation matrix, let’s rotate the following shape:\n\ndm.plotSetup()\nnote = dm.mnote()\ndm.plotShape(note)\n\n\n\n\n\n\n\n\n\nThe variable note is a array of 26 vectors in \\(\\mathbb{R}^2\\) that define its shape.\nIn other words, it is a 2 \\(\\times\\) 26 matrix.\n\n\nTo rotate note we need to multiply each column of note by the rotation matrix \\(A\\).\nIn Python this can be performed using the @ operator.\nThat is, if A and B are matrices,\nA @ B\nwill multiply A by every column of B, and the resulting vectors will be formed into a matrix.\n\ndm.plotSetup()\nangle = 90\ntheta = (angle/180) * np.pi\nA = np.array(\n    [[np.cos(theta), -np.sin(theta)],\n     [np.sin(theta), np.cos(theta)]])\nrnote = A @ note\ndm.plotShape(rnote)",
    "crumbs": [
      "The Matrix of a Linear Transformation"
    ]
  },
  {
    "objectID": "L08MatrixofLinearTransformations.html#geometric-linear-transformations-of-mathbbr2",
    "href": "L08MatrixofLinearTransformations.html#geometric-linear-transformations-of-mathbbr2",
    "title": "Geometric Algorithms",
    "section": "Geometric Linear Transformations of \\(\\mathbb{R}^2\\)",
    "text": "Geometric Linear Transformations of \\(\\mathbb{R}^2\\)\nLet’s use our understanding of how to construct linear transformations to look at some specific linear transformations of \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\).\n\nFirst, let’s recall the linear transformation\n\\[T(\\mathbf{x}) = r\\mathbf{x}.\\]\nWith \\(r &gt; 1\\), this is a dilation. It moves every vector further from the origin.\n\n\nLet’s say the dilation is by a factor of 2.5.\nTo construct the matrix \\(A\\) that implements this transformation, we ask: where do \\({\\bf e_1}\\) and \\({\\bf e_2}\\) go?\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnder the action of \\(A\\), \\(\\mathbf{e_1}\\) goes to \\(\\left[\\begin{array}{c}2.5\\\\0\\end{array}\\right]\\) and \\(\\mathbf{e_2}\\) goes to \\(\\left[\\begin{array}{c}0\\\\2.5\\end{array}\\right]\\).\n\n\nSo the matrix \\(A\\) must be \\(\\left[\\begin{array}{cc}2.5&0\\\\0&2.5\\end{array}\\right]\\).\n\n\nLet’s test this out:\n\nsquare = np.array(\n    [[0,1,1,0],\n     [1,1,0,0]])\nA = np.array(\n    [[2.5, 0],\n     [0, 2.5]])\ndisplay(Latex(rf\"$A = {ltx_array_fmt(A, '{:1.1f}')}$\"))\ndm.plotSetup()\ndm.plotSquare(square)\ndm.plotSquare(A @ square,'r')\n\n\\(A = \\begin{bmatrix}\n  2.5 &  0.0\\\\\n  0.0 &  2.5\n\\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\n\n\ndm.plotSetup(-7,7,-7, 7)\ndm.plotShape(note)\ndm.plotShape(A @ note,'r')\n\n\n\n\n\n\n\n\n\n\nOK, now let’s reflect through the \\(x_1\\) axis. Where do \\({\\bf e_1}\\) and \\({\\bf e_2}\\) go?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA = np.array(\n    [[1,  0],\n     [0, -1]])\ndisplay(Latex(rf\"$A = {ltx_array_fmt(A, '{:d}')}$\"))\ndm.plotSetup()\ndm.plotSquare(square)\ndm.plotSquare(A @ square,'r')\nplt.title(r'Reflection through the $x_1$ axis', size = 20);\n\n\\(A = \\begin{bmatrix}\n  1  &  0 \\\\\n  0  & -1\n\\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\n\n\ndm.plotSetup()\ndm.plotShape(note)\ndm.plotShape(A @ note,'r')\n\n\n\n\n\n\n\n\n\n\nWhat about reflection through the \\(x_2\\) axis?\n\n\n\n\n\\(A = \\begin{bmatrix}\n-1  &   0 \\\\\n   0  &   1\n\\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat about reflection through the line \\(x_1 = x_2\\)?\n\n\n\n\n\\(A = \\begin{bmatrix}\n  0  &  1 \\\\\n  1  &  0\n\\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat about reflection through the line \\(x_1 = -x_2\\)?\n\n\n\n\n\\(A = \\begin{bmatrix}\n  0  & -1 \\\\\n-1  &  0\n\\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat about reflection through the origin?\n\n\n\n\n\\(A = \\begin{bmatrix}\n-1  &  0 \\\\\n  0  & -1\n\\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A = \\begin{bmatrix}\n  0.45 &  0.00\\\\\n  0.00 &  1.00\n\\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A = \\begin{bmatrix}\n  1.0 &  0.0\\\\\n-1.5 &  1.0\n\\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s look at a particular kind of transformation called a projection.\nImagine we took any given point and ‘dropped’ it onto the \\(x_1\\)-axis.\n\n\n\n\\(A = \\begin{bmatrix}\n  1  &  0 \\\\\n  0  &  0\n\\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\n\nWhat happens to the shape of the point set?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A = \\begin{bmatrix}\n  0  &  0 \\\\\n  0  &  1\n\\end{bmatrix}\\)",
    "crumbs": [
      "The Matrix of a Linear Transformation"
    ]
  },
  {
    "objectID": "L08MatrixofLinearTransformations.html#area-is-scaled-by-the-determinant",
    "href": "L08MatrixofLinearTransformations.html#area-is-scaled-by-the-determinant",
    "title": "Geometric Algorithms",
    "section": "Area is Scaled by the Determinant",
    "text": "Area is Scaled by the Determinant\n\nNotice that in some of the transformations above, the “size” of a shape grows or shrinks.\nLet’s look at how area (or volume) of a shape is affected by a linear transformation.\n\n\n\\(A = \\begin{bmatrix}\n  0  &  0 \\\\\n  0  &  1\n\\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\n\nIn this transformation, each unit of area in the blue shape is transformed to a smaller region in the red shape.\nSo to understand how area changes, it suffices to ask what happens to the unit square (or hypercube):\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s denote the matrix of our linear transformation as:\n\\[ A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\]\n\nThen, here is what happens to the unit square:\n\n\n\n\n\n\n\n\n\n\n\nNow, let’s determine the area of the blue diamond in terms of \\(a, b, c\\), and \\(d\\).\n\n\nTo do that, we’ll use this diagram:\n\n\n\n\n\n\n\n\n\n\n\nEach of the triangles and rectangles has an area we can determine in terms of \\(a, b, c\\) and \\(d\\).\n\n\nThe large rectangle has sides \\((a+b)\\) and \\((c+d)\\), so its area is:\n\\[ (a+b)(c+d) = ac + ad + bc + bd. \\]\n\n\nFrom this large rectangle we need to subtract:\n\n\\(bd\\) (red triangles),\n\\(ac\\) (gray triangles), and\n\\(2bc\\) (green rectangles).\n\n\n\nSo the area of the blue diamond is:\n\\[ (ac + ad + bc + bd) - (bd + ac + 2bc) \\]\n\\[ = ad - bc \\]\n\n\nSo we conclude that when we use a linear transformation\n\\[ A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\]\nthe area of a unit square (or any shape) is scaled by a factor of \\(ad - bc\\).\n\n\nThis quantity is a fundamental property of the matrix \\(A\\).\nSo, we give it a name: it is the determinant of \\(A\\).\nWe denote it as\n\\[\\det(A)\\]\n\n\nSo, for a \\(2\\times 2\\) matrix \\(A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\),\n\\[\\det(A) = ad-bc.\\]\n\n\nHowever, the determinant can be defined for any \\(n\\times n\\) (square) matrix.\nFor a square matrix \\(A\\) larger than \\(2\\times 2\\), the determinant tells us how the volume of a unit (hyper)cube is scaled when it is linearly transformed by \\(A\\).\n\n\nWe will learn how to compute determinants for larger matrices in a later lecture.\n\nThere are important cases in which the determinant of a matrix is zero.\n\nWhen does it happen that \\(\\det(A) = 0\\)?\nConsider when \\(A\\) is the matrix of a projection:\n\n\n\n\n\\(A = \\begin{bmatrix}\n  1  &  0 \\\\\n  0  &  0\n\\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\n\nThe unit square has been collapsed onto the \\(x\\)-axis, resulting in a shape with area of zero.\nThis is confirmed by the determinant, which is\n\\[ \\det\\left(\\begin{bmatrix}1 & 0 \\\\ 0 & 0\\end{bmatrix}\\right) = (1 \\cdot 0) - (0 \\cdot 0) = 0.\\]",
    "crumbs": [
      "The Matrix of a Linear Transformation"
    ]
  },
  {
    "objectID": "L08MatrixofLinearTransformations.html#existence-and-uniqueness",
    "href": "L08MatrixofLinearTransformations.html#existence-and-uniqueness",
    "title": "Geometric Algorithms",
    "section": "Existence and Uniqueness",
    "text": "Existence and Uniqueness\nNotice that some of these transformations map multiple inputs to the same output, and some are incapable of generating certain outputs.\nFor example, the projections above can send multiple different points to the same point.\nWe need some terminology to understand these properties of linear transformations.\n\nDefinition. A mapping \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) is said to be onto \\(\\mathbb{R}^m\\) if each \\(\\mathbf{b}\\) in \\(\\mathbb{R}^m\\) is the image of at least one \\(\\mathbf{x}\\) in \\(\\mathbb{R}^n\\).\nIn other words, \\(T\\) is onto if every element of its codomain is in its range.\n\n\nAnother (important) way of thinking about this is that \\(T\\) is onto if there is a solution \\(\\mathbf{x}\\) of\n\\[T(\\mathbf{x}) = \\mathbf{b}\\]\nfor all possible \\(\\mathbf{b}.\\)\nThis is asking an existence question about a solution of the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) for all \\(\\mathbf{b}.\\)\n\n\nHere, we see that \\(T\\) maps points in \\(\\mathbb{R}^2\\) to a plane lying within \\(\\mathbb{R}^3\\).\nThat is, the range of \\(T\\) is a strict subset of the codomain of \\(T\\).\nSo \\(T\\) is not onto \\(\\mathbb{R}^3\\).\n\nIn this case, for every point in \\(\\mathbb{R}^2\\), there is an \\(\\mathbf{x}\\) that maps to that point.\nSo, the range of \\(T\\) is equal to the codomain of \\(T\\).\nSo \\(T\\) is onto \\(\\mathbb{R}^2\\).\nHere is an example of the reflection transformation. The red points are the images of the blue points.\nWhat about this transformation? Is it onto \\(\\mathbb{R}^2\\)?\n\n\n\n\n\n\n\n\n\nHere is an example of the projection transformation. The red points (which all lie on the \\(x\\)-axis) are the images of the blue points.\nWhat about this transformation? Is it onto \\(\\mathbb{R}^2\\)?\n\n\n\n\n\n\n\n\n\nDefinition. A mapping \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) is said to be one-to-one if each \\(\\mathbf{b}\\) in \\(\\mathbb{R}^m\\) is the image of at most one \\(\\mathbf{x}\\) in \\(\\mathbb{R}^n\\).\n\nIf \\(T\\) is one-to-one, then for each \\(\\mathbf{b},\\) the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has either a unique solution, or none at all.\nThis is asking a uniqueness question about a solution of the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) for all \\(\\mathbf{b}\\).\n\n\n\nImage from Linear Algebra and its Applications, by David C. Lay\n\n\n\n\nLet’s examine the relationship between these ideas and some previous definitions.\nIf for all \\(\\mathbf{b}\\), \\(A\\mathbf{x} = \\mathbf{b}\\) is consistent, is \\(T(\\mathbf{x}) = A\\mathbf{x}\\) onto? one-to-one?\n\n\n\n\\(T(\\mathbf{x})\\) is onto. \\(T(\\mathbf{x})\\) may or may not be one-to-one. If the system has multiple solutions for some \\(\\mathbf{b}\\), \\(T(\\mathbf{x})\\) is not one-to-one.\n\n\nIf for all \\(\\mathbf{b}\\), \\(A\\mathbf{x} = \\mathbf{b}\\) is consistent and has a unique solution, is \\(T(\\mathbf{x}) = A\\mathbf{x}\\) onto? one-to-one?\n\n\nYes to both.\n\n\nIf it is not the case that for all \\(\\mathbf{b}\\), \\(A\\mathbf{x} = \\mathbf{b}\\) is consistent, is \\(T(\\mathbf{x}) = A\\mathbf{x}\\) onto? one-to-one?\n\n\n\\(T(\\mathbf{x})\\) is not onto. \\(T(\\mathbf{x})\\) may or may not be one-to-one.\n\n\nIf \\(T(\\mathbf{x}) = A\\mathbf{x}\\) is onto, is \\(A\\mathbf{x} = \\mathbf{b}\\) consistent for all \\(\\mathbf{b}\\)? Is the solution unique for all \\(\\mathbf{b}\\)?\n\n\n\\(A\\mathbf{x} = \\mathbf{b}\\) is consistent for all \\(\\mathbf{b}\\). The solution may not be unique for any \\(\\mathbf{b}\\).\n\n\nIf \\(T(\\mathbf{x}) = A\\mathbf{x}\\) is one-to-one, is \\(A\\mathbf{x} = \\mathbf{b}\\) consistent for all \\(\\mathbf{b}\\)? Is the solution unique for all \\(\\mathbf{b}\\)?\n\n\n\\(A\\mathbf{x} = \\mathbf{b}\\) may or may not be consistent for all \\(\\mathbf{b}\\). For any \\(\\mathbf{b}\\), if there is a solution, it is unique.",
    "crumbs": [
      "The Matrix of a Linear Transformation"
    ]
  },
  {
    "objectID": "L04VectorEquations.html",
    "href": "L04VectorEquations.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#vector-equations",
    "href": "L04VectorEquations.html#vector-equations",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#a-shift-in-perspective",
    "href": "L04VectorEquations.html#a-shift-in-perspective",
    "title": "Geometric Algorithms",
    "section": "A Shift in Perspective",
    "text": "A Shift in Perspective\n\nWe’re going to expand our view of what a linear system can represent.\n\n\nInstead of thinking of it as a collection of equations, we are going to think about it as a single equation.\nThis is a major shift in perspective that will open up an entirely new way of thinking about matrices.\n(…and it’s going to open the door to computer graphics, machine learning, and statistics … later on!)\n\n\nTo make this fundamental shift we need to introduce the idea of a vector. We’ll mainly talk about vectors today.\nWe’ll then return to thinking about a linear system – now interpreted as a vector equation – in the next lecture.",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#vectors",
    "href": "L04VectorEquations.html#vectors",
    "title": "Geometric Algorithms",
    "section": "Vectors",
    "text": "Vectors\nA matrix with only one column is called a column vector, or simply a vector.\nHere are some examples:\n\\[\n\\begin{array}{ccc}\n{\\bf u} = \\left[\\begin{array}{r}3\\\\-1\\end{array}\\right] &\n{\\bf v} = \\left[\\begin{array}{c}.2\\\\.3\\end{array}\\right] &\n{\\bf w} = \\left[\\begin{array}{c}w_1\\\\w_2\\end{array}\\right]\n\\end{array}\n\\]\nWe will always use boldface for vectors – so you can tell that \\(\\mathbf{u}\\) is a vector, while \\(u\\) is just an ordinary number.\nWe will use the term scalar to refer to ordinary numbers (not vectors).\n\nThe numbers within the vector are called components.\nBecause each of these vectors has two components, we say these vectors are in \\(\\mathbb{R}^2.\\)\n\n\nThere is an important notational point here:\nWhen we write \\(\\mathbb{R}^n\\), we mean all the vectors that have exactly \\(n\\) components.\n\n\nHere are some vectors in \\(\\mathbb{R}^3\\):\n\\[\n\\begin{array}{ccc}\n{\\bf u} = \\left[\\begin{array}{c}2\\\\3\\\\4\\end{array}\\right] &\n{\\bf v} = \\left[\\begin{array}{r}-1\\\\0\\\\2\\end{array}\\right] &\n{\\bf w} = \\left[\\begin{array}{c}w_1\\\\w_2\\\\w_3\\end{array}\\right]\n\\end{array}\n\\]",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#vectors-are-fundamental-objects",
    "href": "L04VectorEquations.html#vectors-are-fundamental-objects",
    "title": "Geometric Algorithms",
    "section": "Vectors are Fundamental Objects",
    "text": "Vectors are Fundamental Objects\nWe are going to define operations over vectors, so that we can write equations in terms of vectors.\nIn particular, we now define how to compare vectors, add vectors, and multiply vectors by a scalar.\n\nFirst: Two vectors are equal if and only if their corresponding entries are equal.\nThus \\(\\left[\\begin{array}{c}7\\\\4\\end{array}\\right]\\) and \\(\\left[\\begin{array}{c}4\\\\7\\end{array}\\right]\\) are not equal.\n\n\nNext: Multiplying a vector by a scalar is accomplished by multiplying each entry by the scalar.\nFor example:\n\\[3 \\left[\\begin{array}{r}1\\\\-2\\end{array}\\right] = \\left[\\begin{array}{r}3\\\\-6\\end{array}\\right]\\]\n\n\nAnd finally: the sum of two vectors is the vector whose entries are the corresponding sums.\nFor example:\n\\[\\left[\\begin{array}{r}1\\\\-2\\end{array}\\right] + \\left[\\begin{array}{c}2\\\\5\\end{array}\\right] = \\left[\\begin{array}{r}1 + 2\\\\-2 + 5\\end{array}\\right] = \\left[\\begin{array}{c}3\\\\3\\end{array}\\right].\\]\n\nNote that vectors of different sizes cannot be compared or added.\nFor example, if \\({\\bf u} \\in \\mathbb{R}^2\\) and \\({\\bf v} \\in \\mathbb{R}^3\\):\n\nwe cannot ask whether \\({\\bf u} = {\\bf v}\\), and\n\\({\\bf u} + {\\bf v}\\) is undefined.\n\n\nSo with these three definitions, we have all the tools to write equations using vectors.\nFor example, if \\({\\bf u} = \\left[\\begin{array}{r}1\\\\-2\\end{array}\\right]\\) and \\({\\bf v} = \\left[\\begin{array}{r}2\\\\-5\\end{array}\\right]\\) then\n\\[ 4{\\bf u} - 3{\\bf v} = \\left[\\begin{array}{r}-2\\\\7\\end{array}\\right]\\]",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#vectors-correspond-to-points",
    "href": "L04VectorEquations.html#vectors-correspond-to-points",
    "title": "Geometric Algorithms",
    "section": "Vectors Correspond to Points",
    "text": "Vectors Correspond to Points\n\nAs already noted, an ordered sequence of \\(n\\) numbers can be thought of as a point in \\(n\\)-dimensional space.\n\n\nHence, a vector like \\(\\left[\\begin{array}{c}-2\\\\-1\\end{array}\\right]\\) can be thought of as a point on the plane.\nWe will sometimes write vectors as a list, like this: \\((-2,-1).\\)\n(which lists the vector components from top to bottom).\n\n\n\n\n\n\n\n\n\n\nSometimes we draw an arrow from the origin to the point.\nThis style comes from physics, but can be a helpful visualization in any case.",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#vector-addition-geometrically",
    "href": "L04VectorEquations.html#vector-addition-geometrically",
    "title": "Geometric Algorithms",
    "section": "Vector Addition, Geometrically",
    "text": "Vector Addition, Geometrically\n\nA geometric interpretation of vector sum is as a parallelogram.\n\n\nIf \\({\\bf u}\\) and \\({\\bf v}\\) in \\(\\mathbb{R}^2\\) are represented as points in the plane, then \\({\\bf u} + {\\bf v}\\) corresponds to the fourth vertex of the parallelogram whose other vertices are \\({\\bf u}, 0,\\) and \\({\\bf v}\\).\n\n\n\n\n\n\n\n\n\nThis should be clear from the definition of vector addition (i.e., addition of corresponding elements).",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#vector-scaling-geometrically",
    "href": "L04VectorEquations.html#vector-scaling-geometrically",
    "title": "Geometric Algorithms",
    "section": "Vector Scaling, Geometrically",
    "text": "Vector Scaling, Geometrically\n\nFor a given vector \\({\\bf v}\\) and a scalar \\(a\\), multiplying \\(a\\) and \\({\\bf v}\\) corresponds to lengthening \\(\\bf v\\) by a factor of \\(a\\).\n\n\nSo \\(2\\bf v\\) is twice as long as \\(\\bf v\\).\nMultiplying by a negative value reverses the “direction” of \\(\\bf v\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#the-algebra-of-mathbbrn",
    "href": "L04VectorEquations.html#the-algebra-of-mathbbrn",
    "title": "Geometric Algorithms",
    "section": "The Algebra of \\(\\mathbb{R}^n\\)",
    "text": "The Algebra of \\(\\mathbb{R}^n\\)\nWe’ve defined a new mathematical object (the vector), and it has certain algebraic properties.\n\n\n\\({\\bf u} + {\\bf v} = {\\bf v} + {\\bf u}\\)\n\\(({\\bf u} + {\\bf v}) + {\\bf w} = {\\bf u} + ({\\bf v} + {\\bf w})\\)\n\\({\\bf u} + {\\bf 0} = {\\bf 0} + {\\bf u} = {\\bf u}\\)\n\\({\\bf u} + ({\\bf -u}) = {\\bf -u} + {\\bf u} = {\\bf 0}\\)\n\\(c({\\bf u} + {\\bf v}) = c{\\bf u} + c{\\bf v}\\)\n\\((c+d){\\bf u} = c{\\bf u} + d{\\bf u}\\)\n\\(c(d{\\bf u}) = (cd){\\bf u}\\)\n\\(1{\\bf u} = {\\bf u}\\)\n\nYou can verify each of these by from the definitions of vector addition and scalar-vector multiplication.",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#linear-combinations",
    "href": "L04VectorEquations.html#linear-combinations",
    "title": "Geometric Algorithms",
    "section": "Linear Combinations",
    "text": "Linear Combinations\nA fundamental thing we will do is to construct linear combinations of vectors:\n\\[ {\\bf y} = c_1{\\bf v_1} + ... + c_p{\\bf v_p} \\]\n\nThe \\(c_i\\) values are called weights. Weights can be any real number, including zero. So some examples of linear combinations of \\(\\mathbf{v_1}\\) and \\(\\mathbf{v_2}\\) are:\n\\[ \\sqrt{3}{\\bf v_1} + {\\bf v_2}, \\]\n\\[ \\frac{1}{2}{\\bf v_1} \\;\\; (= \\frac{1}{2}{\\bf v_1} + 0{\\bf v_2}) \\] and\n\\[ {\\bf 0} \\;\\;(= 0{\\bf v_1} + 0{\\bf v_2}) \\]",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#linear-combinations-geometrically",
    "href": "L04VectorEquations.html#linear-combinations-geometrically",
    "title": "Geometric Algorithms",
    "section": "Linear Combinations, Geometrically",
    "text": "Linear Combinations, Geometrically\n\n\n\n\n\n\n\n\n\n\n\\[{\\bf w} = 2{\\bf u} + {\\bf v}\\]\n\n\n\\[{\\bf p} = -{\\bf u} - {\\bf v}\\]\n\n\n\\[{\\bf q} = -{\\bf u} + 3{\\bf v}\\]",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#a-fundamental-question",
    "href": "L04VectorEquations.html#a-fundamental-question",
    "title": "Geometric Algorithms",
    "section": "A Fundamental Question",
    "text": "A Fundamental Question\nWe are now going to take up a very basic question that will lead us to a deeper understanding of linear systems.\n\n\nGiven some set of vectors \\({\\bf a_1, a_2, ..., a_k}\\), can a given vector \\(\\bf b\\) be written as a linear combination of \\({\\bf a_1, a_2, ..., a_k}\\)?\n\n\n\nLet’s take a specific example.\nLet \\({\\bf a_1} = \\left[\\begin{array}{c}1\\\\-2\\\\-5\\end{array}\\right], {\\bf a_2} = \\left[\\begin{array}{c}2\\\\5\\\\6\\end{array}\\right],\\) and \\({\\bf b} = \\left[\\begin{array}{c}7\\\\4\\\\-3\\end{array}\\right]\\).\nWe need to determine whether \\({\\bf b}\\) can be generated as a linear combination of \\({\\bf a_1}\\) and \\({\\bf a_2}\\). That is, we seek to find whether weights \\(x_1\\) and \\(x_2\\) exist such that\n\\[ x_1{\\bf a_1} + x_2{\\bf a_2} = {\\bf b}.\\]\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nSolution. We are going to convert from a single vector equation to a set of linear equations, that is, a linear system.\nWe start by writing the form of the solution, if it exists:\n\\[ x_1{\\bf a_1} + x_2{\\bf a_2} = {\\bf b}.\\]\n\nWritten out, this is:\n\\[ x_1 \\left[\\begin{array}{c}1\\\\-2\\\\-5\\end{array}\\right]+ x_2\\left[\\begin{array}{c}2\\\\5\\\\6\\end{array}\\right] = \\left[\\begin{array}{c}7\\\\4\\\\-3\\end{array}\\right].\\]\n\n\nBy the definition of scalar-vector multiplication, this is:\n\\[\\left[\\begin{array}{c}x_1\\\\-2x_1\\\\-5x_1\\end{array}\\right]+ \\left[\\begin{array}{c}2x_2\\\\5x_2\\\\6x_2\\end{array}\\right]= \\left[\\begin{array}{c}7\\\\4\\\\-3\\end{array}\\right].\\]\n\n\nBy the definition of vector addition, this is:\n\\[\\left[\\begin{array}{r}x_1 + 2x_2\\\\-2x_1 + 5x_2\\\\-5x_1+6x_2\\end{array}\\right] = \\left[\\begin{array}{c}7\\\\4\\\\-3\\end{array}\\right].\\]\n\n\nBy the definition of vector equality, this is:\n\\[\\begin{array}{rcl}x_1 + 2x_2&=&7\\\\-2x_1 + 5x_2&=&4\\\\-5x_1+6x_2&=&-3\\end{array}.\\]\n\n\nWe know how to solve this! Firing up Gaussian Elimination, we first construct the augmented matrix of this system, and then find its reduced row echelon form:\n\\[\\left[\\begin{array}{rrr}1&2&7\\\\-2&5&4\\\\-5&6&-3\\end{array}\\right] \\sim \\left[\\begin{array}{rrr}1&2&7\\\\0&9&18\\\\0&16&32\\end{array}\\right] \\sim \\left[\\begin{array}{rrr}1&2&7\\\\0&1&2\\\\0&16&32\\end{array}\\right] \\sim \\left[\\begin{array}{rrr}1&0&3\\\\0&1&2\\\\0&0&0\\end{array}\\right]\\]\nVoila!\n\n\nNow, reading off the answer, we have \\(x_1 = 3\\), \\(x_2 = 2\\). So we have found the solution to our original problem:\n\\[ 3 \\left[\\begin{array}{c}1\\\\-2\\\\-5\\end{array}\\right]+ 2\\left[\\begin{array}{c}2\\\\5\\\\6\\end{array}\\right] = \\left[\\begin{array}{c}7\\\\4\\\\-3\\end{array}\\right].\\]\nIn other words, we have found that\n\\[ 3{\\bf a_1} + 2{\\bf a_2} = {\\bf b}.\\]\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#a-vector-equation-is-a-linear-system",
    "href": "L04VectorEquations.html#a-vector-equation-is-a-linear-system",
    "title": "Geometric Algorithms",
    "section": "A Vector Equation is a Linear System!",
    "text": "A Vector Equation is a Linear System!\nAnd vice versa!\n\nLet’s state this formally. First, of all, recalling that vectors are columns, we can write the augmented matrix for the linear system in a very simple way.\n\n\nFor the vector equation\n\\[x_1{\\bf a_1} + x_2{\\bf a_2} = {\\bf b},\\]\nthe corresponding linear system has augmented matrix:\n\\[[{\\bf a_1}\\;{\\bf a_2}\\;{\\bf b}].\\]\n\nThen we can make the following statement:\nA vector equation\n\\[ x_1{\\bf a_1} + x_2{\\bf a_2} + ... + x_n{\\bf a_n} = {\\bf b} \\]\nhas the same solution set as the linear system whose augmented matrix is\n\\[ [{\\bf a_1} \\; {\\bf a_2} \\; ... \\; {\\bf a_n} \\; {\\bf b}].\\]\n\nThis is a powerful concept; we have related\n\na single equation involving columns\nto a set of equations corresponding to rows.",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#span",
    "href": "L04VectorEquations.html#span",
    "title": "Geometric Algorithms",
    "section": "Span",
    "text": "Span\nIf a vector equation is equivalent to a linear system, then it must be possible for a vector equation to be inconsistent as well.\nHow can we understand what it means – in terms of vectors – for a vector equation to be inconsistent?\nThe answer involves a new concept: the span of a set of vectors.\n\nLet’s say we are given vectors \\({\\bf a_1}, {\\bf a_2},\\) and \\({\\bf b}\\).\nAnd, say we know that it is possible to express \\(\\mathbf{b}\\) as a linear combination of \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2\\).\nThat is, there are some \\(x_1, x_2\\) such that \\(x_1{\\bf a_1} + x_2{\\bf a_2} = {\\bf b}.\\)\n\n\nThen we say that \\({\\bf b}\\) is in the Span of the set of vectors \\(\\{{\\bf a_1}, {\\bf a_2}\\}.\\)\n\nMore generally, let’s say we are given a set of vectors \\({\\bf v_1, ..., v_p}\\) where each \\({\\bf v_i} \\in \\mathbb{R}^n.\\)\nThen the set of all linear combinations of \\({\\bf v_1, ..., v_p}\\) is denoted by\n\\[ \\operatorname{Span}\\{{\\bf v_1, ..., v_p}\\}\\]\nand is called the subset of \\(\\mathbb{R}^n\\) spanned by \\({\\bf v_1, ..., v_p}.\\)\nSpan of a single vector in \\(\\mathbb{R}^3\\)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nSpan of two vectors in \\(\\mathbb{R}^3\\)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#asking-whether-a-vector-lies-within-a-span",
    "href": "L04VectorEquations.html#asking-whether-a-vector-lies-within-a-span",
    "title": "Geometric Algorithms",
    "section": "Asking Whether a Vector Lies Within a Span",
    "text": "Asking Whether a Vector Lies Within a Span\nAsking whether a vector \\({\\bf b}\\) is in Span\\(\\{{\\bf v_1, ..., v_p}\\}\\) is the same as asking whether the vector equation\n\\[x_1{\\bf v_1} + x_2{\\bf v_2} + \\dots + x_p{\\bf v_p} = {\\bf b}\\]\nhas a solution.\n\n… which we now know is the same as asking whether the linear system with augmented matrix\n\\[ [{\\bf v_1} \\; {\\bf v_2} \\; ... \\; {\\bf v_p} \\; {\\bf b}]\\] has a solution.\n\n\nLet \\({\\bf a_1} = \\left[\\begin{array}{c}1\\\\-2\\\\3\\end{array}\\right], {\\bf a_2} = \\left[\\begin{array}{c}5\\\\-13\\\\-3\\end{array}\\right],\\) and \\({\\bf b} = \\left[\\begin{array}{c}6\\\\8\\\\-5\\end{array}\\right]\\).\nThen Span{\\(\\bf a_1, a_2\\)} is a plane through the origin in \\(\\mathbb{R}^3\\). Is \\(\\bf b\\) in that plane?\n\n\nSolution: Does the equation \\(x_1{\\bf a_1} + x_2{\\bf a_2} = \\bf b\\) have a solution?\nTo answer this, consider the equivalent linear system. Solve the system by row reducing the augmented matrix\n\\[[{\\bf a_1} \\; {\\bf a_2} \\; {\\bf b}]:\\]\n\\[\\left[\\begin{array}{rrr}1&5&6\\\\-2&-13&8\\\\3&-3&-5\\end{array}\\right] \\sim \\left[\\begin{array}{rrr}1&5&6\\\\0&-3&20\\\\0&-18&-23\\end{array}\\right] \\sim \\left[\\begin{array}{rrr}1&5&6\\\\0&-3&20\\\\0&0&-143\\end{array}\\right]\\]\n\n\nThe third row shows that the system has no solution.\nThis means that the vector equation \\(x_1{\\bf a_1} + x_2{\\bf a_2} = \\bf b\\) has no solution.\nSo \\({\\bf b}\\) is not in Span{\\(\\bf a_1, a_2\\)}.\n\n\nWhat does this situation look like geometrically?\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L04VectorEquations.html#two-distinct-vector-spaces",
    "href": "L04VectorEquations.html#two-distinct-vector-spaces",
    "title": "Geometric Algorithms",
    "section": "Two Distinct Vector Spaces",
    "text": "Two Distinct Vector Spaces\nBe sure to keep clear in your mind that we have been working with two different vector spaces.\nOne vector space is for visualizing equations that correspond to rows.\nThe other vector space is for visualizing vector equations (involving columns).\n\nThese are two different ways of visualizing the same linear system.\n\n\nLet’s look at an inconsistent system both ways:\n\n\nHere are the views, first as a vector equation, and then as a system of equations.\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nThe second vector space (and visualization) is more familiar to us right now.\nHowever, as the course goes on we will use the first vector space – the one in which columns are vectors – much more often. Make sure you understand the figure!",
    "crumbs": [
      "Vector Equations"
    ]
  },
  {
    "objectID": "L24SymmetricMatrices.html",
    "href": "L24SymmetricMatrices.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nToday we’ll study a very important class of matrices: symmetric matrices.\n\nWe’ll see that symmetric matrices have properties that relate to both eigendecomposition, and orthogonality.\n\n\nFurthermore, symmetric matrices open up a broad class of problems we haven’t yet touched on: constrained optimization.\nAs a result, symmetric matrices arise very often in applications.\n\nDefinition. A symmetric matrix is a matrix \\(A\\) such that \\(A^T = A\\).\nClearly, such a matrix is square.\nFurthermore, the entries that are not on the diagonal come in pairs, on opposite sides of the diagonal.\nExample. Here are three symmetric matrices:\n\\[\\begin{bmatrix}1&0\\\\0&-3\\end{bmatrix},\\;\\;\\;\\;\\begin{bmatrix}0&-1&0\\\\-1&5&8\\\\0&8&-7\\end{bmatrix},\\;\\;\\;\\;\\begin{bmatrix}a&b&c\\\\b&d&e\\\\c&e&f\\end{bmatrix}\\]\n\nHere are three nonsymmetric matrices:\n\\[\\begin{bmatrix}1&-3\\\\3&0\\end{bmatrix},\\;\\;\\;\\;\\begin{bmatrix}0&-4&0\\\\-6&1&-4\\\\0&-6&1\\end{bmatrix},\\;\\;\\;\\;\\begin{bmatrix}5&4&3&2\\\\4&3&2&1\\\\3&2&1&0\\end{bmatrix}\\]",
    "crumbs": [
      "Symmetric Matrices"
    ]
  },
  {
    "objectID": "L24SymmetricMatrices.html#symmetric-matrices",
    "href": "L24SymmetricMatrices.html#symmetric-matrices",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nToday we’ll study a very important class of matrices: symmetric matrices.\n\nWe’ll see that symmetric matrices have properties that relate to both eigendecomposition, and orthogonality.\n\n\nFurthermore, symmetric matrices open up a broad class of problems we haven’t yet touched on: constrained optimization.\nAs a result, symmetric matrices arise very often in applications.\n\nDefinition. A symmetric matrix is a matrix \\(A\\) such that \\(A^T = A\\).\nClearly, such a matrix is square.\nFurthermore, the entries that are not on the diagonal come in pairs, on opposite sides of the diagonal.\nExample. Here are three symmetric matrices:\n\\[\\begin{bmatrix}1&0\\\\0&-3\\end{bmatrix},\\;\\;\\;\\;\\begin{bmatrix}0&-1&0\\\\-1&5&8\\\\0&8&-7\\end{bmatrix},\\;\\;\\;\\;\\begin{bmatrix}a&b&c\\\\b&d&e\\\\c&e&f\\end{bmatrix}\\]\n\nHere are three nonsymmetric matrices:\n\\[\\begin{bmatrix}1&-3\\\\3&0\\end{bmatrix},\\;\\;\\;\\;\\begin{bmatrix}0&-4&0\\\\-6&1&-4\\\\0&-6&1\\end{bmatrix},\\;\\;\\;\\;\\begin{bmatrix}5&4&3&2\\\\4&3&2&1\\\\3&2&1&0\\end{bmatrix}\\]",
    "crumbs": [
      "Symmetric Matrices"
    ]
  },
  {
    "objectID": "L24SymmetricMatrices.html#orthogonal-diagonalization",
    "href": "L24SymmetricMatrices.html#orthogonal-diagonalization",
    "title": "Geometric Algorithms",
    "section": "Orthogonal Diagonalization",
    "text": "Orthogonal Diagonalization\n\nAt first glance, a symmetric matrix may not seem that special!\nBut in fact symmetric matrices have a number of very nice properties.\nFirst, we’ll look at a remarkable fact:\n\nthe eigenvectors of a symmetric matrix are orthogonal\n\n\nExample. Let’s diagonalize the following symmetric matrix:\n\\[A = \\begin{bmatrix}6&-2&-1\\\\-2&6&-1\\\\-1&-1&5\\end{bmatrix}\\]\n\nSolution.\nThe characteristic equation of \\(A\\) is\n\\[0 = -\\lambda^3 + 17\\lambda^2 -90\\lambda + 144 \\]\n\\[ = -(\\lambda-8)(\\lambda-6)(\\lambda-3)\\]\nSo the eigenvalues are 8, 6, and 3.\n\n\nWe construct a basis for each eigenspace:\n(using our standard method of finding the nullspace of \\(A-\\lambda I\\))\n\\[\\lambda_1 = 8: \\mathbf{v}_1 = \\begin{bmatrix}-1\\\\1\\\\0\\end{bmatrix};\\;\\;\\;\\;\\lambda_2=6: \\mathbf{v}_2 = \\begin{bmatrix}-1\\\\-1\\\\2\\end{bmatrix};\\;\\;\\;\\;\\;\\lambda_3=3: \\mathbf{v}_3=\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\\]\n\n\nThese three vectors form a basis for \\(\\mathbb{R}^3.\\)\nBut here is the remarkable thing: these three vectors form an orthogonal set.\nThat is, any two of these eigenvectors are orthogonal.\n\n\nFor example,\n\\[\\mathbf{v}_1^T\\mathbf{v}_2 = (-1)(-1) + (1)(-1) + (0)(2) = 0\\]\n\n\nAs a result, \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}\\) form an orthogonal basis for \\(\\mathbb{R}^3.\\)\n\n\nLet’s normalize these vectors so they each have length 1:\n\\[\\mathbf{u}_1 = \\begin{bmatrix}-1/\\sqrt{2}\\\\1/\\sqrt{2}\\\\0\\end{bmatrix};\\;\\;\\;\\;\\mathbf{u}_2 = \\begin{bmatrix}-1/\\sqrt{6}\\\\-1/\\sqrt{6}\\\\2/\\sqrt{6}\\end{bmatrix};\\;\\;\\;\\;\\; \\mathbf{u}_3=\\begin{bmatrix}1/\\sqrt{3}\\\\1/\\sqrt{3}\\\\1/\\sqrt{3}\\end{bmatrix}\\]\n\n\nThe orthogonality of the eigenvectors of a symmetric matrix is quite important.\nTo see this, let’s write the diagonalization of \\(A\\) in terms of these eigenvectors and eigenvalues:\n\\[P = \\begin{bmatrix}-1/\\sqrt{2}&-1/\\sqrt{6}&1/\\sqrt{3}\\\\1/\\sqrt{2}&-1/\\sqrt{6}&1/\\sqrt{3}\\\\0&2/\\sqrt{6}&1/\\sqrt{3}\\end{bmatrix},\\;\\;\\;\\;D = \\begin{bmatrix}8&0&0\\\\0&6&0\\\\0&0&3\\end{bmatrix}.\\]\n\n\nThen, \\(A = PDP^{-1},\\) as usual.\n\n\nBut, here is the interesting thing:\n \\(P\\) is square and has orthonormal columns. \nSo \\(P\\) is an orthogonal matrix.\n\n\nSo, that means that \\(P^{-1} = P^T.\\)\n\nSo, \\(A = PDP^T.\\)\n\nThis is a remarkably simple factorization of \\(A\\).\n\n\nThe Spectral Theorem\n\nIt’s not a coincidence that the \\(P\\) matrix was orthogonal.\nIn fact we always get orthogonal eigenvectors when we diagonalize a symmetric matrix.\n\n\nTheorem. If \\(A\\) is symmetric, then any two eigenvectors of \\(A\\) from different eigenspaces are orthogonal.\n\n\nProof.\nLet \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) be eigenvectors that correspond to distinct eigenvalues, say, \\(\\lambda_1\\) and \\(\\lambda_2.\\)\nTo show that \\(\\mathbf{v}_1^T\\mathbf{v}_2 = 0,\\) compute\n\\[\\lambda_1\\mathbf{v}_1^T\\mathbf{v}_2 = (\\lambda_1\\mathbf{v}_1)^T\\mathbf{v}_2\\]\n\n\n\\[=(A\\mathbf{v}_1)^T\\mathbf{v}_2\\]\n\n\n\\[=(\\mathbf{v}_1^TA^T)\\mathbf{v}_2\\]\n\n\n\\[=\\mathbf{v}_1^T(A\\mathbf{v}_2)\\]\n\n\n\\[=\\mathbf{v}_1^T(\\lambda_2\\mathbf{v}_2)\\]\n\n\n\\[=\\lambda_2\\mathbf{v}_1^T\\mathbf{v}_2\\]\n\n\nSo we conclude that \\(\\lambda_1(\\mathbf{v}_1^T\\mathbf{v}_2) = \\lambda_2(\\mathbf{v}_1^T\\mathbf{v}_2).\\)\nBut \\(\\lambda_1 \\neq \\lambda_2,\\) so this can only happen if \\(\\mathbf{v}_1^T\\mathbf{v}_2 = 0.\\)\nSo \\(\\mathbf{v}_1\\) is orthogonal to \\(\\mathbf{v}_2.\\)\n\n\nThe same argument applies to any other pair of eigenvectors with distinct eigenvalues.\nSo any two eigenvectors of \\(A\\) from different eigenspaces are orthogonal.\n\nWe can now introduce a special kind of diagonalizability:\nAn \\(n\\times n\\) matrix is said to be orthogonally diagonalizable if there are an orthogonal matrix \\(P\\) (with \\(P^{-1} = P^T\\)) and a diagonal matrix \\(D\\) such that\n\\[A = PDP^T = PDP^{-1}\\]\n\nSuch a diagonalization requires \\(n\\) linearly independent and orthonormal eigenvectors.\n\n\nWhen is it possible for a matrix’s eigenvectors to form an orthogonal set?\n\n\nWell, if \\(A\\) is orthogonally diagonalizable, then\n\\[A^T = (PDP^T)^T = (P^T)^TD^TP^T = PDP^T = A\\]\n\n\nSo \\(A\\) is symmetric!\nThat is, whenever \\(A\\) is orthogonally diagonalizable, it is symmetric.\n\n\nIt turns out the converse is true (though we won’t prove it): every symmetric matrix is orthogonally diagonalizable.\n\nHence we obtain the following important theorem:\nTheorem. An \\(n\\times n\\) matrix is orthogonally diagonalizable if and only if it is a symmetric matrix.\n\nRemember that when we studied diagonalization, we found that it was a difficult process to determine if an arbitrary matrix was diagonalizable.\nBut here, we have a very nice rule: every symmetric matrix is (orthogonally) diagonalizable.",
    "crumbs": [
      "Symmetric Matrices"
    ]
  },
  {
    "objectID": "L24SymmetricMatrices.html#quadratic-forms",
    "href": "L24SymmetricMatrices.html#quadratic-forms",
    "title": "Geometric Algorithms",
    "section": "Quadratic Forms",
    "text": "Quadratic Forms\n\nAn important reason to study symmetric matrices has to do with quadratic expressions.\nUp until now, we have mainly focused on linear equations – equations in which the \\(x_i\\) terms occur only to the first power.\n\n\nActually, though, we have looked at some quadratic expressions when we considered least-squares problems.\nFor example, we looked at expressions such as \\(\\Vert x\\Vert^2\\) which is \\(\\sum x_i^2.\\)\n\n\nWe’ll now look at quadratic expressions generally. We’ll see that there is a natural and useful connection to symmetric matrices.\n\nDefinition. A quadratic form is a function of variables, eg, \\(x_1, x_2, \\dots, x_n,\\) in which every term has degree two.\nExamples:\n\\(4x_1^2 + 2x_1x_2 + 3x_2^2\\) is a quadratic form.\n\\(4x_1^2 + 2x_1\\) is not a quadratic form.\n\nQuadratic forms arise in many settings, including signal processing, physics, economics, and statistics.\nIn computer science, quadratic forms arise in optimization and graph theory, among other areas.\nEssentially, what an expression like \\(x^2\\) is to a scalar, a quadratic form is to a vector.\n\nFact. Every quadratic form can be expressed as \\(\\mathbf{x}^TA\\mathbf{x}\\), where \\(A\\) is a symmetric matrix.\nThere is a simple way to go from a quadratic form to a symmetric matrix, and vice versa.\nTo see this, let’s look at some examples.\n\nExample. Let \\(\\mathbf{x} = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}.\\) Compute \\(\\mathbf{x}^TA\\mathbf{x}\\) for the matrix \\(A = \\begin{bmatrix}4&0\\\\0&3\\end{bmatrix}.\\)\n\n\nSolution.\n\\[\\mathbf{x}^TA\\mathbf{x} = \\begin{bmatrix}x_1&x_2\\end{bmatrix}\\begin{bmatrix}4&0\\\\0&3\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}\\]\n\n\n\\[= \\begin{bmatrix}x_1&x_2\\end{bmatrix}\\begin{bmatrix}4x_1\\\\3x_2\\end{bmatrix}\\]\n\n\n\\[= 4x_1^2 + 3x_2^2.\\]\n\nExample. Compute \\(\\mathbf{x}^TA\\mathbf{x}\\) for the matrix \\(A = \\begin{bmatrix}3&-2\\\\-2&7\\end{bmatrix}.\\)\n\nSolution.\n\\[\\mathbf{x}^TA\\mathbf{x} = \\begin{bmatrix}x_1&x_2\\end{bmatrix}\\begin{bmatrix}3&-2\\\\-2&7\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}\\]\n\n\n\\[=x_1(3x_1 - 2x_2) + x_2(-2x_1 + 7x_2)\\]\n\n\n\\[=3x_1^2-2x_1x_2-2x_2x_1+7x_2^2\\]\n\n\n\\[=3x_1^2-4x_1x_2+7x_2^2\\]\n\n\nNotice the simple structure:\n\\[a_{11} \\text{ is multiplied by } x_1 x_1\\]\n\\[a_{12} \\text{ is multiplied by } x_1 x_2\\]\n\\[a_{21} \\text{ is multiplied by } x_2 x_1\\]\n\\[a_{22} \\text{ is multiplied by } x_2 x_2\\]\n\n\nWe can write this concisely:\n\\[ \\mathbf{x}^TA\\mathbf{x} = \\sum_{ij} a_{ij}x_i x_j \\]\n\nExample. For \\(\\mathbf{x}\\) in \\(\\mathbb{R}^3\\), let\n\\[Q(\\mathbf{x}) = 5x_1^2 + 3x_2^2 + 2x_3^2 - x_1x_2 + 8x_2x_3.\\]\nWrite this quadratic form \\(Q(\\mathbf{x})\\) as \\(\\mathbf{x}^TA\\mathbf{x}\\).\n\nSolution.\nThe coefficients of \\(x_1^2, x_2^2, x_3^2\\) go on the diagonal of \\(A\\).\n\n\nBased on the previous example, we can see that the coefficient of each cross term \\(x_ix_j\\) is the sum of two values in symmetric positions on opposite sides of the diagonal of \\(A\\).\n\n\nSo to make \\(A\\) symmetric, the coefficient of \\(x_ix_j\\) for \\(i\\neq j\\) must be split evenly between the \\((i,j)\\)- and \\((j,i)\\)-entries of \\(A\\).\n\n\nYou can check that for\n\\[Q(\\mathbf{x}) = 5x_1^2 + 3x_2^2 + 2x_3^2 - x_1x_2 + 8x_2x_3\\]\nthat\n\\[Q(\\mathbf{x}) = \\mathbf{x}^TA\\mathbf{x} = \\begin{bmatrix}x_1&x_2&x_3\\end{bmatrix}\\begin{bmatrix}5&-1/2&0\\\\-1/2&3&4\\\\0&4&2\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}\\]",
    "crumbs": [
      "Symmetric Matrices"
    ]
  },
  {
    "objectID": "L24SymmetricMatrices.html#classifying-quadratic-forms",
    "href": "L24SymmetricMatrices.html#classifying-quadratic-forms",
    "title": "Geometric Algorithms",
    "section": "Classifying Quadratic Forms",
    "text": "Classifying Quadratic Forms\n\nNotice that \\(\\mathbf{x}^TA\\mathbf{x}\\) is a scalar.\nIn other words, when \\(A\\) is an \\(n\\times n\\) matrix, the quadratic form \\(Q(\\mathbf{x}) = \\mathbf{x}^TA\\mathbf{x}\\) is a real-valued function with domain \\(\\mathbb{R}^n\\).\n\n\nHere are four quadratic forms with domain \\(\\mathbb{R}^2\\).\nNotice that except at \\(\\mathbf{x}=\\mathbf{0},\\) the values of \\(Q(\\mathbf{x})\\) are all positive in the first case, and all negative in the last case.\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nThe differences between these surfaces is important for problems such as optimization.\nIn an optimization problem, one seeks the minimum or maximum value of a function (perhaps over a subset of its domain).\n\nDefinition. A quadratic form \\(Q\\) is:\n  1. positive definite if \\(Q(\\mathbf{x}) &gt; 0\\) for all \\(\\mathbf{x} \\neq 0.\\)\n  2. positive semidefinite if \\(Q(\\mathbf{x}) \\geq 0\\) for all \\(\\mathbf{x} \\neq 0.\\)\n  3. indefinite if \\(Q(\\mathbf{x})\\) assumes both positive and negative values.\n  4. negative semidefinite if \\(Q(\\mathbf{x}) \\leq 0\\) for all \\(\\mathbf{x} \\neq 0.\\)\n  5. negative definite if \\(Q(\\mathbf{x}) &lt; 0\\) for all \\(\\mathbf{x} \\neq 0.\\)",
    "crumbs": [
      "Symmetric Matrices"
    ]
  },
  {
    "objectID": "L24SymmetricMatrices.html#classifying-quadratic-forms-1",
    "href": "L24SymmetricMatrices.html#classifying-quadratic-forms-1",
    "title": "Geometric Algorithms",
    "section": "Classifying Quadratic Forms",
    "text": "Classifying Quadratic Forms\n\nKnowing which kind of quadratic form one is dealing with is important for optimization.\n\n\nConsider these two quadratic forms:\n\\[ P(\\mathbf{x}) = \\mathbf{x}^TA\\mathbf{x}, \\text{ where } A = \\begin{bmatrix}3&2\\\\2&1\\end{bmatrix} \\]\n\\[ Q(\\mathbf{x}) = \\mathbf{x}^TB\\mathbf{x}, \\text{ where } B = \\begin{bmatrix}3&2\\\\2&3\\end{bmatrix} \\]\nNotice that that matrices differ in only one position.\n\n\nLet’s say we would like to find\n\nthe minimum value of \\(P(\\mathbf{x})\\) over all \\(\\mathbf{x}\\), or\nthe minimum value of \\(Q(\\mathbf{x})\\) over all \\(\\mathbf{x}\\)\n\nIn each case, we need to ask: is it possible? Does a minimum value even exist?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClearly, we cannot minimize \\(\\mathbf{x}^TA\\mathbf{x}\\) over all \\(\\mathbf{x}\\),\n… but we can minimize \\(\\mathbf{x}^TB\\mathbf{x}\\) over all \\(\\mathbf{x}\\).\n\n\nHow can we distinguish between these two cases in general?\nThat is, in high dimension, without the ability to visualize?\n\n\nThere is a remarkably simple way to determine, for any quadratic form, which class it falls into.\n\nTheorem. Let \\(A\\) be an \\(n\\times n\\) symmetric matrix.\nThen a quadratic form \\(\\mathbf{x}^TA\\mathbf{x}\\) is\n  1. positive definite if and only if the eigenvalues of \\(A\\) are all positive.\n  2. positive semidefinite if and only if the eigenvalues of \\(A\\) are all nonnegative.\n  3. indefinite if and only if \\(A\\) has both positive and negative eigenvalues.\n  4. negative semidefinite if and only if the eigenvalues of \\(A\\) are all nonpositive.\n  5. negative definite if and only if the eigenvalues of \\(A\\) are all negative.\n\nProof.\nA proof sketch for the positive definite case.\nLet’s consider \\(\\mathbf{u}_i\\), an eigenvector of \\(A\\). Then\n\\[\\mathbf{u}_i^TA\\mathbf{u}_i = \\lambda_i\\mathbf{u}_i^T\\mathbf{u}_i.\\]\n\n\nIf all eigenvalues are positive, then all such terms are positive.\n\n\nSince \\(A\\) is symmetric, it is diagonalizable and so its eigenvectors span \\(\\mathbb{R}^n\\).\nSo any \\(\\mathbf{x}\\) can be expressed as a weighted sum of \\(A\\)’s eigenvectors.\n\n\nWriting out the expansion of \\(\\mathbf{x}^TA\\mathbf{x}\\) in terms of \\(A\\)’s eigenvectors, we get only positive terms.\n\nExample. Let’s look at the four quadratic forms above, along with their associated matrices:\n\n\n\\[A = \\begin{bmatrix}3&0\\\\0&7\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\n\n\\[A = \\begin{bmatrix}3&0\\\\0&7\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[A = \\begin{bmatrix}2&0\\\\0&-1\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\n\n\\[A = \\begin{bmatrix}-3&0\\\\0&-7\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample. Is \\(Q(\\mathbf{x}) = 3x_1^2 + 2x_2^2 + x_3^2 + 4x_1x_2 + 4x_2x_3\\) positive definite?\n\nSolution. Because of all the plus signs, this form “looks” positive definite. But the matrix of the form is\n\\[\\begin{bmatrix}3&2&0\\\\2&2&2\\\\0&2&1\\end{bmatrix}\\]\nand the eigenvalues of this matrix turn out to be 5, 2, and -1.\nSo \\(Q\\) is an indefinite quadratic form.",
    "crumbs": [
      "Symmetric Matrices"
    ]
  },
  {
    "objectID": "L24SymmetricMatrices.html#constrained-optimization",
    "href": "L24SymmetricMatrices.html#constrained-optimization",
    "title": "Geometric Algorithms",
    "section": "Constrained Optimization",
    "text": "Constrained Optimization\n\nA common kind of optimization is to find the maximum or the minimum value of a quadratic form \\(Q(\\mathbf{x})\\) for \\(\\mathbf{x}\\) in some specified set.\nFor example, a common constraint is that \\(\\mathbf{x}\\) varies over the set of unit vectors.\nThis is called constrained optimization.\nWhile it can be a difficult problem in general, for quadratic forms it has a particularly elegant solution.\n\n\nThe requirement that a vector \\(\\mathbf{x}\\) in \\(\\mathbb{R}^n\\) be a unit vector can be stated in several equivalent ways:\n\\[\\Vert\\mathbf{x}\\Vert = 1,\\;\\;\\;\\;\\Vert\\mathbf{x}\\Vert^2=1,\\;\\;\\;\\;\\mathbf{x}^T\\mathbf{x} = 1.\\]\n\n\nHere is an example problem:\n\\[\\text{minimize } 3x_1^2 + 7x_2^2 \\text{ subject to } \\Vert\\mathbf{x}\\Vert = 1 \\]\n\n\nLet’s visualize this problem.\nHere is the quadratic form \\(z = 3x_1^2 + 7x_2^2\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow the set of all vectors \\(\\Vert\\mathbf{x}\\Vert = 1\\) is a circle.\nWe plot this circle in the \\((x_1, x_2)\\) plane in green,\nand we plot the value that \\(z\\) takes on for those points in blue.\n\nIntersection of quadratic form \\(z = 3 x_1^2 + 7 x_2^2\\) and \\(\\Vert \\mathbf{x}\\Vert = 1\\)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\nThe set of blue points is the set \\(z = 3x_1^2 + 7x_2^2\\) such that \\(\\Vert \\mathbf{x}\\Vert = 1\\).\n\n\nNow, we can see the geometric sense of a constrained optimization problem.\nIn particular, we can visualize the solutions to two constrained optimizations:\n\\[ \\min 3x_1^2 + 7x_2^2 \\text{ such that } \\Vert\\mathbf{x}\\Vert = 1 \\]\nand\n\\[ \\max 3x_1^2 + 7x_2^2 \\text{ such that } \\Vert\\mathbf{x}\\Vert = 1 \\]\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "Symmetric Matrices"
    ]
  },
  {
    "objectID": "L24SymmetricMatrices.html#solving-constrained-optimization-over-quadratic-forms",
    "href": "L24SymmetricMatrices.html#solving-constrained-optimization-over-quadratic-forms",
    "title": "Geometric Algorithms",
    "section": "Solving Constrained Optimization Over Quadratic Forms",
    "text": "Solving Constrained Optimization Over Quadratic Forms\n\nWhen a quadratic form has no cross-product terms, it is easy to find the maximum and minimum of \\(Q(\\mathbf{x})\\) for \\(\\mathbf{x}^T\\mathbf{x} = 1.\\)\n\n\nExample. Find the maximum and minimum values of \\(Q(\\mathbf{x}) = 9x_1^2 + 4x_2^2 + 3x_3^2\\) subject to the constraint \\(\\mathbf{x}^T\\mathbf{x} = 1.\\)\n\n\nSince \\(x_2^2\\) and \\(x_3^2\\) are nonnegative, we know that\n\\[4x_2^2 \\leq 9x_2^2\\;\\;\\;\\;\\text{and}\\;\\;\\;\\;3x_3^2\\leq 9x_3^2.\\]\n\n\nSo\n\\[Q(\\mathbf{x}) = 9x_1^2 + 4x_2^2 + 3x_3^2\\]\n\n\n\\[\\leq 9x_1^2 + 9x_2^2 + 9x_3^2\\]\n\n\n\\[=9(x_1^2 + x_2^2 + x_3^2)\\]\n\n\n\\[=9\\]\nWhenever \\(x_1^2 + x_2^2 + x_3^2 = 1.\\) So the maximum value of \\(Q(\\mathbf{x})\\) cannot exceed 9 when \\(\\mathbf{x}\\) is a unit vector.\n\n\nFurthermore, \\(Q(\\mathbf{x}) = 9\\) when \\(\\mathbf{x}=(1,0,0).\\)\nThus 9 is the maximum value of \\(Q(\\mathbf{x})\\) for \\(\\mathbf{x}^T\\mathbf{x} = 1\\).\n\n\nA similar argument shows that the minimum value of \\(Q(\\mathbf{x})\\) when \\(\\mathbf{x}^T\\mathbf{x}=1\\) is 3.\n\n\nEigenvalues Solve Contrained Optimization\n\nObservation.\nNote that the matrix of the quadratic form in the example is\n\\[A = \\begin{bmatrix}9&0&0\\\\0&4&0\\\\0&0&3\\end{bmatrix}\\]\n\n\n\nProof: Since \\(A\\) is symmetric, it has an orthogonal diagonalization \\(P^TAP = D = \\operatorname{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)\\).\nSince \\(P\\) is orthogonal, if \\(\\Vert\\mathbf{x}\\Vert = 1\\), then \\(\\Vert P\\mathbf{x}\\Vert = 1\\).\nSo, for any \\(\\mathbf{x}\\), define \\(\\mathbf{y} = P\\mathbf{x}\\).\n\\[\\max_{\\mathbf{x}^T\\mathbf{x} = 1} \\mathbf{x}^TA\\mathbf{x} = \\max_{\\mathbf{y}^T\\mathbf{y} = 1} \\mathbf{y}^TD\\mathbf{y}\\]\n\\[ = \\max_{\\mathbf{y}^T\\mathbf{y} = 1} \\sum_{i=1}^n \\lambda_i y_i^2 \\]\n\\[ \\leq \\max_{\\mathbf{y}^T\\mathbf{y} = 1} \\lambda_1 \\sum_{i=1}^n y_i^2 \\]\n\\[ = \\lambda_1.\\]\nNote that equality is obtained when \\(\\mathbf{x}\\) is an eigenvector of unit norm associated with \\(\\lambda_1.\\)\n\nSo the eigenvalues of \\(A\\) are \\(9, 4,\\) and \\(3\\).\nWe note that the greatest and least eigenvalues equal, respectively, the (constrained) maximum and minimum of \\(Q(\\mathbf{x})\\).\n\n\nIn fact, this is true for any quadratic form.\n\n\nTheorem. Let \\(A\\) be an \\(n\\times n\\) symmetric matrix, and let\n\\[M = \\max_{\\mathbf{x}^T\\mathbf{x} = 1} \\mathbf{x}^TA\\mathbf{x}.\\]\nThen \\(M = \\lambda_1\\), the greatest eigenvalue of \\(A\\).\n\n\nThe value of \\(Q(\\mathbf{x})\\) is \\(\\lambda_1\\) when \\(\\mathbf{x}\\) is the unit eigenvector corresponding to \\(\\lambda_1\\).\n\n\nA similar theorem holds for the constrained minimum of \\(Q(\\mathbf{x})\\) and the least eigenvector \\(\\lambda_n\\).",
    "crumbs": [
      "Symmetric Matrices"
    ]
  },
  {
    "objectID": "L20Orthogonality.html",
    "href": "L20Orthogonality.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Transcript of oral arguments before the US Supreme Court, Briscoe v. Virginia, January 11, 2010:\nMR. FRIEDMAN: I think that issue is entirely orthogonal to the issue here because the Commonwealth is acknowledging –\nCHIEF JUSTICE ROBERTS: I’m sorry. Entirely what?\nMR. FRIEDMAN: Orthogonal. Right angle. Unrelated. Irrelevant.\nCHIEF JUSTICE ROBERTS: Oh.\nJUSTICE SCALIA: What was that adjective? I liked that.\nMR. FRIEDMAN: Orthogonal.\nCHIEF JUSTICE ROBERTS: Orthogonal.\nMR. FRIEDMAN: Right, right.\nJUSTICE SCALIA: Orthogonal, ooh.\n(Laughter.)\nJUSTICE KENNEDY: I knew this case presented us a problem.\n(Laughter.)\n\n\nMany parts of this page are based on Linear Algebra and its Applications, by David C. Lay",
    "crumbs": [
      "Analytic Geometry in $\\mathbb{R}^n$"
    ]
  },
  {
    "objectID": "L20Orthogonality.html#analytic-geometry-in-mathbbrn",
    "href": "L20Orthogonality.html#analytic-geometry-in-mathbbrn",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Transcript of oral arguments before the US Supreme Court, Briscoe v. Virginia, January 11, 2010:\nMR. FRIEDMAN: I think that issue is entirely orthogonal to the issue here because the Commonwealth is acknowledging –\nCHIEF JUSTICE ROBERTS: I’m sorry. Entirely what?\nMR. FRIEDMAN: Orthogonal. Right angle. Unrelated. Irrelevant.\nCHIEF JUSTICE ROBERTS: Oh.\nJUSTICE SCALIA: What was that adjective? I liked that.\nMR. FRIEDMAN: Orthogonal.\nCHIEF JUSTICE ROBERTS: Orthogonal.\nMR. FRIEDMAN: Right, right.\nJUSTICE SCALIA: Orthogonal, ooh.\n(Laughter.)\nJUSTICE KENNEDY: I knew this case presented us a problem.\n(Laughter.)\n\n\nMany parts of this page are based on Linear Algebra and its Applications, by David C. Lay",
    "crumbs": [
      "Analytic Geometry in $\\mathbb{R}^n$"
    ]
  },
  {
    "objectID": "L20Orthogonality.html#the-challenge-extending-geometric-intuition-to-high-dimension",
    "href": "L20Orthogonality.html#the-challenge-extending-geometric-intuition-to-high-dimension",
    "title": "Geometric Algorithms",
    "section": "The Challenge: Extending Geometric Intuition to High Dimension",
    "text": "The Challenge: Extending Geometric Intuition to High Dimension\n\nOur challenge today is to begin to lay down the elements of analytic geometry.\nThe concepts we deal with today will be familiar to you.\n\n\nOur goal is not to introduce new ideas, but to cast old ideas into greater generality.\nIn particular, we will take familiar notions and reformulate them in terms of vectors …\n… vectors in arbitrary dimension.\n\n\nLet’s start with a simple example in \\(\\mathbb{R}^2\\).\nHow would you determine the angle \\(\\theta\\) below?\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is fairly easy. Probably we’d take out a protractor and use it to measure \\(\\theta\\).\n\n\nOK, let’s go up one dimension, to \\(\\mathbb{R}^3\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis seems a little more challenging, but we could probably do it.\nLet’s go up one dimension, to \\(\\mathbb{R}^4\\):\n\n\nThis seems hard!\n\nWhat we need are ways to capture simple notions:\n\nlength,\ndistance,\northogonality (perpendicularity), and\nangle.\n\nHowever we need to take these notions that are familiar from our 3D world and see how to define them for spaces of arbitrary dimension, ie, \\(\\mathbb{R}^n\\).\nInterestingly, it turns out that these notions (length, distance, perpendicularity, angle) all depend on one key notion: the inner product.\nIn fact, the notion is so important that we refer to a vector space for which there is an inner product as an inner product space.\n\nSo let’s briefly return to and review the inner product.",
    "crumbs": [
      "Analytic Geometry in $\\mathbb{R}^n$"
    ]
  },
  {
    "objectID": "L20Orthogonality.html#inner-product-review",
    "href": "L20Orthogonality.html#inner-product-review",
    "title": "Geometric Algorithms",
    "section": "Inner Product (Review)",
    "text": "Inner Product (Review)\n\nRecall that we consider vectors such as \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathbb{R}^n\\) to be \\(n\\times1\\) matrices.\nThen \\(\\mathbf{u}^T\\mathbf{v}\\) is a scalar, called the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}.\\)\nYou will also see this called the dot product.\nIt is sometimes written as \\(\\mathbf{u} \\mathbf{\\cdot} \\mathbf{v}\\), or \\(\\langle \\mathbf{u}, \\mathbf{v}\\rangle\\)\nbut we will always write \\(\\mathbf{u}^T\\mathbf{v}.\\)\n\n\nThe inner product is the sum of the componentwise product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\).\n\n\nIf \\(\\mathbf{u} = \\begin{bmatrix}u_1\\\\u_2\\\\\\vdots\\\\u_n\\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix}v_1\\\\v_2\\\\\\vdots\\\\v_n\\end{bmatrix},\\) then the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is:\n\\[\\mathbf{u}^T\\mathbf{v} = \\begin{bmatrix}u_1&u_2&\\dots&u_n\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots\\\\v_n\\end{bmatrix} = u_1v_1 + u_2v_2 + \\dots + u_nv_n = \\sum_{i=1}^n u_iv_i.\\]\n\nLet’s remind ourselves of the properties of the inner product:\n\nTheorem. Let \\(\\mathbf{u}\\),\\(\\mathbf{v}\\), and \\(\\mathbf{w}\\) be vectors in \\(\\mathbb{R}^n\\), and let \\(c\\) be a scalar. Then:\n  1. \\(\\mathbf{u}^T\\mathbf{v} = \\mathbf{v}^T\\mathbf{u}\\)\nInner product is symmetric. Note that these two expressions are the transpose of each other – but of course the transpose of a scalar is itself!\n\n\n  2. \\((\\mathbf{u}+\\mathbf{v})^T\\mathbf{w} = \\mathbf{u}^T\\mathbf{w} + \\mathbf{v}^T\\mathbf{w}\\)\n  3. \\((c\\mathbf{u})^T\\mathbf{v} = c(\\mathbf{u}^T\\mathbf{v}) = \\mathbf{u}^T(c\\mathbf{v})\\)\nInner product is linear in each term.\n\n\n  4. \\(\\mathbf{u}^T\\mathbf{u} \\geq 0,\\;\\;\\;\\text{and}\\;\\mathbf{u}^T\\mathbf{u} = 0\\;\\text{if and only if}\\;\\mathbf{u} = 0\\)\nInner product of a vector with itself is never negative.\n\n\nThe first three are restatements of facts about matrix-vector products.\nThe last one is straightforward, but important.\n\n\nNow, given that review, let’s start talking about geometry.",
    "crumbs": [
      "Analytic Geometry in $\\mathbb{R}^n$"
    ]
  },
  {
    "objectID": "L20Orthogonality.html#vector-norm",
    "href": "L20Orthogonality.html#vector-norm",
    "title": "Geometric Algorithms",
    "section": "Vector Norm",
    "text": "Vector Norm\nOK, armed with the inner product, let’s get started.\nOur first question will be: How do we measure the length of a vector?\n\nLet’s say we are in \\(\\mathbb{R}^2\\). Then the length follows directly from the Pythagorean theorem:\n\n\n\n\n\n\n\n\n\n\n\n\nWhat happens when we move to \\(\\mathbb{R}^3\\)?\n\nIt turns out we need to apply the Pythagorean Theorem an additional time:\n\n\n\n\n\n\n\n\n\n\n\n\nSo the length of a vector in \\(\\mathbb{R}^n\\) is\n\\[ \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2} = \\sqrt{\\sum_{i=1}^n{v_i}^2}\\]\n\nNow let’s express this in a way that does not require writing the individual components of \\(\\mathbf{v}\\).\n\n\nWe notice that the above expression for the length of \\(\\mathbf{v}\\) is the same as:\n\\[\\sqrt{\\mathbf{v}^T\\mathbf{v}}.\\]\nNotice that this is always defined because \\(\\mathbf{v}^T\\mathbf{v}\\) is nonnegative.\n\nLength is such a fundamental concept that we introduce a special notation and name for it.\nDefinition. The norm of \\(\\mathbf{v}\\) is the nonnegative scalar \\(\\Vert\\mathbf{v}\\Vert\\) defined by\n\\[\\Vert\\mathbf{v}\\Vert = \\sqrt{\\mathbf{v}^T\\mathbf{v}} = \\sqrt{\\sum_{i=1}^n{v_i}^2}.\\]\n\nNormalization to Unit Length\nFor any scalar \\(c\\), the length of \\(c\\mathbf{v}\\) is \\(|c|\\) times the length of \\(\\mathbf{v}\\). That is,\n\\[\\Vert c\\mathbf{v}\\Vert = \\vert c\\vert\\Vert\\mathbf{v}\\Vert.\\]\n\nSo, for example, \\(\\Vert(-2)\\mathbf{v}\\Vert = 2\\Vert \\mathbf{v}\\Vert\\).\n\n\nA vector of length 1 is called a unit vector.\n\n\nIf we divide a nonzero vector \\(\\mathbf{v}\\) by its length – that is, multiply by \\(1/\\Vert\\mathbf{v}\\Vert\\) – we obtain a unit vector \\(\\mathbf{u}\\).\nWe say that we have normalized \\(\\mathbf{v}\\), and that \\(\\mathbf{u}\\) is in the same direction as \\(\\mathbf{v}.\\)\n\nExample. Let \\(\\mathbf{v} = \\begin{bmatrix}1\\\\-2\\\\2\\\\0\\end{bmatrix}.\\) Find the unit vector \\(\\mathbf{u}\\) in the same direction as \\(\\mathbf{v}.\\)\n\nSolution.\nFirst, compute the length of \\(\\mathbf{v}\\):\n\\[\\Vert\\mathbf{v}\\Vert^2 = \\mathbf{v}^T\\mathbf{v} = (1)^2 + (-2)^2 + (2)^2 + (0)^2 = 9\\]\n\\[\\Vert\\mathbf{v}\\Vert = \\sqrt{9} = 3\\]\n\n\nThen multiply \\(\\mathbf{v}\\) by \\(1/\\Vert\\mathbf{v}\\Vert\\) to obtain\n\\[\\mathbf{u} = \\frac{1}{\\Vert\\mathbf{v}\\Vert}\\mathbf{v} = \\frac{1}{3}\\mathbf{v} = \\frac{1}{3}\\begin{bmatrix}1\\\\-2\\\\2\\\\0\\end{bmatrix} = \\begin{bmatrix}1/3\\\\-2/3\\\\2/3\\\\0\\end{bmatrix}\\]\n\n\nIt’s important to note that we can’t actually visualize \\(\\mathbf{u}\\) but we can still reason geometrically about it as a unit vector.\n\n\nFor example, we can talk about (2D) circles, (3D) spheres, four-dimensional spheres, five-dimensional spheres, etc.",
    "crumbs": [
      "Analytic Geometry in $\\mathbb{R}^n$"
    ]
  },
  {
    "objectID": "L20Orthogonality.html#distance",
    "href": "L20Orthogonality.html#distance",
    "title": "Geometric Algorithms",
    "section": "Distance",
    "text": "Distance\n\nIt’s very useful to be able to talk about the distance between two points (or vectors) in \\(\\mathbb{R}^n\\).\n\n\nWe can start from basics:\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the number line (ie, \\(\\mathbb{R}^1\\)), the distance between two points \\(a\\) and \\(b\\) is \\(\\vert a-b\\vert\\).\n\n\nThe same is true in \\(\\mathbb{R}^n\\).\n\nDefinition. For \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathbb{R}^n,\\) the distance between \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), written as \\(\\operatorname{dist}(\\mathbf{u},\\mathbf{v}),\\) is the length of the vector \\(\\mathbf{u}-\\mathbf{v}\\). That is,\n\\[\\operatorname{dist}(\\mathbf{u},\\mathbf{v}) = \\Vert\\mathbf{u}-\\mathbf{v}\\Vert.\\]\n\nThis definition agrees with the usual formulas for the Euclidean distance between two points. The usual formula is\n\\[\\operatorname{dist}(\\mathbf{u},\\mathbf{v}) = \\sqrt{(u_1-v_1)^2 + (u_2-v_2)^2 + \\dots + (u_n-v_n)^2}.\\]\n\n\nWhich you can see is equal to\n\\[\\Vert\\mathbf{u}-\\mathbf{v}\\Vert = \\sqrt{(\\mathbf{u}-\\mathbf{v})^T(\\mathbf{u}-\\mathbf{v})} = \\sqrt{\\begin{bmatrix}u_1-v_1\\dots&u_n-v_n\\end{bmatrix}\\begin{bmatrix}u_1-v_1\\\\\\vdots\\\\u_n-v_n\\end{bmatrix}}\\]\n\nThere is a geometric view as well.\n\nFor example, consider the vectors \\(\\mathbf{u} = \\begin{bmatrix}7\\\\1\\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix}3\\\\2\\end{bmatrix}\\) in \\(\\mathbb{R}^2\\).\nThen one can see that the distance from \\(\\mathbf{u}\\) to \\(\\mathbf{v}\\) is the same as the length of the vector \\(\\mathbf{u}-\\mathbf{v}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis shows that the distance between two vectors is the length of their difference.",
    "crumbs": [
      "Analytic Geometry in $\\mathbb{R}^n$"
    ]
  },
  {
    "objectID": "L20Orthogonality.html#orthogonality",
    "href": "L20Orthogonality.html#orthogonality",
    "title": "Geometric Algorithms",
    "section": "Orthogonality",
    "text": "Orthogonality\n\nNow we turn to another familiar notion from 2D geometry, which we’ll generalize to \\(\\mathbb{R}^n\\):  the notion of being perpendicular.\n\n\nOnce again, we seek a way to express perpendicularity of two vectors, regardless of the dimension they live in.\n\n\nWe will say that two vectors are perpendicular if they form a right angle at the origin.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s draw a line connecting \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\).\nThen \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are perpendicular if and only if they make a right triangle with the origin.\nSo \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are perpendicular if and only if the Pythagorean Theorem is satisified for this triangle.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the length of the red side of the triangle?\nAccording to the definitions we’ve developed today, it is \\(\\Vert \\mathbf{u} - \\mathbf{v} \\Vert\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo the blue and green lines are perpendicular if and only if:\n\\[ \\Vert \\mathbf{u} - \\mathbf{v} \\Vert^2 = \\Vert \\mathbf{u} \\Vert^2 + \\Vert \\mathbf{v} \\Vert^2\\]\n\n\nLet’s see what this implies from an algebraic standpoint.\n\n\nFirst let’s simplify the expression for squared distance from \\(\\mathbf{u}\\) to \\(\\mathbf{v}\\):\n\\[[\\operatorname{dist}(\\mathbf{u},\\mathbf{v})]^2 = \\Vert\\mathbf{u}-\\mathbf{v}\\Vert^2\\]\n\n\n\\[ = (\\mathbf{u}-\\mathbf{v})^T(\\mathbf{u}-\\mathbf{v})\\]\n\n\n\\[ = (\\mathbf{u}^T-\\mathbf{v}^T)(\\mathbf{u}-\\mathbf{v})\\]\n\n\n\\[ = \\mathbf{u}^T(\\mathbf{u}-\\mathbf{v}) - \\mathbf{v}^T(\\mathbf{u}-\\mathbf{v})\\]\n\n\n\\[ = \\mathbf{u}^T\\mathbf{u} - \\mathbf{u}^T\\mathbf{v} - \\mathbf{v}^T\\mathbf{u} + \\mathbf{v}^T\\mathbf{v}\\]\n\n\nNow, remember that inner product is symmetric, ie, \\(\\mathbf{u}^T\\mathbf{v} = \\mathbf{v}^T\\mathbf{u}\\), so\n\\[ = \\mathbf{u}^T\\mathbf{u} + \\mathbf{v}^T\\mathbf{v} - 2\\mathbf{u}^T\\mathbf{v}\\]\n\n\n\\[ = \\Vert\\mathbf{u}\\Vert^2 + \\Vert\\mathbf{v}\\Vert^2 - 2\\mathbf{u}^T\\mathbf{v}\\]\n\n\nNow, let’s go back to the Pythagorean Theorem.\n\\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are perpendicular if and only if:\n\\[ \\Vert \\mathbf{u} - \\mathbf{v} \\Vert^2 = \\Vert \\mathbf{u} \\Vert^2 + \\Vert \\mathbf{v} \\Vert^2\\]\n\n\nBut we’ve seen that this means:\n\\[ \\Vert\\mathbf{u}\\Vert^2 + \\Vert\\mathbf{v}\\Vert^2 - 2\\mathbf{u}^T\\mathbf{v}= \\Vert \\mathbf{u} \\Vert^2 + \\Vert \\mathbf{v} \\Vert^2\\]\n\nSo now we can define perpendicularity in \\(\\mathbb{R}^n\\):\nDefinition. Two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathbb{R}^n\\) are orthogonal to each other if \\(\\mathbf{u}^T\\mathbf{v} = 0.\\)\n\nAs you can see, we have introduced a new term for this notion: orthogonal.\nSo when we are referring to vectors, orthogonal means the same thing as perpendicular.",
    "crumbs": [
      "Analytic Geometry in $\\mathbb{R}^n$"
    ]
  },
  {
    "objectID": "L20Orthogonality.html#the-angle-between-two-vectors",
    "href": "L20Orthogonality.html#the-angle-between-two-vectors",
    "title": "Geometric Algorithms",
    "section": "The Angle Between Two Vectors",
    "text": "The Angle Between Two Vectors\n\nThere is an important connection between the inner product of two vectors and the angle between them.\nThis connection is very useful (eg, in thinking about data mining operations).\n\n\nWe start from the law of cosines:\n\\[ c^2 = a^2 + b^2 - 2ab\\cos\\theta\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s interpret this law in terms of vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\).\nOnce again, it is the angle that these vectors make at the origin that we are concerned with:\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplying the law of cosines we get:\n\\[\\Vert\\mathbf{u}-\\mathbf{v}\\Vert^2 = \\Vert\\mathbf{u}\\Vert^2 + \\Vert\\mathbf{v}\\Vert^2 - 2\\Vert\\mathbf{u}\\Vert\\Vert\\mathbf{v}\\Vert\\cos\\theta\\]\n\n\nNow, previously we calculated that:\n\\[  \\Vert\\mathbf{u}-\\mathbf{v}\\Vert^2 = (\\mathbf{u}-\\mathbf{v})^T(\\mathbf{u}-\\mathbf{v})\\]\n\\[ = \\Vert\\mathbf{u}\\Vert^2 + \\Vert\\mathbf{v}\\Vert^2 - 2\\mathbf{u}^T\\mathbf{v}\\]\n\n\nWhich means that\n\\[ 2\\mathbf{u}^T\\mathbf{v} = 2\\Vert\\mathbf{u}\\Vert\\Vert\\mathbf{v}\\Vert\\cos\\theta\\]\n\n\nSo\n\\[ \\mathbf{u}^T\\mathbf{v} = \\Vert\\mathbf{u}\\Vert\\Vert\\mathbf{v}\\Vert\\cos\\theta\\]\n\n\nThis is a very important connection between the notion of inner product and trigonometry.\n\n\nAs a quick check, note that if \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are nonzero, and \\(\\mathbf{u}^T\\mathbf{v} = 0\\), then \\(\\cos\\theta = 0.\\)\nIn other words, the angle between \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is 90 degrees (or 270 degrees). So this agrees with our definition of orthogonality.\n\nOne implication in particular concerns unit vectors.\n\\[ \\mathbf{u}^T\\mathbf{v} = \\Vert\\mathbf{u}\\Vert\\Vert\\mathbf{v}\\Vert\\cos\\theta\\]\n\nSo\n\\[ \\frac{\\mathbf{u}^T\\mathbf{v}}{\\Vert\\mathbf{u}\\Vert\\Vert\\mathbf{v}\\Vert} = \\cos\\theta\\]\n\n\n\\[ \\frac{\\mathbf{u}^T}{\\Vert\\mathbf{u}\\Vert}\\frac{\\mathbf{v}}{\\Vert\\mathbf{v}\\Vert} = \\cos\\theta\\]\n\n\nNote that \\(\\frac{\\mathbf{u}}{\\Vert\\mathbf{u}\\Vert}\\) and \\(\\frac{\\mathbf{v}}{\\Vert\\mathbf{v}\\Vert}\\) are unit vectors.\n\n\nSo we have the very simple rule, that for two unit vectors, their inner product is the cosine of the angle between them!\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere \\(\\mathbf{u} = \\begin{bmatrix}0.95\\\\0.31\\end{bmatrix},\\) and \\(\\mathbf{v} = \\begin{bmatrix}0.20\\\\0.98\\end{bmatrix}.\\)\nSo \\(\\mathbf{u}^T\\mathbf{v} = (0.95\\cdot 0.20) + (0.31 \\cdot 0.98) = 0.5\\)\nSo \\(\\cos\\theta = 0.5.\\)\nSo \\(\\theta = 60\\) degrees.\n\nExample. Find the angle formed by the vectors:\n\\[\\mathbf{u} = \\begin{bmatrix}1\\\\3\\\\-7\\\\-2\\end{bmatrix} \\;\\;\\text{and}\\;\\; \\mathbf{v} = \\begin{bmatrix}8\\\\-2\\\\4\\\\6\\end{bmatrix}\\]\n\nSolution.\nFirst normalize the vectors:\n\\[\\Vert\\mathbf{u}\\Vert = \\sqrt{1^2 + 3^2 + (-7)^2 + (-2)^2} = 7.93 \\] \\[\\Vert\\mathbf{v}\\Vert = \\sqrt{8^2 + (-2)^2 + 4^2 + 6^2} = 10.95 \\]\n\n\nSo\n\\[\\frac{\\mathbf{u}}{\\Vert\\mathbf{u}\\Vert} = \\begin{bmatrix}0.13\\\\0.38\\\\-0.88\\\\-0.25\\end{bmatrix}\n\\;\\;\\text{and}\\;\\;\\frac{\\mathbf{v}}{\\Vert\\mathbf{v}\\Vert} = \\begin{bmatrix}0.73\\\\-0.18\\\\0.36\\\\0.54\\end{bmatrix}\\]\n\n\nSo\n\\[\\frac{\\mathbf{u}}{\\Vert\\mathbf{u}\\Vert} = \\begin{bmatrix}0.13\\\\0.38\\\\-0.88\\\\-0.25\\end{bmatrix}\n\\;\\;\\text{and}\\;\\;\\frac{\\mathbf{v}}{\\Vert\\mathbf{v}\\Vert} = \\begin{bmatrix}0.73\\\\-0.18\\\\0.36\\\\0.54\\end{bmatrix}\\]\n\n\n\\[ = (0.13\\cdot0.73)+(0.38\\cdot -0.18)+(-0.88\\cdot 0.36)+(-0.25\\cdot0.54)\\]\n\n\n\\[= -0.44\\]\n\n\nThen:\n\\[\\theta = \\cos^{-1}(-0.44)\\]\n\\[= 116\\;\\text{degrees.}\\]\n\n\nExample Application: Cosine Similarity.\nA typical example where these techniques are valuable arises in data science.\nLet’s say we are given two documents \\(d_1\\) and \\(d_2\\). Each is a collection of “words.”\nFor a goal such as information retrieval, we’d like to know whether the two documents are “similar”.\n\nOne common way to formalize similarity is to say that documents are similar if the sets of words they contain are similar.\n\n\nWe can measure this as follows: Construct a vector \\(\\mathbf{x}_1\\) that counts the frequency of occurrence of certain words in \\(d_1\\):\n\\[ \\begin{bmatrix}\ndog\\\\ car \\\\ house \\\\ \\dots \\\\ door\n\\end{bmatrix} \\;\\; \\rightarrow \\;\\;\n\\begin{bmatrix}\n3\\\\ 7 \\\\ 4 \\\\ \\dots \\\\ 20\n\\end{bmatrix} \\]\nDo the same thing for \\(d_2\\), yielding \\(\\mathbf{x_2}\\).\n\n\nWhat we have done is taken individual documents and represented them as vectors \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) in a very high dimensional space, \\(\\mathbb{R}^n\\).\nHere \\(n\\) could be 10,000 or more.\n\n\nNow, to ask whether the two documents are similar, we simply ask “do their vectors point in the same general direction?”\n\n\nAnd, despite the fact that we are in a very high dimensional space which we cannot visualize, we can nonetheless answer the question easily.\nThe angle between the two document-vectors is simply:\n\\[\\theta_{1,2} = \\cos^{-1}\\frac{\\mathbf{x_1}^T}{\\Vert\\mathbf{x_1}\\Vert}\\frac{\\mathbf{x_2}}{\\Vert\\mathbf{x_2}\\Vert}\\]",
    "crumbs": [
      "Analytic Geometry in $\\mathbb{R}^n$"
    ]
  },
  {
    "objectID": "L14Subspaces.html",
    "href": "L14Subspaces.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\nSo far have been working with vector spaces like \\(\\mathbb{R}^2, \\mathbb{R}^3, \\mathbb{R}^{1000}, \\mathbb{R}^n.\\)\nBut there are more vector spaces. . .\n\n\nToday we’ll define a subspace and show how the concept helps us understand the nature of matrices and their linear transformations.\n\nConceptually, a subspace is a set of vectors that is contained in another space …\nsuch that linear combinations of the vectors in the subspace are always also in the subspace.\n\nDefinition. A subspace is any set \\(H\\) in \\(\\mathbb{R}^n\\) that has three properties:\n\nThe zero vector is in \\(H\\).\nFor each \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(H\\), the sum \\(\\mathbf{u} + \\mathbf{v}\\) is in \\(H\\).\nFor each \\(\\mathbf{u}\\) in \\(H\\) and each scalar \\(c,\\) the vector \\(c\\mathbf{u}\\) is in \\(H\\).\n\n\n\nAnother way of stating properties 2 and 3 is that \\(H\\) is closed under addition and scalar multiplication.\n\n\nIn other words, a subspace has the “Las Vegas property”:\n\nWhat happens in \\(H\\), stays in \\(H\\).\n\n\n\n\nThe first thing to note is that there is a close connection between Span and Subspace:\n\n Every Span is a Subspace. \n\n\nTo see this, let’s take a specific example.\nFor example, take \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) in \\(\\mathbb{R}^n\\), and let \\(H\\) = Span\\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}.\\)\nThen \\(H\\) is a subspace of \\(\\mathbb{R}^n\\).\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nLet’s check this:\n\nThe zero vector is in \\(H\\)\n\n… because \\({\\bf 0} = 0\\mathbf{v}_1 + 0\\mathbf{v}_2\\),\n… so \\({\\bf 0}\\) is in Span\\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}.\\)\n\n\n\nThe sum of any two vectors in \\(H\\) is in \\(H\\)\n\nIn other words, if \\(\\mathbf{u} = s_1\\mathbf{v}_1 + s_2\\mathbf{v}_2,\\) and \\(\\mathbf{v} = t_1\\mathbf{v}_1 + t_2\\mathbf{v}_2,\\)\n… their sum \\(\\mathbf{u} + \\mathbf{v}\\) is \\((s_1+t_1)\\mathbf{v}_1 + (s_2+t_2)\\mathbf{v}_2,\\)\n… which is in \\(H\\).\n\n\n\nFor any scalar \\(c\\), \\(c\\mathbf{u}\\) is in \\(H\\)\n\nbecause \\(c\\mathbf{u} = c(s_1\\mathbf{v}_1 + s_2\\mathbf{v}_2) = (cs_1\\mathbf{v}_1 + cs_2\\mathbf{v}_2).\\)\n\n\nBecause every span is a subspace, we refer to \\(\\operatorname{Span}\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_p\\}\\) as the subspace spanned by \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_p.\\)\n\nOK, here is another subspace – a line:\n\n\n\n\n\n\n\n\n\n\nNext question: is any line a subspace?\nWhat about a line that is not through the origin?\n\n\n\n\n\n\n\n\n\n\n\nIn fact, a line \\(L\\) not through the origin fails all three requirements for a subspace:\n\n\n\n\\(L\\) does not contain the zero vector.\n\n\n\n\n\\(L\\) is not closed under addition.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(L\\) is not closed under scalar multiplication.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the other hand, any line, plane, or hyperplane through the origin is a subspace.\nMake sure you can see why (or prove it to yourself).",
    "crumbs": [
      "Subspaces"
    ]
  },
  {
    "objectID": "L14Subspaces.html#subspaces",
    "href": "L14Subspaces.html#subspaces",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\nSo far have been working with vector spaces like \\(\\mathbb{R}^2, \\mathbb{R}^3, \\mathbb{R}^{1000}, \\mathbb{R}^n.\\)\nBut there are more vector spaces. . .\n\n\nToday we’ll define a subspace and show how the concept helps us understand the nature of matrices and their linear transformations.\n\nConceptually, a subspace is a set of vectors that is contained in another space …\nsuch that linear combinations of the vectors in the subspace are always also in the subspace.\n\nDefinition. A subspace is any set \\(H\\) in \\(\\mathbb{R}^n\\) that has three properties:\n\nThe zero vector is in \\(H\\).\nFor each \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(H\\), the sum \\(\\mathbf{u} + \\mathbf{v}\\) is in \\(H\\).\nFor each \\(\\mathbf{u}\\) in \\(H\\) and each scalar \\(c,\\) the vector \\(c\\mathbf{u}\\) is in \\(H\\).\n\n\n\nAnother way of stating properties 2 and 3 is that \\(H\\) is closed under addition and scalar multiplication.\n\n\nIn other words, a subspace has the “Las Vegas property”:\n\nWhat happens in \\(H\\), stays in \\(H\\).\n\n\n\n\nThe first thing to note is that there is a close connection between Span and Subspace:\n\n Every Span is a Subspace. \n\n\nTo see this, let’s take a specific example.\nFor example, take \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) in \\(\\mathbb{R}^n\\), and let \\(H\\) = Span\\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}.\\)\nThen \\(H\\) is a subspace of \\(\\mathbb{R}^n\\).\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nLet’s check this:\n\nThe zero vector is in \\(H\\)\n\n… because \\({\\bf 0} = 0\\mathbf{v}_1 + 0\\mathbf{v}_2\\),\n… so \\({\\bf 0}\\) is in Span\\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}.\\)\n\n\n\nThe sum of any two vectors in \\(H\\) is in \\(H\\)\n\nIn other words, if \\(\\mathbf{u} = s_1\\mathbf{v}_1 + s_2\\mathbf{v}_2,\\) and \\(\\mathbf{v} = t_1\\mathbf{v}_1 + t_2\\mathbf{v}_2,\\)\n… their sum \\(\\mathbf{u} + \\mathbf{v}\\) is \\((s_1+t_1)\\mathbf{v}_1 + (s_2+t_2)\\mathbf{v}_2,\\)\n… which is in \\(H\\).\n\n\n\nFor any scalar \\(c\\), \\(c\\mathbf{u}\\) is in \\(H\\)\n\nbecause \\(c\\mathbf{u} = c(s_1\\mathbf{v}_1 + s_2\\mathbf{v}_2) = (cs_1\\mathbf{v}_1 + cs_2\\mathbf{v}_2).\\)\n\n\nBecause every span is a subspace, we refer to \\(\\operatorname{Span}\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_p\\}\\) as the subspace spanned by \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_p.\\)\n\nOK, here is another subspace – a line:\n\n\n\n\n\n\n\n\n\n\nNext question: is any line a subspace?\nWhat about a line that is not through the origin?\n\n\n\n\n\n\n\n\n\n\n\nIn fact, a line \\(L\\) not through the origin fails all three requirements for a subspace:\n\n\n\n\\(L\\) does not contain the zero vector.\n\n\n\n\n\\(L\\) is not closed under addition.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(L\\) is not closed under scalar multiplication.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the other hand, any line, plane, or hyperplane through the origin is a subspace.\nMake sure you can see why (or prove it to yourself).",
    "crumbs": [
      "Subspaces"
    ]
  },
  {
    "objectID": "L14Subspaces.html#two-important-subspaces",
    "href": "L14Subspaces.html#two-important-subspaces",
    "title": "Geometric Algorithms",
    "section": "Two Important Subspaces",
    "text": "Two Important Subspaces\n\nNow let’s start to use the subspace concept to characterize matrices.\nWe are thinking of these matrices as linear operators.\n\n\nEvery matrix has associated with it two subspaces:\n\ncolumn space and\nnull space.\n\n\n\nColumn Space\n\nDefinition. The column space of a matrix \\(A\\) is the set \\({\\operatorname{Col}}\\ A\\) of all linear combinations of the columns of \\(A\\).\nIf \\(A\\) = \\([\\mathbf{a}_1 \\;\\cdots\\; \\mathbf{a}_n]\\), with columns in \\(\\mathbb{R}^m,\\) then \\({\\operatorname{Col}}\\ A\\) is the same as Span\\(\\{\\mathbf{a}_1,\\dots,\\mathbf{a}_n\\}.\\)\n\n\nThe column space of an \\(m\\times n\\) matrix is a subspace of \\(\\mathbb{R}^m.\\)\nIn particular, note that \\({\\operatorname{Col}}\\ A\\) equals \\(\\mathbb{R}^m\\) only when the columns of \\(A\\) span \\(\\mathbb{R}^m.\\)\nOtherwise, \\({\\operatorname{Col}}\\ A\\) is only part of \\(\\mathbb{R}^m.\\)\n\n\nWhen a system of linear equations is written in the form \\(A\\mathbf{x} = \\mathbf{b},\\) the column space of \\(A\\) is the set of all \\(\\mathbf{b}\\) for which the system has a solution.\n\n\nEquivalently, when we consider the linear operator \\(T: \\mathbb{R}^n\\rightarrow\\mathbb{R}^m\\) that is implemented by the matrix \\(A\\), the column space of \\(A\\) is the range of \\(T.\\)\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nNull Space\nThe null space of matrix \\(A\\) is the set of all vectors that are mapped to the origin by \\(A\\).\nWe can think of these vectors as having been ‘destroyed’ by \\(A\\) – their information is lost.\n\nFormally, we define the null space as follows:\nDefinition. The null space of a matrix \\(A\\) is the set of all solutions of the homogeneous equation \\(A\\mathbf{x} = 0.\\)\n\n\nWe denote the null space of \\(A\\) as \\(\\operatorname{Nul} A\\).\n\n\nWhen \\(A\\) has \\(n\\) columns, a solution of \\(A\\mathbf{x} = {\\bf 0}\\) is a vector in \\(\\mathbb{R}^n.\\) So the null space of \\(A\\) is a subset of \\(\\mathbb{R}^n.\\)\nIn fact, \\(\\operatorname{Nul} A\\) is a subspace of \\(\\mathbb{R}^n.\\)\n\nTheorem. The null space of an \\(m\\times n\\) matrix \\(A\\) is a subspace of \\(\\mathbb{R}^n.\\)\nEquivalently, the set of all solutions of a system \\(A\\mathbf{x} = {\\bf 0}\\) of \\(m\\) homogeneous linear equations in \\(n\\) unknowns is a subspace of \\(\\mathbb{R}^n.\\)\n\nProof.\n\nThe zero vector is in \\(\\operatorname{Nul} A\\) because \\(A{\\bf 0} = {\\bf 0}.\\)\n\n\n\n\nThe sum of two vectors in \\(\\operatorname{Nul} A\\) is in \\(\\operatorname{Nul} A.\\)\n\nTake two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) that are in \\(\\operatorname{Nul} A.\\) By definition \\(A\\mathbf{u} = {\\bf 0}\\) and \\(A\\mathbf{v} = {\\bf 0}.\\)\nThen \\(\\mathbf{u} + \\mathbf{v}\\) is in \\(\\operatorname{Nul} A\\) because \\(A(\\mathbf{u} + \\mathbf{v}) = A\\mathbf{u} + A\\mathbf{v} = {\\bf 0} + {\\bf 0} = {\\bf 0}.\\)\n\n\n\nAny scalar multiple of a vector in \\({\\operatorname{Nul}}\\ A\\) is in \\({\\operatorname{Nul}}\\ A.\\)\n\nTake a vector \\(\\mathbf{v}\\) that is in \\({\\operatorname{Nul}}\\ A.\\) Then \\(A(c\\mathbf{v}) = cA\\mathbf{v} = c{\\bf 0} = {\\bf 0}.\\)\n\n\nTesting whether a vector \\(\\mathbf{v}\\) is in \\({\\operatorname{Nul}}\\ A\\) is easy: simply compute \\(A\\mathbf{v}\\) and see if the result is zero.\n\n\n\nThere’s a space known as \\(\\operatorname{Nul}\\),  Where vectors are no fun at all,  For when equations we solve,  And zero is the goal,  In null space, the vectors take the fall\n\n\n– ChatGPT, March 2023\n\n\n\nComparing \\({\\operatorname{Col}}\\ A\\) and \\({\\operatorname{Nul}}\\ A\\).\n\nWhat is the relationship between these two subspaces that are defined using \\(A\\)?\n\n\nActually, there is no particular connection (at this moment in the course).\nThe important thing to note at present is that these two subspaces live in different “universes”.\nFor an \\(m\\times n\\) matrix,\n\nthe column space is a subset of \\(\\mathbb{R}^m\\) (all its vectors have \\(m\\) components),\nwhile the null space is a subset of \\(\\mathbb{R}^n\\) (all its vectors have \\(n\\) components).\n\n\n\nHowever: next lecture we will make a connection!",
    "crumbs": [
      "Subspaces"
    ]
  },
  {
    "objectID": "L14Subspaces.html#a-basis-for-a-subspace",
    "href": "L14Subspaces.html#a-basis-for-a-subspace",
    "title": "Geometric Algorithms",
    "section": "A Basis for a Subspace",
    "text": "A Basis for a Subspace\n\nLet’s say you have a subspace.\nFor example, perhaps it is \\(\\operatorname{Span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\mathbf{a}_3\\}\\).\nWe would like to find the simplest way of describing this space.\n\n\nFor example, consider this subspace:\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nNote that \\(\\mathbf{a}_3\\) is a scalar multiple of \\(\\mathbf{a}_2\\). Thus:\n\\(\\operatorname{Span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\mathbf{a}_3\\}\\)\nis the same subspace as\n\\(\\operatorname{Span}\\{\\mathbf{a}_1, \\mathbf{a}_2\\}\\).\nCan you see why?\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nMaking this idea more general:\n\nwe would like to describe a subspace as the span of a set of vectors, and\nthat set of vectors should have the fewest members as possible!\n\n\n\nSo in our example above, we would prefer to say that the subspace is:\n\\(H = \\operatorname{Span}\\{\\mathbf{a}_1, \\mathbf{a}_2\\}\\)\nrather than\n\\(H = \\operatorname{Span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\mathbf{a}_3\\}\\).\nIn other words, the more “concisely” we can describe the subspace, the better.\n\nNow, given some subspace, how small a spanning set can we find?\n\nHere is the key idea we will use to answer that question:\nIt can be shown that the smallest possible spanning set must be linearly independent.\n\n\nWe will call such minimally-small sets of vectors a basis for the space.\n\nDefinition. A basis for a subspace \\(H\\) of \\(\\mathbb{R}^n\\) is a linearly independent set in \\(H\\) that spans \\(H.\\)\n\n\n\n\n\n\n\n\n\n\n\n\nSo in the example above, a basis for \\(H\\) could be:\n\\(\\{\\mathbf{a}_1, \\mathbf{a}_2\\}\\)\nor\n\\(\\{\\mathbf{a}_1, \\mathbf{a}_3\\}.\\)\n\n\nHowever,\n\\(\\{\\mathbf{a}_1, \\mathbf{a}_2, \\mathbf{a}_3\\}\\)\nis not a basis for \\(H\\).\nThat is because \\(\\{\\mathbf{a}_1, \\mathbf{a}_2, \\mathbf{a}_3\\}\\) are not linearly independent.\n(Conceptually, there are “too many vectors” in this set).\n\n\nAnd furthermore,\n\\(\\{\\mathbf{a}_1\\}\\)\nis not a basis for \\(H\\).\nThat is because \\(\\{\\mathbf{a}_1\\}\\) does not span \\(H\\).\n(Conceptually, there are “not enough vectors” in this set).\n\n\nTheir span forms a mathematical feat,  But to understand their place,  In this vectorial space,  We need a basis, oh so complete!  A basis is a set of vectors, you see,  That span the subspace perfectly,  No vector is wasted,  All others are based on this basic,  A strong foundation, for this subspace to be.\n\n– ChatGPT, March 2023\nExample.\nThe columns of any invertible \\(n\\times n\\) matrix form a basis for \\(\\mathbb{R}^n.\\)\nThis is because, by the Invertible Matrix Theorem, they are linearly independent, and they span \\(\\mathbb{R}^n.\\)\n\nSo, for example, we could use the identity matrix, \\(I.\\) It columns are \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_n.\\)\n\n\nThe set \\(\\{\\mathbf{e}_1,\\dots,\\mathbf{e}_n\\}\\) is called the standard basis for \\(\\mathbb{R}^n.\\)",
    "crumbs": [
      "Subspaces"
    ]
  },
  {
    "objectID": "L14Subspaces.html#bases-for-null-and-column-spaces",
    "href": "L14Subspaces.html#bases-for-null-and-column-spaces",
    "title": "Geometric Algorithms",
    "section": "Bases for Null and Column Spaces",
    "text": "Bases for Null and Column Spaces\n\nBeing able to express a subspace in terms of a basis is very powerful.\nIt gives us a concise way of describing the subspace.\nAnd we will see in the next lecture, that it will allow us to introduce ideas of coordinate systems and dimension.\nHence, we will often want to be able to describe subspaces like \\(\\operatorname{Col} A\\) or \\(\\operatorname{Nul} A\\) using their bases.\n\n\nFinding a basis for the Null Space\n\nWe’ll start with finding a basis for the null space of a matrix.\n\n\nExample. Find a basis for the null space of the matrix\n\\[A = \\begin{bmatrix}-3&6&-1&1&-7\\\\1&-2&2&3&-1\\\\2&-4&5&8&-4\\end{bmatrix}.\\]\n\n\nSolution. We would like to describe the set of all solutions of \\(A\\mathbf{x} = {\\bf 0}.\\)\n\n\nWe start by writing the solution of \\(A\\mathbf{x} = {\\bf 0}\\) in parametric form:\n\\[[A \\;{\\bf 0}] \\sim \\begin{bmatrix}1&-2&0&-1&3&0\\\\0&0&1&2&-2&0\\\\0&0&0&0&0&0\\end{bmatrix}, \\;\\;\\;\n\\begin{array}{rrrrrcl}x_1&-2x_2&&-x_4&+3x_5&=&0\\\\&&x_3&+2x_4&-2x_5&=&0\\\\&&&&0&=&0\\end{array}\\]\n\n\nSo \\(x_1\\) and \\(x_3\\) are basic, and \\(x_2, x_4,\\) and \\(x_5\\) are free.\nSo the general solution is:\n\\[\\begin{array}{rcl}x_1&=&2x_2 + x_4 -3x_5,\\\\\nx_3&=&-2x_4 + 2x_5.\\end{array}\\]\n\n\nNow, what we want to do is write the solution set as a weighted combination of vectors.\nThis is a neat trick – we are creating a vector equation.\nThe key idea is that the free variables will become the weights.\n\n\n\\[\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\\\x_5\\end{bmatrix} = \\begin{bmatrix}2x_2 + x_4 - 3x_5\\\\x_2\\\\-2x_4 + 2x_5\\\\x_4\\\\x_5\\end{bmatrix} \\]\n\n\n\\[ =\nx_2\\begin{bmatrix}2\\\\1\\\\0\\\\0\\\\0\\end{bmatrix}+x_4\\begin{bmatrix}1\\\\0\\\\-2\\\\1\\\\0\\end{bmatrix}+x_5\\begin{bmatrix}-3\\\\0\\\\2\\\\0\\\\1\\end{bmatrix} \\]\n\n\n\\[= x_2\\mathbf{u} + x_4\\mathbf{v} + x_5{\\bf w}.\\]\n\n\nNow what we have is an expression that describes the entire solution set of \\(A\\mathbf{x} = {\\bf 0}.\\)\n\n\nSo \\({\\operatorname{Nul}}\\ A\\) is the set of all linear combinations of \\(\\mathbf{u}, \\mathbf{v},\\) and \\({\\bf w}\\). That is, \\({\\operatorname{Nul}}\\ A\\) is the subspace spanned by \\(\\{\\mathbf{u}, \\mathbf{v}, {\\bf w}\\}.\\)\n\n\nFurthermore, this construction automatically makes \\(\\mathbf{u}, \\mathbf{v},\\) and \\({\\bf w}\\) linearly independent.\nSince each weight appears by itself in one position, the only way for the whole weighted sum to be zero is if every weight is zero – which is the definition of linear independence.\n\n\nSo \\(\\{\\mathbf{u}, \\mathbf{v}, {\\bf w}\\}\\) is a basis for \\({\\operatorname{Nul}}\\ A.\\)\n\n\nConclusion: by finding a parametric description of the solution of the equation \\(A\\mathbf{x} = {\\bf 0},\\) we can construct a basis for the nullspace of \\(A\\).\n\n\n\nFinding a Basis for the Column Space\n\nTo find a basis for the column space, we have an easier starting point.\nWe know that the column space is the span of the matrix columns.\nSo, we can choose matrix columns to make up the basis.\nThe question is: which columns should we choose?\n\n\nWarmup.\nWe start with a warmup example.\nSuppose we have a matrix \\(B\\) that happens to be in reduced echelon form:\n\\[B = \\begin{bmatrix}1&0&-3&5&0\\\\0&1&2&-1&0\\\\0&0&0&0&1\\\\0&0&0&0&0\\end{bmatrix}.\\]\n\n\nDenote the columns of \\(B\\) by \\(\\mathbf{b}_1,\\dots,\\mathbf{b}_5\\).\nNote that \\(\\mathbf{b}_3 = -3\\mathbf{b}_1 + 2\\mathbf{b}_2\\) and \\(\\mathbf{b}_4 = 5\\mathbf{b}_1-\\mathbf{b}_2.\\)\n\n\nSo any combination of \\(\\mathbf{b}_1,\\dots,\\mathbf{b}_5\\) is actually just a combination of \\(\\mathbf{b}_1, \\mathbf{b}_2,\\) and \\(\\mathbf{b}_5.\\)\nSo \\(\\{\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_5\\}\\) spans \\({\\operatorname{Col}}\\ B\\).\n\n\nAlso, \\(\\mathbf{b}_1, \\mathbf{b}_2,\\) and \\(\\mathbf{b}_5\\) are linearly independent, because they are columns from an identity matrix.\nSo: the pivot columns of \\(B\\) form a basis for \\({\\operatorname{Col}}\\ B.\\)\n\n\nNote that this means: there is no combination of columns 1, 2, and 5 that yields the zero vector.\n(Other than the trivial combination of course.)\n\n\nSo, for matrices in reduced row echelon form, we have a simple rule for the basis of the column space:\nChoose the columns that hold the pivots.\n\nThe general case.\nNow I’ll show that the pivot columns of \\(A\\) form a basis for \\({\\operatorname{Col}}\\ A\\) for any \\(A\\).\n\nConsider the case where \\(A\\mathbf{x} = {\\bf 0}\\) for some nonzero \\(\\mathbf{x}.\\)\n\n\nThis says that there is a linear dependence relation between some of the columns of \\(A\\).\nIf any of the entries in \\(\\mathbf{x}\\) are zero, then those columns do not participate in the linear dependence relation.\n\n\nWhen we row-reduce \\(A\\) to its reduced echelon form \\(B\\), the columns are changed, but the equations \\(A\\mathbf{x} = {\\bf 0}\\) and \\(B\\mathbf{x} = {\\bf 0}\\) have the same solution set.\nSo this means that the columns of \\(A\\) have exactly the same dependence relationships as the columns of \\(B\\).\n\n\nIn other words:\n\nIf some column of \\(B\\) can be written as a combination of other columns of \\(B\\), then the same is true of the corresponding columns of \\(A\\).\nIf no combination of certain columns of \\(B\\) yields the zero vector, then no combination of corresponding columns of \\(A\\) yields the zero vector.\n\n\n\nIn other words:\n\nIf some set of columns of \\(B\\) spans the column space of \\(B\\), then the same columns of \\(A\\) span the column space of \\(A\\).\nIf some set of columns of \\(B\\) are linearly independent, then the same columns of \\(A\\) are linearly independent.\n\n\n\nExample. Consider the matrix \\(A\\):\n\\[A = \\begin{bmatrix}1&3&3&2&-9\\\\-2&-2&2&-8&2\\\\2&3&0&7&1\\\\3&4&-1&11&-8\\end{bmatrix}\\]\nIt is row equivalent to the matrix \\(B\\) that we considered above. So to find its basis, we simply need to look at the basis for its reduced row echelon form. We already computed that a basis for \\({\\operatorname{Col}}\\ B\\) was columns 1, 2, and 5.\n\n\nTherefore we can immediately conclude that a basis for \\({\\operatorname{Col}}\\ A\\) is \\(A\\)’s columns 1, 2, and 5.\nSo a basis for \\({\\operatorname{Col}}\\ A\\) is:\n\\[\\left\\{\\begin{bmatrix}1\\\\-2\\\\2\\\\3\\end{bmatrix},\\begin{bmatrix}3\\\\-2\\\\3\\\\4\\end{bmatrix},\\begin{bmatrix}-9\\\\2\\\\1\\\\-8\\end{bmatrix}\\right\\}\\]\n\n\nTheorem. The pivot columns of a matrix \\(A\\) form a basis for the column space of \\(A\\).\n\n\nBe careful here – note that we compute the reduced row echelon form of \\(A\\) to find which columns are pivot columns…\nbut we use the columns of \\(A\\) itself as the basis for \\({\\operatorname{Col}}\\ A\\)!",
    "crumbs": [
      "Subspaces"
    ]
  },
  {
    "objectID": "L13ComputerGraphics.html",
    "href": "L13ComputerGraphics.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nComputer Graphics is the study of creating, synthesizing, manipulating, and using visual information in the computer.\nToday we’ll study the mathematics behind computer graphics.\n\n\nIf you are interested in learning more about computer graphics, you can take CS 480 here at BU. A good book for more detail specifically on mathematical aspects is Mathematics for 3D Game Programming and Computer Graphics, by Eric Lengyel.",
    "crumbs": [
      "Computer Graphics"
    ]
  },
  {
    "objectID": "L13ComputerGraphics.html#computer-graphics",
    "href": "L13ComputerGraphics.html#computer-graphics",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nComputer Graphics is the study of creating, synthesizing, manipulating, and using visual information in the computer.\nToday we’ll study the mathematics behind computer graphics.\n\n\nIf you are interested in learning more about computer graphics, you can take CS 480 here at BU. A good book for more detail specifically on mathematical aspects is Mathematics for 3D Game Programming and Computer Graphics, by Eric Lengyel.",
    "crumbs": [
      "Computer Graphics"
    ]
  },
  {
    "objectID": "L13ComputerGraphics.html#computer-graphics-is-everywhere",
    "href": "L13ComputerGraphics.html#computer-graphics-is-everywhere",
    "title": "Geometric Algorithms",
    "section": "Computer Graphics is Everywhere",
    "text": "Computer Graphics is Everywhere\n\nComputer graphics (CG) is pervasive in the world today.\n\n\n\nImage credits: CS 184 Lecture Slides, UC Berkeley, Ng Ren\nCG is used in films and games.\n\nCG is used in science and engineering for product design, visualization and computer-aided design.\n\nCG is used in the arts: graphical user interfaces, digital photography, graphic design, and fine arts.\n\nRemarkably, all of the applications we see here are based, at their core, on linear algebra.\nToday we’ll start to unlock the methods of computer graphics and see how they depend on linear algebra.\n\nTo do that, we’ll go back to the notion of a linear transformation.",
    "crumbs": [
      "Computer Graphics"
    ]
  },
  {
    "objectID": "L13ComputerGraphics.html#linear-transformations",
    "href": "L13ComputerGraphics.html#linear-transformations",
    "title": "Geometric Algorithms",
    "section": "Linear Transformations",
    "text": "Linear Transformations\n\nIn the lecture on linear transformations and matrices we talked about linear transformations of \\(\\mathbb{R}^2\\):\n\nReflection\n\nover the \\(x_1\\) or \\(x_2\\) axes,\nover the lines \\(x_2=x_1\\) or \\(x_2=-x_1\\), and\nthrough the origin.\n\nHorizontal and vertical dilation and contraction.\nHorizontal and vertical shearing.\nProjection onto the \\(x_1\\) or \\(x_2\\) axes or onto a line through the origin.\n\nToday, we’ll talk about 3D transformations: linear transformations on \\(\\mathbb{R}^3\\).\n\nThe computer graphics seen in movies and videogames works in three stages:\n\nA 3D model of the scene objects is created;\nThe model is converted into (many small) polygons in 3D that approximate the surfaces of the model; and\nThe polygons are transformed via a linear transformation to yield a 2D representation that can be shown on a flat screen.\n\n\nThere is interesting mathematics in each stage, but the transformations that take place in the third stage are linear, and that’s what we’ll study today.\n\n\nInitially, object models may be expressed in terms of smooth functions like polynomials.\nHowever the first step is to convert those smooth functions into a set of discrete pieces – coordinates and line segments.\nAll subsequent processing is done in terms of the discrete coordinates that approximate the shape of the original model.\nThe reason for this conversion is that most transformations needed in graphics are linear.\nExpressing the scene in terms of coordinates is equivalent to expressing it in terms of vectors, that is, in \\(\\mathbb{R}^3\\).\nAnd linear transformations on vectors are always matrix multiplications, so implementation is simple and uniform.\nThe resulting representation consists of lists of 3D coordinates called faces. Each face is a polygon.\nThe lines drawn between coordinates are implied by the way that coordinates are grouped into faces.\n\nThe Coordinate System\nThe standard coordinate system used in computer graphics places the axes in this relationship:\n\n\nNote the relative position of three axes. We generally think of the \\(z\\)-axis as coming out of the screen towards us.\n\n\nWe consider the columns of the identity – \\(\\mathbf{e}_1, \\mathbf{e}_2,\\) and \\(\\mathbf{e}_3\\) – to be associated with the \\(x\\), \\(y\\), and \\(z\\) directions in this figure.\n\n\n\nAn Example\nHere is a view of a ball-like object. It is centered at the origin.\nThe ball is represented in terms of 3D coordinates, but we are only plotting the \\(x\\) and \\(y\\) coordinates here.\nColors correspond to faces.",
    "crumbs": [
      "Computer Graphics"
    ]
  },
  {
    "objectID": "L13ComputerGraphics.html#rotation",
    "href": "L13ComputerGraphics.html#rotation",
    "title": "Geometric Algorithms",
    "section": "Rotation",
    "text": "Rotation\n\nImagine that we want to circle the camera around the ball by moving sideways. In terms of what we would see through the camera, this is equivalent to rotating the ball around the \\(y\\) axis.\n\n\nRotation is a linear transformation. We can implement it by multiplying the coordinates of the ball by a rotation matrix. To define the rotation matrix, we need to think about what happens to each of the columns of \\(I: \\mathbf{e}_1, \\mathbf{e}_2,\\) and \\(\\mathbf{e}_3.\\)\n\n\nTo rotate through an angle of \\(\\alpha\\) radians around the \\(y\\) axis,\nthe vector \\({\\bf e_1} = \\left[\\begin{array}{r}1\\\\0\\\\0\\end{array}\\right]\\) goes to \\(\\left[\\begin{array}{c}\\cos \\alpha\\\\0\\\\-\\sin \\alpha\\end{array}\\right].\\)\nOf course, \\({\\bf e_2}\\) is unchanged.\nAnd \\({\\bf e_3} = \\left[\\begin{array}{r}0\\\\0\\\\1\\end{array}\\right]\\) goes to \\(\\left[\\begin{array}{c}\\sin \\alpha\\\\0\\\\\\cos \\alpha\\end{array}\\right].\\)\n\n\nSo the entire rotation matrix is:\n\\[\\begin{bmatrix}\\cos \\alpha&0&\\sin \\alpha\\\\0&1&0\\\\-\\sin \\alpha&0&\\cos\\alpha\\end{bmatrix}.\\]\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "Computer Graphics"
    ]
  },
  {
    "objectID": "L13ComputerGraphics.html#translation",
    "href": "L13ComputerGraphics.html#translation",
    "title": "Geometric Algorithms",
    "section": "Translation",
    "text": "Translation\n\nManipulating graphics objects using matrix multiplication is very convenient.\n\n\nHowever, there is a commonly used operation that is not a linear transformation: translation – that is, movement in space.\n\n\nRemember that a transformation \\(T(x)\\) is linear if \\(T(x + y) = T(x) + T(y)\\) and \\(T(cx) = cT(x)\\).\nNow, if \\(T(x)\\) is a translation, then \\(T(x) = x + b\\) for some nonzero \\(b\\).\n\n\nBut then \\(T(x + y) \\neq T(x) + T(y)\\).\nSo a translation is not a linear transformation.\n\n\nThere is a nice way to avoid this difficulty, while keeping things linear.\nWe will use what are called homogeneous coordinates.\n\n\nWhen using homogeneous coordinates, we add another component to the vector representing a point.\n\n\nThe coordinates of the point in \\(\\mathbb{R}^4\\) are the homogeneous coordinates for the point in \\(\\mathbb{R}^3.\\)\n\n\nThe extra component gives us a constant that we can scale and add to the other coordinates, as needed, via matrix multiplication.\n\n\nThis means for 3D graphics, all transformation matrices are \\(4\\times 4.\\)\n\n\nExample\nLet’s say we want to move a point \\((x, y, z)\\) to location \\((x+h, y+k, z+m).\\)\n\nWe represent the point in homogeneous coordinates as \\(\\left[\\begin{array}{r}x\\\\y\\\\z\\\\1\\end{array}\\right].\\)\n\n\nThe transformation corresponding to this ‘translation’ is:\n\\[\\left[\\begin{array}{cccc}1&0&0&h\\\\0&1&0&k\\\\0&0&1&m\\\\0&0&0&1\\end{array}\\right]\\left[\\begin{array}{r}x\\\\y\\\\z\\\\1\\end{array}\\right] = \\left[\\begin{array}{c}x+h\\\\y+k\\\\z+m\\\\1\\end{array}\\right].\\]\n\n\nIf we only consider \\(x, y,\\) and \\(z\\) this is not a linear transformation. But of course, in \\(\\mathbb{R}^4\\) this most definitely is a linear transformation.\nWe have ‘sheared’ in the fourth dimension, which affects the other three. A very useful trick!\n\n\n\nConstructing Matrices for Homogeneous Coordinates\nFor any transformation \\(A\\) that is linear in \\(\\mathbb{R}^3\\) (such as scaling, rotation, reflection, shearing, etc.), we can construct the corresponding matrix for homogeneous coordinates quite simply:\n\nIf\n\\[\nA = \\begin{bmatrix}\\blacksquare&\\blacksquare&\\blacksquare\\\\\\blacksquare&\\blacksquare&\\blacksquare\\\\\\blacksquare&\\blacksquare&\\blacksquare\\end{bmatrix}\n\\]\n\n\nThen the corresponding transformation for homogeneous coordinates is:\n\\[\\begin{bmatrix}\\blacksquare&\\blacksquare&\\blacksquare&0\\\\\\blacksquare&\\blacksquare&\\blacksquare&0\\\\\\blacksquare&\\blacksquare&\\blacksquare&0\\\\0&0&0&1\\end{bmatrix}\\]\n\n\nIn other words, when performing a linear transformation on \\(x, y\\), and \\(z\\), one simply ‘carries along’ the extra coordinate without modifying it.",
    "crumbs": [
      "Computer Graphics"
    ]
  },
  {
    "objectID": "L13ComputerGraphics.html#matrices-for-3d-transformations",
    "href": "L13ComputerGraphics.html#matrices-for-3d-transformations",
    "title": "Geometric Algorithms",
    "section": "Matrices for 3D Transformations",
    "text": "Matrices for 3D Transformations\n\n\\[\\begin{bmatrix} s_x&0&0&0 \\\\ 0&s_y&0&0 \\\\0&0&s_z&0 \\\\0&0&0&1\\end{bmatrix}\\]\n\n\nTranslation\n\\[\\begin{bmatrix} 1&0&0&h\\\\0&1&0&k\\\\0&0&1&m\\\\0&0&0&1\\end{bmatrix}\\]\n\n\nRotation around x-, y-, z- axis couterclockwise and looking towards the origin\n\\[R_x(\\alpha)=\\begin{bmatrix}1&0&0&0\\\\ 0&\\cos \\alpha&-\\sin \\alpha&0\\\\0&\\sin \\alpha&\\cos\\alpha&0\\\\0&0&0&1\\end{bmatrix}.\\]\n\\[R_y(\\alpha)=\\begin{bmatrix}\\cos \\alpha&0&\\sin \\alpha&0\\\\0&1&0&0\\\\-\\sin \\alpha&0&\\cos\\alpha&0\\\\0&0&0&1\\end{bmatrix}.\\]\n\\[R_z(\\alpha)=\\begin{bmatrix}\\cos \\alpha&-\\sin \\alpha&0&0\\\\\\sin \\alpha&\\cos\\alpha&0&0\\\\0&0&1&0\\\\0&0&0&1\\end{bmatrix}.\\]",
    "crumbs": [
      "Computer Graphics"
    ]
  },
  {
    "objectID": "L13ComputerGraphics.html#perspective-projections",
    "href": "L13ComputerGraphics.html#perspective-projections",
    "title": "Geometric Algorithms",
    "section": "Perspective Projections",
    "text": "Perspective Projections\n\nThere is another nonlinear transformation that is important in computer graphics: perspective.\n\n\nHappily, we will see that homogeneous coordinates allow us to capture this too as a linear transformation in \\(\\mathbb{R}^4\\).\n\n\nThe eye, or a camera, captures light (essentially) in a single location, and hence gathers light rays that are converging.\nSo, to portray a scene with realistic appearance, it is necessary to reproduce this effect.\nThe effect can be thought of as “nearer objects are larger than further objects.”\n\n\nThis was (re)discovered by Renaissance artists (supposedly first by Filippo Brunelleschi, around 1405).\nHere is a famous example: Raphael’s School of Athens (1510).\n\n\n\nImages from here.\n\n\nWe now understand that this effect interacts in a powerful way with neural circuitry in our brains.\nThe mind reconstructs a sense of three dimensions from the two dimensional information presented to it by the retina.\nThis is done by sophisticated processing in the visual cortex, which is a fairly large portion of the brain.\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nNotice that when the image is stationary, it appears flat (like a picture frame). As soon as it starts to move, it springs into 3D in perception.\n\n\nComputing Perspective\nTo render a scene with perspective, we simply have to consider how objects appear when viewed from a point (like an eye or a camera).\n\n\nFigures from Linear Alegebra and its Applications, Lay, Lay, and MacDonald.\n\nA standard setup for computing a perspective transformation is as shown in this figure:\n\n\n\nFor simplicity, we will let the \\(xy\\)-plane represent the screen. This is called the viewing plane.\nThe eye/camera is located on the \\(z\\) axis, at the point \\((0,0,d)\\). This is call the center of projection.\n\n\nA perspective projection maps each point \\((x,y,z)\\) onto an image point \\((x^*,y^*,0)\\) so that the center of projection and the two points are all on the same line.\n\n\n\n\n\nWe can compute the projection transformation using similar triangles.\nThe triangle in the \\(xz\\)-plane shows the lengths of corresponding line segments.\n\n\nSimilar triangles show that\n\\[\\frac{x^*}{d} = \\frac{x}{d-z}\\]\n\n\nand so\n\\[x^*=\\frac{dx}{d-z}\\;=\\;\\frac{x}{1-z/d}.\\]\n\n\nNotice that the function \\(T: (x,z)\\mapsto\\frac{x}{1-z/d}\\) is not linear.\n\n\nHowever, using homogeneous coordinates, we can construct a linear version of \\(T\\) in \\(\\mathbb{R}^4.\\)\n\n\nTo do so, we establish the following convention: we will allow the fourth coordinate to vary away from 1.\nHowever, when we plot, for a point \\(\\begin{bmatrix}x\\\\y\\\\z\\\\\\end{bmatrix}\\) we will plot the point \\(\\begin{bmatrix}\\frac{x}{h}\\\\\\frac{y}{h}\\\\\\frac{z}{h}\\end{bmatrix}.\\)\n\n\nIn this way, by dividing the \\(x,y,z\\) coordinates by the \\(h\\) coordinate, we can implement a nonlinear transform in \\(\\mathbb{R}^3.\\)\n\n\nSo, to implement the perspective transform, we want \\(\\begin{bmatrix}x\\\\y\\\\z\\\\1\\end{bmatrix}\\) to map to \\(\\begin{bmatrix}\\frac{x}{1-z/d}\\\\\\frac{y}{1-z/d}\\\\0\\\\1\\end{bmatrix}.\\)\n\n\nThe way we will implement this is to actually cause it to map to \\(\\begin{bmatrix}x\\\\y\\\\0\\\\1-z/d\\end{bmatrix}.\\)\nThen, when we plot (dividing \\(x\\) and \\(y\\) by the \\(h\\) value) we will get the proper transform.\n\n\nThe matrix that implements this transformation is quite simple:\n\\[\\begin{bmatrix}1&0&0&0\\\\0&1&0&0\\\\0&0&0&0\\\\0&0&-1/d&1\\end{bmatrix}.\\]\nSo:\n\\[\\begin{bmatrix}1&0&0&0\\\\0&1&0&0\\\\0&0&0&0\\\\0&0&-1/d&1\\end{bmatrix}\\begin{bmatrix}x\\\\y\\\\z\\\\1\\end{bmatrix} = \\begin{bmatrix}x\\\\y\\\\0\\\\1-z/d\\end{bmatrix}.\\]",
    "crumbs": [
      "Computer Graphics"
    ]
  },
  {
    "objectID": "L13ComputerGraphics.html#composing-transformations",
    "href": "L13ComputerGraphics.html#composing-transformations",
    "title": "Geometric Algorithms",
    "section": "Composing Transformations",
    "text": "Composing Transformations\n\nOne big payoff for casting all graphics operations as linear transformations comes in the composition of transformations.\n\n\nConsider two linear transformations \\(T\\) and \\(S\\). For example, \\(T\\) could be a scaling and \\(S\\) could be a rotation. Assume \\(S\\) is implemented by a matrix \\(A\\) and \\(T\\) is implemented by a matrix \\(B\\).\n\n\nTo first scale and then rotate a vector \\(\\mathbf{x}\\) we would compute \\(S(T(\\mathbf{x}))\\).\nOf course this is implemented as \\(A(B\\mathbf{x}).\\)\n\n\nBut note that this is the same as \\((AB)\\mathbf{x}.\\) In other words, \\(AB\\) is a single matrix that both scales and rotates \\(\\mathbf{x}\\).\n\n\nBy extension, we can combine any arbitrary sequence of linear transformations into a single matrix. This greatly simplifies high-speed graphics.\nNote though that if \\(C = AB\\), then \\(C\\) is the transformation that first applies \\(B\\), then applies \\(A\\) (order matters).\n\nExample. Let’s work in homogenous coordinates for points in \\(\\mathbb{R}^2\\). Find the \\(3\\times 3\\) matrix that corresponds to the composite transformation of first scaling by 0.3, then rotation of 90\\(^\\circ\\) about the origin, and finally a translation of \\(\\begin{bmatrix}-0.5\\\\2\\end{bmatrix}\\) to each point of a figure.\n\n\n\n\n\n\n\n\n\nWe’ll use this triangle to show the sequence of transformations.\n\n\n\n\n\n\n\n\n\nScaling by 0.3.\n\n\n\n\n\n\n\n\n\nScaling by 0.3, then rotating by 90\\(^\\circ\\) about the origin.\n\n\n\n\n\n\n\n\n\nScaling by 0.3, then rotation of 90\\(^\\circ\\) about the origin, then translation of \\(\\begin{bmatrix}-0.5\\\\2\\end{bmatrix}\\).\nLet’s see how to compose these three transformations into a single matrix multiplication.\n\nThe scaling matrix is:\n\\[S = \\begin{bmatrix}0.3&0&0\\\\0&0.3&0\\\\0&0&1\\end{bmatrix}\\]\n\n\nThe rotation matrix is:\n\\[R = \\begin{bmatrix}0&-1&0\\\\1&0&0\\\\0&0&1\\end{bmatrix}\\]\n(Note that \\(\\sin 90^\\circ = 1\\) and \\(\\cos 90^\\circ = 0.\\))\n\n\nThe translation matrix is:\n\\[T = \\begin{bmatrix}1&0&-0.5\\\\0&1&2\\\\0&0&1\\end{bmatrix}\\]\n\n\nSo the matrix for the composite transformation is:\n\\[TRS = \\begin{bmatrix}1&0&-0.5\\\\0&1&2\\\\0&0&1\\end{bmatrix}\\begin{bmatrix}0&-1&0\\\\1&0&0\\\\0&0&1\\end{bmatrix}\\begin{bmatrix}0.3&0&0\\\\0&0.3&0\\\\0&0&1\\end{bmatrix} = \\begin{bmatrix}0&-0.3&-0.5\\\\0.3&0&2\\\\0&0&1\\end{bmatrix}.\\]\n\n\nNote that \\(TRS\\) first applies S (scaling), then applis R (rotation), and last applies T (translation).\n\nWe need to be careful with the order of the matrices!\nConsider a vector \\(\\mathbf{x}\\in \\mathbb{R}^2\\).\nLet \\(P\\) be projection matrix that projects \\(\\mathbf{x}\\) onto the x-axis and \\(R\\) be a rotation matrix that rotates \\(\\mathbf{x}\\) by 30\\(^\\circ\\) about the origin.\nNote that \\(PR \\ne RP\\):\n\n\\(PR\\) first rotates the vector and then applies projection, while\n\\(RP\\) first projects the vector and then applies rotation.",
    "crumbs": [
      "Computer Graphics"
    ]
  },
  {
    "objectID": "L13ComputerGraphics.html#homework-7",
    "href": "L13ComputerGraphics.html#homework-7",
    "title": "Geometric Algorithms",
    "section": "Homework 7",
    "text": "Homework 7\nHomework 7 will allow you to explore these ideas by computing various transformation matrices. I will give you some 3D wireframe objects and you will compute the necessary matrices to create a simple animation.\n\nHere is what your finished product will look like:\n\n\n\n \n Your browser does not support the video tag.",
    "crumbs": [
      "Computer Graphics"
    ]
  },
  {
    "objectID": "L13ComputerGraphics.html#fast-computation-of-linear-systems-using-graphics-processing-units-gpus",
    "href": "L13ComputerGraphics.html#fast-computation-of-linear-systems-using-graphics-processing-units-gpus",
    "title": "Geometric Algorithms",
    "section": "Fast Computation of Linear Systems using Graphics Processing Units (GPUs)",
    "text": "Fast Computation of Linear Systems using Graphics Processing Units (GPUs)\n\n\n\nSources in this section.\n\n\n\nExamples of Other Linear Systems Used in Computer Graphics\nCloth Simulation\nIn cloth simulation, we treat the cloth as a collection of particles interconnected with different types of springs. The springs cause the cloth to behave properly with respect to bending, shearing, etc.\nHooke’s Law and Newton’s 2nd Law are used as the equations of motion to capture the cloth behaviors. These are linear difference equations.\n\n\n\n\n\n\n\n\n\n\n        \n        \n\n\n\nHair Simulation\nA similar strategy is used for hair, except that the connections are 1-D (along each hair strand) instead of 2-D.\n\n\n\n\n\n\n\n\n\nConvolution\nInterestingly, the fact that GPUs are optimized for fast convolution was crucial in their adoption for fast machine learning.\n\n\n\n\n\nSource",
    "crumbs": [
      "Computer Graphics"
    ]
  },
  {
    "objectID": "L16Eigenvectors.html",
    "href": "L16Eigenvectors.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\n\n\n\n\n\n\n\n\n\nDuring the time period shown, the number of CS majors was growing very rapidly. I needed to have some estimate of where the numbers might be going.\nModeling the number of students in the CS major is challenging because students enter and leave the major at various points during their undergraduate degree.\n\n\nWe will use a linear difference model that we will train on historical data.\nOur state variable is\n\\[\\mathbf{x}_t = \\begin{bmatrix}x_{1,t}\\\\ x_{2,t}\\\\ x_{3,t}\\\\ x_{4,t}\\end{bmatrix}\\]\nwhere \\(x_{i,t}\\) is the number of students who are declared CS majors and in their \\(i\\)th year at BU, in the fall of year \\(t\\).\nOur model is a linear dynamical system:\n\\[ \\mathbf{x}_{t+1} = A \\mathbf{x}_t.\\]\n\n\nGiven historical data which are measurements of our state variable from past years, we can estimate \\(A\\) by a method called least squares.\n(We will cover least squares later in the semester.)\n\n\nHere is the \\(A\\) that we get when look at historical data:\n\nprint(A)\n\n[[ 0.62110633  0.28792787  0.0204491   0.10003897]\n [ 0.65239203  0.55574243  0.30323349 -0.16349735]\n [ 0.33101614  1.24636712 -0.26153545  0.07684781]\n [ 0.49319575 -0.30684656  1.00419585  0.07532737]]\n\n\n\n\nNow, using \\(A\\), we can predict growth into the future via \\(\\mathbf{x}_{t+1} = A \\mathbf{x}_t.\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice how the matrix \\(A\\) captures the complex way that one year’s student population relates to the next year’s population.\nBecause of this complexity, can be hard to understand what is going on just by looking at the values of \\(A\\).\nBut, when we look at the plot above, it appears that there is a relatively simple kind of exponential growth going on.\nCan we “extract” this exponential growth model from \\(A\\)? How?\n\n\nToday’s lecture will start to develop the tools to answer such questions (and more).",
    "crumbs": [
      "Eigenvectors and Eigenvalues"
    ]
  },
  {
    "objectID": "L16Eigenvectors.html#eigenvectors-and-eigenvalues",
    "href": "L16Eigenvectors.html#eigenvectors-and-eigenvalues",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\n\n\n\n\n\n\n\n\n\nDuring the time period shown, the number of CS majors was growing very rapidly. I needed to have some estimate of where the numbers might be going.\nModeling the number of students in the CS major is challenging because students enter and leave the major at various points during their undergraduate degree.\n\n\nWe will use a linear difference model that we will train on historical data.\nOur state variable is\n\\[\\mathbf{x}_t = \\begin{bmatrix}x_{1,t}\\\\ x_{2,t}\\\\ x_{3,t}\\\\ x_{4,t}\\end{bmatrix}\\]\nwhere \\(x_{i,t}\\) is the number of students who are declared CS majors and in their \\(i\\)th year at BU, in the fall of year \\(t\\).\nOur model is a linear dynamical system:\n\\[ \\mathbf{x}_{t+1} = A \\mathbf{x}_t.\\]\n\n\nGiven historical data which are measurements of our state variable from past years, we can estimate \\(A\\) by a method called least squares.\n(We will cover least squares later in the semester.)\n\n\nHere is the \\(A\\) that we get when look at historical data:\n\nprint(A)\n\n[[ 0.62110633  0.28792787  0.0204491   0.10003897]\n [ 0.65239203  0.55574243  0.30323349 -0.16349735]\n [ 0.33101614  1.24636712 -0.26153545  0.07684781]\n [ 0.49319575 -0.30684656  1.00419585  0.07532737]]\n\n\n\n\nNow, using \\(A\\), we can predict growth into the future via \\(\\mathbf{x}_{t+1} = A \\mathbf{x}_t.\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice how the matrix \\(A\\) captures the complex way that one year’s student population relates to the next year’s population.\nBecause of this complexity, can be hard to understand what is going on just by looking at the values of \\(A\\).\nBut, when we look at the plot above, it appears that there is a relatively simple kind of exponential growth going on.\nCan we “extract” this exponential growth model from \\(A\\)? How?\n\n\nToday’s lecture will start to develop the tools to answer such questions (and more).",
    "crumbs": [
      "Eigenvectors and Eigenvalues"
    ]
  },
  {
    "objectID": "L16Eigenvectors.html#eigenvectors-and-eigenvalues-1",
    "href": "L16Eigenvectors.html#eigenvectors-and-eigenvalues-1",
    "title": "Geometric Algorithms",
    "section": "Eigenvectors and Eigenvalues",
    "text": "Eigenvectors and Eigenvalues\n\nAs we’ve seen, linear transformations (thinking geometrically) can “move” a vector to a new location.\n\n\nFor example, a linear transformation \\(A\\mathbf{x}\\) can do the following to \\(\\mathbf{x}\\):\n\nrotate \\(\\mathbf{x}\\)\nreflect \\(\\mathbf{x}\\)\nshear \\(\\mathbf{x}\\)\nproject \\(\\mathbf{x}\\)\nscale \\(\\mathbf{x}\\)\n\n\n\nOf the above transformations, one is a bit different: scaling.\nThat is because, if a matrix \\(A\\) scales \\(\\mathbf{x}\\),\nthat transformation could also have been expressed without a matrix-vector multiplication\n\n\nthat is, a scalinbg can be simply written as \\(\\lambda\\mathbf{x}\\) for some scalar value \\(\\lambda\\).\n\n\nIn many ways, life would be simpler if we could avoid all of the complexities of rotation, reflection, and projection … and only work in terms of scaling.\nThat is the idea we will develop today.\n\nLet’s start with a simple example: a shear matrix:\n\\[ A = \\begin{bmatrix}1&0.1\\\\0&1\\end{bmatrix} \\]\nLet’s look at what this matrix does to each point in the plane.\n\n\n\n\n\n\n\n\n\nWe can use arrows to show how each red point is moved to the corresponding blue point.\n\n\n\n\n\n\n\n\n\nNow let’s look at a more complicated situation.\nConsider the matrix \\(A = \\begin{bmatrix}1.0&0.2\\\\0.1&0.9\\end{bmatrix}\\).\n\n\n\n\n\n\n\n\n\nThe starting point for understanding what \\(A\\) does is to look for special vectors which do not change their direction when multiplied by \\(A\\).\n\nExample.\nLet \\(A = \\begin{bmatrix}3&-2\\\\1&0\\end{bmatrix}, \\mathbf{u} = \\begin{bmatrix}-1\\\\1\\end{bmatrix}, \\mathbf{v}=\\begin{bmatrix}2\\\\1\\end{bmatrix}.\\) The images of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) under multiplication by \\(A\\) are shown here:\n\n\n\n\n\n\n\n\n\n\n\nFor a “typical” vector (like \\(\\mathbf{u}\\)) the action of \\(A\\) is to scale, reflect, and/or rotate it.\nBut for some special vectors (like \\(\\mathbf{v}\\)) the action of \\(A\\) is only to “stretch” it (without any rotation or reflection).\nIn fact, \\(A\\mathbf{v}\\) is simply \\(2\\mathbf{v}.\\)\n\n\nToday we will by studying equations such as\n\\[ A\\mathbf{x} = 2\\mathbf{x}\\;\\;\\;\\;\\text{or}\\;\\;\\;\\;A\\mathbf{x} = -4\\mathbf{x}\\]\nwhere special vectors are transformed by \\(A\\) into scalar multiples of themselves.\n\nDefinitions.\n\nAn eigenvector of an \\(n\\times n\\) matrix \\(A\\) is a nonzero vector \\(\\mathbf{x}\\) such that \\(A\\mathbf{x} = \\lambda\\mathbf{x}\\) for some scalar \\(\\lambda.\\)\n\n\nA scalar \\(\\lambda\\) is called an eigenvalue of \\(A\\) if there is a nontrivial solution \\(\\mathbf{x}\\) of \\(A\\mathbf{x} = \\lambda\\mathbf{x}.\\)\n\n\nSuch an \\(\\mathbf{x}\\) is called an eigenvector corresponding to \\(\\lambda.\\)",
    "crumbs": [
      "Eigenvectors and Eigenvalues"
    ]
  },
  {
    "objectID": "L16Eigenvectors.html#checking-eigenvectors-and-eigenvalues",
    "href": "L16Eigenvectors.html#checking-eigenvectors-and-eigenvalues",
    "title": "Geometric Algorithms",
    "section": "Checking Eigenvectors and Eigenvalues",
    "text": "Checking Eigenvectors and Eigenvalues\n\nBefore considering how to compute eigenvectors, let’s look at simply how to determine if a given vector is an eigenvector of a matrix.\n\n\nExample. Let \\(A = \\begin{bmatrix}1&6\\\\5&2\\end{bmatrix}, \\mathbf{u} = \\begin{bmatrix}6\\\\-5\\end{bmatrix}, \\mathbf{v}=\\begin{bmatrix}3\\\\-2\\end{bmatrix}.\\)\nAre \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) eigenvectors of \\(A\\)?\n\n\nSolution.\n\\[A\\mathbf{u} = \\begin{bmatrix}1&6\\\\5&2\\end{bmatrix}\\begin{bmatrix}6\\\\-5\\end{bmatrix} = \\begin{bmatrix}-24\\\\20\\end{bmatrix} = -4\\begin{bmatrix}6\\\\-5\\end{bmatrix} = -4\\mathbf{u}.\\]\n\n\n\\[A\\mathbf{v} = \\begin{bmatrix}1&6\\\\5&2\\end{bmatrix}\\begin{bmatrix}3\\\\-2\\end{bmatrix} = \\begin{bmatrix}-9\\\\11\\end{bmatrix} \\neq \\lambda\\begin{bmatrix}3\\\\-2\\end{bmatrix}.\\]\n\n\nSo \\(\\mathbf{u}\\) is an eigenvector corresponding to the eigenvalue \\(-4\\),\nbut \\(\\mathbf{v}\\) is not an eigenvector of \\(A\\).",
    "crumbs": [
      "Eigenvectors and Eigenvalues"
    ]
  },
  {
    "objectID": "L16Eigenvectors.html#finding-eigenvectors",
    "href": "L16Eigenvectors.html#finding-eigenvectors",
    "title": "Geometric Algorithms",
    "section": "Finding Eigenvectors",
    "text": "Finding Eigenvectors\nOur first task will be to find eigenvectors. We’ll start by working through an example.\n\nExample. Let’s use the same matrix \\(A = \\begin{bmatrix}1&6\\\\5&2\\end{bmatrix}.\\)\nWe’ll now show that 7 is an eigenvalue of the matrix \\(A\\), and we will find the corresponding eigenvectors.\n\n\nFor 7 to be an eigenvalue of \\(A\\), it must be the case that\n\\[ A\\mathbf{x}=7\\mathbf{x}\\]\nhas a nontrivial solution.\n\n\nThis is equivalent to:\n\\[ A\\mathbf{x} - 7\\mathbf{x} = {\\bf 0}\\]\n\n\nor\n\\[ (A - 7I)\\mathbf{x} = {\\bf 0}.\\]\n\n\nTo solve this homogeneous equation, form the matrix\n\\[A - 7I = \\begin{bmatrix}1&6\\\\5&2\\end{bmatrix}-\\begin{bmatrix}7&0\\\\0&7\\end{bmatrix} = \\begin{bmatrix}-6&6\\\\5&-5\\end{bmatrix}.\\]\n\n\nTo see that \\((A - 7I)\\mathbf{x} = {\\bf 0}\\) has a nontrivial solution, observe that the columns of \\(A-7I\\) are obviously linearly dependent.\nSo there are nonzero vectors that are solutions of \\((A - 7I)\\mathbf{x} = {\\bf 0}\\).\nSo 7 is an eigenvalue of \\(A\\).\n\n\nTo find the corresponding eigenvectors, we solve \\((A - 7I)\\mathbf{x} = {\\bf 0}\\) using row operations:\n\\[\\begin{bmatrix}-6&6&0\\\\5&-5&0\\end{bmatrix}\\sim\\begin{bmatrix}1&-1&0\\\\0&0&0\\end{bmatrix}.\\]\n\n\nThis says that \\(x_1 = x_2,\\) and \\(x_2\\) is free.\nSo the general solution has the form \\(x_2\\begin{bmatrix}1\\\\1\\end{bmatrix}.\\)\nEach vector of this form with \\(x_2 \\neq 0\\) is an eigenvector corresponding to \\(\\lambda = 7.\\)",
    "crumbs": [
      "Eigenvectors and Eigenvalues"
    ]
  },
  {
    "objectID": "L16Eigenvectors.html#eigenspace",
    "href": "L16Eigenvectors.html#eigenspace",
    "title": "Geometric Algorithms",
    "section": "Eigenspace",
    "text": "Eigenspace\n\nNow, let’s generalize what we just did.\nFrom the previous example, we can conclude that \\(\\lambda\\) is an eigenvalue of an \\(n\\times n\\) matrix \\(A\\) if and only if the equation\n\\[(A-\\lambda I)\\mathbf{x} = {\\bf 0}\\]\nhas a nontrivial solution.\n\n\nNotice that the set of all solutions of that equation is just\nthe null space of the matrix \\(A-\\lambda I.\\)\n\n\nSo the set of all eigenvectors corresponding to a particular \\(\\lambda\\) (along with the trivial solution, that is the \\(\\mathbf{0}\\) vector) forms a subspace of \\(\\mathbb{R}^n\\),\nand is called the eigenspace of \\(A\\) corresponding to \\(\\lambda\\).\n\nExample. For matrix \\(A = \\begin{bmatrix}1&6\\\\5&2\\end{bmatrix},\\) we found that the general solution for the eigenvector corresponding to \\(\\lambda = 7\\) is the expression \\(x_2\\begin{bmatrix}1\\\\1\\end{bmatrix}.\\)\n\nThis means that the eigenspace corresponding to \\(\\lambda = 7\\) consist|s of all multiples of \\(\\begin{bmatrix}1\\\\1\\end{bmatrix},\\)\n… in other words \\(\\operatorname{Span}\\left\\{\\begin{bmatrix}1\\\\1\\end{bmatrix}\\right\\}\\).\n\n\nSo we would also say that \\(\\left\\{\\begin{bmatrix}1\\\\1\\end{bmatrix}\\right\\}\\) is a basis for the eigenspace corresponding to \\(\\lambda = 7\\).\n\n\nWe could also show that the eigenspace corresponding to \\(\\lambda = -4\\) is \\(\\operatorname{Span}\\left\\{\\begin{bmatrix}-6\\\\5\\end{bmatrix}\\right\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nExample. Let \\(A = \\begin{bmatrix}4&-1&6\\\\2&1&6\\\\2&-1&8\\end{bmatrix}.\\) An eigenvalue of \\(A\\) is \\(2.\\) Find a basis for the the corresponding eigenspace.\n\nSolution. The eigenspace we are looking for is the nullspace of \\(A - 2I.\\)\nWe will use the strategy for finding a basis for a nullspace that we learned in the last lecture.\n\n\nForm\n\\[A - 2I = \\begin{bmatrix}4&-1&6\\\\2&1&6\\\\2&-1&8\\end{bmatrix}-\\begin{bmatrix}2&0&0\\\\0&2&0\\\\0&0&2\\end{bmatrix} = \\begin{bmatrix}2&-1&6\\\\2&-1&6\\\\2&-1&6\\end{bmatrix}\\]\n\n\nand row reduce the augmented matrix for \\((A-2I)\\mathbf{x}={\\bf 0}\\):\n\\[\\begin{bmatrix}2&-1&6&0\\\\2&-1&6&0\\\\2&-1&6&0\\end{bmatrix} \\sim \\begin{bmatrix}2&-1&6&0\\\\0&0&0&0\\\\0&0&0&0\\end{bmatrix}\\]\n\n\nAt this point, it is clear that 2 is indeed an eigenvalue of \\(A\\),\nbecause the equation \\((A-2I)\\mathbf{x} = {\\bf 0}\\) has free variables.\n\n\nThe general solution is \\(x_1 = \\frac{1}{2}x_2 -3x_3.\\)\nExpressed as a vector, the general solution is:\n\\[\\begin{bmatrix}\\frac{1}{2}x_2 - 3x_3\\\\x_2\\\\x_3\\end{bmatrix}.\\]\n\n\nExpressed as a vector sum, this is:\n\\[x_2\\begin{bmatrix}1/2\\\\1\\\\0\\end{bmatrix} + x_3\\begin{bmatrix}-3\\\\0\\\\1\\end{bmatrix}\\;\\;\\;\\;\\text{with $x_2$ and $x_3$ free.}\\]\n\n\nSo this eigenspace is a two-dimensional subspace of \\(\\mathbb{R}^3,\\) and a basis for this subspace is\n\\[\\left\\{\\begin{bmatrix}1/2\\\\1\\\\0\\end{bmatrix},\\begin{bmatrix}-3\\\\0\\\\1\\end{bmatrix}\\right\\}.\\]\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "Eigenvectors and Eigenvalues"
    ]
  },
  {
    "objectID": "L16Eigenvectors.html#eigenvalues-of-a-triangular-matrix",
    "href": "L16Eigenvectors.html#eigenvalues-of-a-triangular-matrix",
    "title": "Geometric Algorithms",
    "section": "Eigenvalues of a Triangular Matrix",
    "text": "Eigenvalues of a Triangular Matrix\n\nTheorem. The eigenvalues of a triangular matrix are the entries on its main diagonal.\n\n\nProof. We’ll consider the \\(3\\times 3\\) case. If \\(A\\) is upper triangular, then \\(A-\\lambda I\\) has the form\n\\[A - \\lambda I = \\begin{bmatrix}a_{11}&a_{12}&a_{13}\\\\0&a_{22}&a_{23}\\\\0&0&a_{33}\\end{bmatrix} - \\begin{bmatrix}\\lambda&0&0\\\\0&\\lambda&0\\\\0&0&\\lambda\\end{bmatrix}\\]\n\n\n\\[ = \\begin{bmatrix}a_{11}-\\lambda&a_{12}&a_{13}\\\\0&a_{22}-\\lambda&a_{23}\\\\0&0&a_{33}-\\lambda\\end{bmatrix}\\]\n\n\nNow, \\(\\lambda\\) is an eigenvalue of \\(A\\) if and only if the equation \\((A-\\lambda I)\\mathbf{x} = {\\bf 0}\\) has a nontrivial solution –\nin other words, if and only if the equation has a free variable.\n\n\nThe special pattern that the zero entries have in \\(A\\) means that there will be a free variable if any diagonal entry is also zero, because then there will be a column without a pivot.\n\n\nThat is, \\((A-\\lambda I)\\mathbf{x} = {\\bf 0}\\) has a free variable if and only if at least one of the entries on the diagonal of \\((A-\\lambda I)\\) is zero.\n\n\nThis happens if and only if \\(\\lambda\\) equals one of the entries \\(a_{11}, a_{22},\\) or \\(a_{33}\\) on the diagonal of \\(A\\).\n\n\nExample. Let \\(A = \\begin{bmatrix}3&6&-8\\\\0&0&6\\\\0&0&2\\end{bmatrix}\\) and \\(B = \\begin{bmatrix}4&0&0\\\\-2&1&0\\\\5&3&4\\end{bmatrix}\\).\n\n\nThen the eigenvalues of \\(A\\) are 3, 0, and 2. The eigenvalues of \\(B\\) are 4 and 1.",
    "crumbs": [
      "Eigenvectors and Eigenvalues"
    ]
  },
  {
    "objectID": "L16Eigenvectors.html#invertibility-and-eigenvalues",
    "href": "L16Eigenvectors.html#invertibility-and-eigenvalues",
    "title": "Geometric Algorithms",
    "section": "Invertibility and Eigenvalues",
    "text": "Invertibility and Eigenvalues\n\nSo far we haven’t probed what it means for a matrix to have an eigenvalue of 0.\n\n\nThis happens if and only if the equation \\(A\\mathbf{x} = 0\\mathbf{x}\\) has a nontrivial solution.\n\n\nBut that equation is equivalent to \\(A\\mathbf{x} = {\\bf 0}\\) which has a nontrivial solution if and only if \\(A\\) is not invertible.\n\n\nThis means that\n\n0 is an eigenvalue of A if and only if A is not invertible.\n\n\n\nThis draws an important connection between invertibility and zero eigenvalues.\n\n\nSo we have yet another addition to the Invertible Matrix Theorem!",
    "crumbs": [
      "Eigenvectors and Eigenvalues"
    ]
  },
  {
    "objectID": "L16Eigenvectors.html#eigenvectors-solve-difference-equations",
    "href": "L16Eigenvectors.html#eigenvectors-solve-difference-equations",
    "title": "Geometric Algorithms",
    "section": "Eigenvectors Solve Difference Equations",
    "text": "Eigenvectors Solve Difference Equations\n\nLet’s return to the problem we considered at the outset: predicting future values of \\(\\mathbf{x}_t\\) (the number of CS majors of each class in year \\(t\\)).\n\n\nWe modeled the future as \\(\\mathbf{x}_{t+1} = A\\mathbf{x}_t\\;\\;\\;(t = 0,1,2,\\dots).\\)\nSo then \\(\\mathbf{x}_{t} = A^t\\mathbf{x}_0\\;\\;\\;(t = 0,1,2,\\dots).\\)\n\n\nA solution of such an equation is an explicit description of \\(\\{\\mathbf{x}_t\\}\\) whose formula for each \\(\\mathbf{x}_t\\) does not depend directly on \\(A\\) or on the preceding terms in the sequence other than the initial term \\(\\mathbf{x}_0.\\)\n\n\nThe simplest way to build a solution is to take an eigenvector \\(\\mathbf{x}_0\\) and its corresponding eigenvalue \\(\\lambda\\).\nThen\n\\[ \\mathbf{x}_t = \\lambda^t \\mathbf{x}_0 \\;\\;\\;(t = 0,1,2,\\dots)\\]\n\n\nThis sequence is a solution because\n\\[A\\mathbf{x}_t = A(\\lambda^t\\mathbf{x}_0) = \\lambda^t(A\\mathbf{x}_0) = \\lambda^t(\\lambda\\mathbf{x}_0) = \\lambda^{t+1}\\mathbf{x}_0 = \\mathbf{x}_{t+1}.\\]\n\nWe can extend this to combinations of eigenvectors.\n\nIf \\(A\\) has two eigenvectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) corresponding to eigenvalues \\(\\lambda\\) and \\(\\mu\\), then another solution is\n\\[\\mathbf{x}_t = \\lambda^t\\mathbf{u} + \\mu^t \\mathbf{v}\\]\n\n\nTo address the problem of predicting the CS major population,\nwe assume that we can express the state vector \\(\\mathbf{x}\\) as a linear combination of eigenvectors \\(\\mathbf{u}_1, \\mathbf{u}_2, \\dots,\\mathbf{u}_p\\).\nThat is,\n\\[\\mathbf{x}_0 = a_1\\mathbf{u}_1 + a_2\\mathbf{u}_2+\\dots+a_p\\mathbf{u}_p.\\]\n\n\nThen at time \\(t\\),\n\\[\\mathbf{x}_t = \\lambda_1^ta_1\\mathbf{u}_1 + \\lambda_2^ta_2\\mathbf{u}_2 + \\dots + \\lambda_p^ta_p\\mathbf{u}_p.\\]\nWhich gives us an explicit solution for the number of students in the major in any future year.\n\n\nNote that the expression above is exponential in each of the \\(\\lambda_1,\\dots,\\lambda_p\\).\n\n\nThe resulting value will be essentially determined by the largest \\(\\lambda_i\\).\nTo see this, let’s say that \\(\\lambda_1\\) is the largest eigenvalue, and \\(\\lambda_2\\) is the second largest.\n\n\nThen the proportion of \\(\\mathbf{u}_1\\) to \\(\\mathbf{u}_2\\) contained in \\(\\mathbf{x}_t\\) will be given by \\((\\lambda_1/\\lambda_2)^t\\).\nSince this ratio is larger than 1, the relative proportion of \\(\\mathbf{u}_1\\) will grow exponentially.\n\n\nThis shows the importance of the largest eigenvalue in the value of \\(\\mathbf{x}_t\\) for large \\(t\\).\nIn the long run, \\(\\mathbf{x}_t\\) will be very close to \\(\\lambda_1^t\\mathbf{u}_1.\\)\n\n\nThe largest eigenvalue shows the asymptotic rate of growth (or shrinkage) of the state vector \\(\\mathbf{x}_t\\).\n\n\nFor the matrix \\(A\\) that describes CS major population growth, the largest eigenvalue is 1.19.\nIn other words, this models the population growth rate of CS majors at that time as increasing about 19% per year.",
    "crumbs": [
      "Eigenvectors and Eigenvalues"
    ]
  },
  {
    "objectID": "L17CharacteristicEqn.html",
    "href": "L17CharacteristicEqn.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nToday we deepen our study of linear dynamical systems,\nsystems that evolve according to the equation:\n\\[\\mathbf{x}_{k+1} = A\\mathbf{x}_k.\\]\n\nLet’s look at some examples of how such dynamical systems can evolve in \\(\\mathbb{R}^2\\).\n\nFirst we’ll look at the system corresponding to:\n\\[ A = \\begin{bmatrix}\\cos 0.1&-\\sin 0.1\\\\\\sin 0.1&\\cos 0.1\\end{bmatrix} \\]\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nNext let’s look at the system corresponding to:\n\\[ A = \\begin{bmatrix}1.1&0\\\\0&0.9\\end{bmatrix} \\]\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nNext let’s look at the system corresponding to:\n\\[ A = \\begin{bmatrix}0.8&0.5\\\\-0.1&1.0\\end{bmatrix} \\]\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nThere are very different things happening in these three cases!\nCan we find a general method for understanding what is going on in each case?\n\nThe study of eigenvalues and eigenvectors is the key to acquiring that understanding.\n\n\nWe will see that to understand each case, we need to learn how to extract the eigenvalues and eigenvectors of \\(A\\).",
    "crumbs": [
      "The Characteristic Equation"
    ]
  },
  {
    "objectID": "L17CharacteristicEqn.html#the-characteristic-equation",
    "href": "L17CharacteristicEqn.html#the-characteristic-equation",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nToday we deepen our study of linear dynamical systems,\nsystems that evolve according to the equation:\n\\[\\mathbf{x}_{k+1} = A\\mathbf{x}_k.\\]\n\nLet’s look at some examples of how such dynamical systems can evolve in \\(\\mathbb{R}^2\\).\n\nFirst we’ll look at the system corresponding to:\n\\[ A = \\begin{bmatrix}\\cos 0.1&-\\sin 0.1\\\\\\sin 0.1&\\cos 0.1\\end{bmatrix} \\]\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nNext let’s look at the system corresponding to:\n\\[ A = \\begin{bmatrix}1.1&0\\\\0&0.9\\end{bmatrix} \\]\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nNext let’s look at the system corresponding to:\n\\[ A = \\begin{bmatrix}0.8&0.5\\\\-0.1&1.0\\end{bmatrix} \\]\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nThere are very different things happening in these three cases!\nCan we find a general method for understanding what is going on in each case?\n\nThe study of eigenvalues and eigenvectors is the key to acquiring that understanding.\n\n\nWe will see that to understand each case, we need to learn how to extract the eigenvalues and eigenvectors of \\(A\\).",
    "crumbs": [
      "The Characteristic Equation"
    ]
  },
  {
    "objectID": "L17CharacteristicEqn.html#finding-lambda",
    "href": "L17CharacteristicEqn.html#finding-lambda",
    "title": "Geometric Algorithms",
    "section": "Finding \\(\\lambda\\)",
    "text": "Finding \\(\\lambda\\)\n\nIn the last lecture we saw that, if we know an eigenvalue \\(\\lambda\\) of a matrix \\(A,\\) then computing the corresponding eigenspace can be done by constructing a basis for\n\\[\\operatorname{Nul}\\, (A-\\lambda I).\\]\n\n\nToday we’ll discuss how to determine the eigenvalues of a matrix \\(A\\).\n\n\nThe theory will make use of the determinant of a matrix.\n\nLet’s recall that the determinant of a \\(2\\times 2\\) matrix \\(A = \\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}\\) is \\(ad-bc.\\)\n\nWe also have learned that \\(A\\) is invertible if and only if its determinant is not zero.\nAlso recall that the inverse of of \\(A\\) is \\(\\frac{1}{ad-bc}\\begin{bmatrix}d&-b\\\\-c&a\\end{bmatrix}.\\)\n\n\nLet’s use these facts to help us find the eigenvalues of a \\(2\\times 2\\) matrix.\n\nExample.\nLet’s find the eigenvalues of \\(A = \\begin{bmatrix}2&3\\\\3&-6\\end{bmatrix}.\\)\n\nSolution. We must find all scalars \\(\\lambda\\) such that the matrix equation\n\\[(A-\\lambda I)\\mathbf{x} = {\\bf 0}\\]\nhas a nontrivial solution.\n\n\nBy the Invertible Matrix Theorem, this problem is equivalent to finding all \\(\\lambda\\) such that the matrix \\(A-\\lambda I\\) is not invertible.\n\n\nNow,\n\\[ A - \\lambda I = \\begin{bmatrix}2&3\\\\3&-6\\end{bmatrix} - \\begin{bmatrix}\\lambda&0\\\\0&\\lambda\\end{bmatrix} = \\begin{bmatrix}2-\\lambda&3\\\\3&-6-\\lambda\\end{bmatrix}.\\]\n\n\nWe know that \\(A\\) is not invertible exactly when its determinant is zero.\n\n\nSo the eigenvalues of \\(A\\) are the solutions of the equation\n\\[\\det(A-\\lambda I) = \\det\\begin{bmatrix}2-\\lambda&3\\\\3&-6-\\lambda\\end{bmatrix} = 0.\\]\n\n\nSince \\(\\det\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix} = ad-bc,\\) then\n\\[\\det(A-\\lambda I) = (2-\\lambda)(-6-\\lambda)-(3)(3)\\]\n\n\n\\[ = -12 + 6\\lambda -2\\lambda + \\lambda^2 - 9\\]\n\n\n\\[= \\lambda^2+4\\lambda-21\\]\n\n\n\\[=(\\lambda-3)(\\lambda + 7)\\]\n\n\nIf \\(\\det(A-\\lambda I) = 0,\\) then \\(\\lambda = 3\\) or \\(\\lambda = -7.\\) So the eigenvalues of \\(A\\) are \\(3\\) and \\(-7\\).\n\nThe same idea works for \\(n\\times n\\) matrices – but, for that, we need to define a determinant for larger matrices.\n\nDeterminants of Larger Matrices\nPreviously, we’ve defined a determinant for a \\(2\\times 2\\) matrix.\n\nTo find eigenvalues for larger matrices, we need to define the determinant for any sized (ie, \\(n\\times n\\)) matrix.\n\n\nDefinition. Let \\(A\\) be an \\(n\\times n\\) matrix, and let \\(U\\) be any echelon form obtained from \\(A\\) by row replacements and row interchanges (no row scalings), and let \\(r\\) be the number of such row interchanges.\n\n\nThen the determinant of \\(A\\), written as \\(\\det A\\), is \\((-1)^r\\) times the product of the diagonal entries \\(u_{11},\\dots,u_{nn}\\) in \\(U\\).\n\n\nIf \\(A\\) is invertible, then \\(u_{11},\\dots,u_{nn}\\) are all pivots.\n\n\nIf \\(A\\) is not invertible, then at least one diagonal entry is zero, and so the product \\(u_{11} \\dots u_{nn}\\) is zero.\n\n\nIn other words:\n\\[\\det\\ A = \\left\\{\\begin{array}{ll}(-1)^r\\cdot\\left(\\text{product of pivots in $U$}\\right),&\\text{when $A$ is invertible}\\\\\n0,&\\text{when $A$ is not invertible}\\end{array}\\right.\\]\n\nExample. Compute \\(\\det A\\) for \\(A = \\begin{bmatrix}1&5&0\\\\2&4&-1\\\\0&-2&0\\end{bmatrix}.\\)\n\nSolution. The following row reduction uses one row interchange:\n\\[A \\sim \\begin{bmatrix}1&5&0\\\\0&-6&-1\\\\0&-2&0\\end{bmatrix} \\sim \\begin{bmatrix}1&5&0\\\\0&-2&0\\\\0&-6&-1\\end{bmatrix} \\sim \\begin{bmatrix}1&5&0\\\\0&-2&0\\\\0&0&-1\\end{bmatrix}.\\]\n\n\nSo \\(\\det A\\) equals \\((-1)^1(1)(-2)(-1) = (-2).\\)\n\n\nThe remarkable thing is that any other way of computing the echelon form gives the same determinant. For example, this row reduction does not use a row interchange:\n\\[A \\sim \\begin{bmatrix}1&5&0\\\\0&-6&-1\\\\0&-2&0\\end{bmatrix} \\sim \\begin{bmatrix}1&5&0\\\\0&-6&-1\\\\0&0&1/3\\end{bmatrix}.\\]\n\n\nUsing this echelon form to compute the determinant yields \\((-1)^0(1)(-6)(1/3) = -2,\\) the same as before.\n\n\n\nInvertibility\n\nThe formula for the determinant shows that \\(A\\) is invertible if and only if \\(\\det A\\) is nonzero.\n\n\nWe have yet another part to add to the Invertible Matrix Theorem:\n\n\nLet \\(A\\) be an \\(n\\times n\\) matrix. Then \\(A\\) is invertible if and only if:\n\nThe determinant of \\(A\\) is not zero.\nThe number 0 is not an eigenvalue of \\(A\\).\n\nThe second fact is true because if zero is an eigenvalue of \\(A\\), then \\(\\det A = \\det(A - \\lambda I) = 0.\\)\n\n\n\nTo see that \\(\\det AB = (\\det A) (\\det B),\\) think about \\(A\\) and \\(B\\) as linear transformations. Then \\(AB\\) is the composition of those transformations. So the amount that \\(AB\\) scales the unit hypercube is the product of the scaling performed by \\(A\\) and the scaling performed by \\(B\\).\nSome facts about determinants:\n\n\\(\\det AB = (\\det A) (\\det B).\\)\n\\(\\det A^T = \\det A.\\)\nIf \\(A\\) is triangular, then \\(\\det A\\) is the product of the entries on the main diagonal of \\(A\\).",
    "crumbs": [
      "The Characteristic Equation"
    ]
  },
  {
    "objectID": "L17CharacteristicEqn.html#the-characteristic-equation-1",
    "href": "L17CharacteristicEqn.html#the-characteristic-equation-1",
    "title": "Geometric Algorithms",
    "section": "The Characteristic Equation",
    "text": "The Characteristic Equation\n\nSo, \\(A\\) is invertible if and only if \\(\\det A\\) is not zero.\n\n\nTo return to the question of how to compute eigenvalues of \\(A,\\) recall that \\(\\lambda\\) is an eigenvalue if and only if \\((A-\\lambda I)\\) is not invertible.\n\n\nWe capture this fact using the characteristic equation:\n\\[\\det(A-\\lambda I) = 0.\\]\n\n\nWe can conclude that \\(\\lambda\\) is an eigenvalue of an \\(n\\times n\\) matrix \\(A\\) if and only if \\(\\lambda\\) satisfies the characteristic equation \\(\\det(A-\\lambda I) = 0.\\)\n\nExample. Find the characteristic equation of\n\\[A = \\begin{bmatrix}5&-2&6&-1\\\\0&3&-8&0\\\\0&0&5&4\\\\0&0&0&1\\end{bmatrix}\\]\n\nSolution. Form \\(A - \\lambda I,\\) and note that \\(\\det A\\) is the product of the entries on the diagonal of \\(A,\\) if \\(A\\) is triangular.\n\n\n\\[\\det(A-\\lambda I) = \\det\\begin{bmatrix}5-\\lambda&-2&6&-1\\\\0&3-\\lambda&-8&0\\\\0&0&5-\\lambda&4\\\\0&0&0&1-\\lambda\\end{bmatrix}\\]\n\n\n\\[=(5-\\lambda)(3-\\lambda)(5-\\lambda)(1-\\lambda).\\]\n\n\nSo the characteristic equation is:\n\\[(\\lambda-5)^2(\\lambda-3)(\\lambda-1) = 0.\\]\n\n\nExpanding this out we get:\n\\[\\lambda^4 - 14\\lambda^3 + 68 \\lambda^2 - 130\\lambda + 75 = 0.\\]\n\n\nNotice that, once again, \\(\\det(A-\\lambda I)\\) is a polynomial in \\(\\lambda\\).\n\n\nIn fact, for any \\(n\\times n\\) matrix, \\(\\det(A-\\lambda I)\\) is a polynomial of degree \\(n\\), called the characteristic polynomial of \\(A\\).\n\n\nWe say that the eigenvalue 5 in this example has multiplicity 2, because \\((\\lambda -5)\\) occurs two times as a factor of the characteristic polynomial. In general, the mutiplicity of an eigenvalue \\(\\lambda\\) is its multiplicity as a root of the characteristic equation.\n\nExample. The characteristic polynomial of a \\(6\\times 6\\) matrix is \\(\\lambda^6 - 4\\lambda^5 - 12\\lambda^4.\\) Find the eigenvalues and their multiplicity.\n\nSolution Factor the polynomial\n\\[\\lambda^6 - 4\\lambda^5 - 12\\lambda^4 = \\lambda^4(\\lambda^2-4\\lambda-12) = \\lambda^4(\\lambda-6)(\\lambda+2)\\]\nSo the eigenvalues are 0 (with multiplicity 4), 6, and -2.\n\n\nSince the characteristic polynomial for an \\(n\\times n\\) matrix has degree \\(n,\\) the equation has \\(n\\) roots, counting multiplicities – provided complex numbers are allowed.\n\n\nNote that even for a real matrix, eigenvalues may sometimes be complex.\n\n\nPractical Issues.\nThese facts show that there is, in principle, a way to find eigenvalues of any matrix. However, you need not compute eigenvalues for matrices larger than \\(2\\times 2\\) by hand. For a matrix \\(3\\times 3\\) or larger, you will generally use a computer (unless the matrix has special structure).",
    "crumbs": [
      "The Characteristic Equation"
    ]
  },
  {
    "objectID": "L17CharacteristicEqn.html#similarity",
    "href": "L17CharacteristicEqn.html#similarity",
    "title": "Geometric Algorithms",
    "section": "Similarity",
    "text": "Similarity\nAn important concept for things that come later is the notion of similar matrices.\n\nDefinition. If \\(A\\) and \\(B\\) are \\(n\\times n\\) matrices, then \\(A\\) is similar to \\(B\\) if there is an invertible matrix \\(P\\) such that \\(P^{-1}AP = B,\\) or, equivalently, \\(A = PBP^{-1}.\\)\n\n\nSimilarity is symmetric, so if \\(A\\) is similar to \\(B\\), then \\(B\\) is similar to \\(A\\). Hence we just say that \\(A\\) and \\(B\\) are similar.\nChanging \\(A\\) into \\(B\\) is called a similarity transformation.\n\nAn important way to think of similarity between \\(A\\) and \\(B\\) is that they have the same eigenvalues.\n\nTheorem. If \\(n\\times n\\) matrices \\(A\\) and \\(B\\) are similar, then they have the same characteristic polynomial, and hence the same eigenvalues (with the same multiplicities.)\n\n\nProof. If \\(B = P^{-1}AP,\\) then\n\\[B - \\lambda I = P^{-1}AP - \\lambda P^{-1}P\\]\n\n\n\\[ = P^{-1}(AP-\\lambda P)\\]\n\n\n\\[ = P^{-1}(A-\\lambda I)P\\]\n\n\nNow let’s construct the characteristic polynomial by taking the determinant:\n\\[\\det(B-\\lambda I) = \\det[P^{-1}(A-\\lambda I)P]\\]\n\n\nUsing the properties of determinants we discussed earlier, we compute:\n\\[ = \\det(P^{-1})\\cdot\\det(A-\\lambda I)\\cdot\\det(P).\\]\n\n\nSince \\(\\det(P^{-1})\\cdot\\det(P) = \\det(P^{-1}P) = \\det I = 1,\\) we can see that\n\\[\\det(B-\\lambda I) = \\det(A - \\lambda I).\\]",
    "crumbs": [
      "The Characteristic Equation"
    ]
  },
  {
    "objectID": "L17CharacteristicEqn.html#markov-chains",
    "href": "L17CharacteristicEqn.html#markov-chains",
    "title": "Geometric Algorithms",
    "section": "Markov Chains",
    "text": "Markov Chains\nLet’s return to the problem of solving a Markov Chain.\n\nAt this point, we can place the theory of Markov Chains into the broader context of eigenvalues and eigenvectors.\n\nTheorem. The largest eigenvalue of a Markov Chain is 1.\n\nProof. First of all, it is obvious that 1 is an eigenvalue of a Markov chain since we know that every Markov Chain \\(A\\) has a steady-state vector \\(\\mathbf{v}\\) such that \\(A\\mathbf{v} = \\mathbf{v}.\\)\n\n\n\nPreviously we only stated that every Markov chain has a steady state. Now we can prove it: a steady state of \\(A\\) will have eigenvalue 1. So to prove the existence of the steady-state, we need to show that \\(A-\\lambda I = A-I\\) has a nontrivial nullspace. This can be easily seen since every column of \\(A-I\\) sums to zero. So the bottom row of \\(A-I\\) is a linear combination of the rows above, meaning that we would get at least one row of all zeros in the row echelon form of \\(A-I\\). So \\((A-I)\\mathbf{x} = \\mathbf{0}\\) has a nontrivial solution set, which is the eigenspace of steady states of \\(A\\).\n\nTo prove that 1 is the largest eigenvalue, recall that each column of a Markov Chain sums to 1.\n\n\nSo, every row of \\(A^T\\) sums to 1.\nHence, for any vector \\(\\mathbf{x}\\), no element of \\(A^T\\mathbf{x}\\) can be greater than the largest element of \\(\\mathbf{x}\\). This is because each element of \\(A^T\\mathbf{x}\\) is the weighted sum of \\(\\mathbf{x}\\) with weights adding up to 1.\nSo \\(A^T\\) cannot have an eigenvalue greater than 1.\n\n\nNow, any matrix \\(A\\) and \\(A^T\\) have the same eigenvalues.\nTo see this, suppose \\(\\lambda\\) is an eigenvalue of \\(A\\). Then det(\\(A-\\lambda I\\)) = 0.\nBut det(\\(A-\\lambda I\\)) = det\\(((A-\\lambda I)^T)\\) = det(\\(A^T-\\lambda I\\)).\nSo then \\(\\lambda\\) is an eigenvalue of \\(A^T\\).\n\n\nSo there can be no \\(\\lambda &gt; 1\\) such that \\(A\\mathbf{x} = \\lambda \\mathbf{x}.\\)\n\n\nThe Complete Solution for a Markov Chain\n\nPreviously, we were only able to ask about the “eventual” steady state of a Markov Chain.\nBut another crucial question is: how quickly does a particular Markov Chain reach steady state from some initial starting condition?\nBy completely solving a Markov Chain, we will determine both its steady state and its rate of convergence.\n\nLet’s use an example: we previously studied the Markov Chain defined by\n\\[A = \\begin{bmatrix}0.95&0.03\\\\0.05&0.97\\end{bmatrix}.\\]\n\nLet’s ask how quickly it reaches steady state, from the starting point defined as\n\\[\\mathbf{x}_0 = \\begin{bmatrix}0.6\\\\0.4\\end{bmatrix}.\\]\n\n\nRemember that \\(\\mathbf{x}_0\\) is probability vector – its entries are nonnegative and sum to 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the methods we studied today, we find the characteristic equation, which is:\n\\[\\lambda^2 -1.92\\lambda +0.92 \\]\n\n\nUsing the quadratic formula, we find the roots of this equation to be 1 and 0.92.\n(Note that, as expected, the largest eigenvalue of the chain is 1.)\n\n\nNext, using the methods in the previous lecture, we find a basis for each eigenspace of \\(A\\) (each nullspace of \\(A-\\lambda I\\)).\nFor \\(\\lambda = 1\\), a corresponding eigenvector is \\(\\mathbf{v}_1 = \\begin{bmatrix}3\\\\5\\end{bmatrix}.\\)\nFor \\(\\lambda = 0.92\\), a corresponding eigenvector is \\(\\mathbf{v}_2 = \\begin{bmatrix}1\\\\-1\\end{bmatrix}.\\)\n\n\nNext, we write \\(\\mathbf{x}_0\\) as a linear combination of \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2.\\) This can be done because \\(\\{\\mathbf{v}_1,\\mathbf{v}_2\\}\\) is obviously a basis for \\(\\mathbb{R}^2.\\)\n\n\nTo write \\(\\mathbf{x}_0\\) this way, we want to solve the vector equation\n\\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 = \\mathbf{x}_0\\]\nIn other words:\n\\[[\\mathbf{v}_1\\;\\mathbf{v}_2]\\begin{bmatrix}c_1\\\\c_2\\end{bmatrix} = \\mathbf{x}_0.\\]\n\n\nThe matrix \\([\\mathbf{v}_1\\;\\mathbf{v}_2]\\) is invertible, so,\n\\[\\begin{bmatrix}c_1\\\\c_2\\end{bmatrix} = [\\mathbf{v}_1\\;\\mathbf{v}_2]^{-1} \\mathbf{x}_0 = \\begin{bmatrix}3&1\\\\5&-1\\end{bmatrix}^{-1}\\begin{bmatrix}0.6\\\\0.4\\end{bmatrix}.\\]\n\n\n\\[ = \\frac{1}{-8}\\begin{bmatrix}-1&-1\\\\-5&3\\end{bmatrix}\\begin{bmatrix}0.6\\\\0.4\\end{bmatrix} = \\begin{bmatrix}0.125\\\\0.225\\end{bmatrix}.\\]\n\n\nSo, now we can put it all together.\nWe know that\n\\[\\mathbf{x}_0 = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2\\]\nand we know the values of \\(c_1, c_2\\) that make this true.\n\n\nSo let’s compute each \\(\\mathbf{x}_k\\):\n\\[\\mathbf{x}_1 = A\\mathbf{x}_0 = c_1A\\mathbf{v}_1 + c_2A\\mathbf{v}_2\\]\n\n\n\\[ = c_1\\mathbf{v}_1 + c_2(0.92)\\mathbf{v}_2.\\]\n\n\nNow note the power of the eigenvalue approach:\n\\[\\mathbf{x}_2 = A\\mathbf{x}_1 = c_1A\\mathbf{v}_1 + c_2(0.92)A\\mathbf{v}_2\\]\n\n\n\\[=c_1\\mathbf{v}_1 + c_2(0.92)^2\\mathbf{v}_2.\\]\n\n\nAnd so in general:\n\\[\\mathbf{x}_k = c_1\\mathbf{v}_1 + c_2(0.92)^k\\mathbf{v}_2\\;\\;\\;(k = 0, 1, 2, \\dots)\\]\n\n\nAnd using the \\(c_1\\) and \\(c_2\\) and \\(\\mathbf{v}_1,\\) \\(\\mathbf{v}_2\\) we computed above:\n\n\n\\[\\mathbf{x}_k = 0.125\\begin{bmatrix}3\\\\5\\end{bmatrix} + 0.225(0.92)^k\\begin{bmatrix}1\\\\-1\\end{bmatrix}\\;\\;\\;(k = 0, 1, 2, \\dots)\\]\n\n\nThis explicit formula for \\(\\mathbf{x}_k\\) gives the solution of the Markov Chain \\(\\mathbf{x}_{k+1} = A\\mathbf{x}_k\\) starting from the initial state \\(\\mathbf{x}_0.\\)\n\n\nIn other words:\n\\[\\mathbf{x}_0 = 0.125\\begin{bmatrix}3\\\\5\\end{bmatrix} + 0.225\\begin{bmatrix}1\\\\-1\\end{bmatrix}\\]\n\\[\\mathbf{x}_1 = 0.125\\begin{bmatrix}3\\\\5\\end{bmatrix} + 0.207\\begin{bmatrix}1\\\\-1\\end{bmatrix}\\]\n\\[\\mathbf{x}_2 = 0.125\\begin{bmatrix}3\\\\5\\end{bmatrix} + 0.190\\begin{bmatrix}1\\\\-1\\end{bmatrix}\\]\n\\[\\mathbf{x}_3 = 0.125\\begin{bmatrix}3\\\\5\\end{bmatrix} + 0.175\\begin{bmatrix}1\\\\-1\\end{bmatrix}\\]\n\\[ ... \\]\n\\[\\mathbf{x}_\\infty = 0.125\\begin{bmatrix}3\\\\5\\end{bmatrix}\\]\n\n\nThe equation tells us how quickly the chain converges:\nit converges like \\((0.92)^k\\)\n… which of course goes to zero as \\(k \\rightarrow \\infty\\).\n\n\nThus \\(\\mathbf{x}_k \\rightarrow 0.125\\mathbf{v}_1 = \\begin{bmatrix}0.375\\\\0.625\\end{bmatrix}.\\)",
    "crumbs": [
      "The Characteristic Equation"
    ]
  },
  {
    "objectID": "L11MarkovChains.html",
    "href": "L11MarkovChains.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\n\nAndrei Markov, 1856 - 1922, St Petersburg.\n\n\n\nAAMarkov by unknown; originally uploaded to en-wikipedia by en:User:Mhym - From the official web site of the Russian Academy of Sciences: http://www.ras.ru/win/db/show_per.asp?P=.id-53175.ln-en.dl-.pr-inf.uk-0. Licensed under Public Domain via Wikimedia Commons.\nMarkov was part of the great tradition of mathematics in Russia.\nMarkov started out working in number theory but then got interested in probability. He enjoyed poetry and the great Russian poet Pushkin. Markov studied the sequence of letters found in the text of Eugene Onegin, in particular the sequence of consonants and vowels.\nHe sought a way to describe patterns in sequences, such as text like Eugene Onegin. This eventually led to the idea of a system in which one transitions between states, and the probability of going to another state depends only on the current state.\nHence, Markov pioneered the study of systems in which the future state of the system depends only on the present state in a random fashion. This has turned out to be a terrifically useful idea. For example, it is the starting point for analysis of the movement of stock prices, and the dynamics of animal populations.\nThese have since been termed “Markov Chains.”\nMarkov chains are essential tools in understanding, explaining, and predicting phenomena in computer science, physics, biology, economics, and finance.\nToday we will study an application of linear algebra. You will see how the concepts we use, such as vectors and matrices, get applied to a particular problem.\nMany applications in computing are concerned with how a system behaves over time.\nThink of a Web server that is processing requests for Web pages, or network that is moving packets from place to place.\nWe would like to describe how systems like these operate, and analyze them to understand their performance limits.\n\nA typical way to model a time-evolving system is:\n\ndefine some vector that describes the state of the system, and\nformulate a rule that specifies how to compute the next state of the system based on the current state of the system.\n\n\nSo we would say that the state of the system at time \\(k\\) is a vector \\({\\bf x_k} \\in \\mathbb{R}^n,\\) and\n\\[{\\bf x_{k+1}} = T({\\bf x_k}),\\;\\;\\;\\text{for time}\\;k=0,1,2...\\]\nwhere \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n.\\)\n\n\nThe vector \\({\\bf x_k}\\) is called the state vector.\n\n\nOf course, we are going to be particularly interested in the case where \\(T\\) is a linear transformation.\nThen we know that we can write the difference equation as:\n\\[ {\\bf x_{k+1}} = A{\\bf x_k},\\]\nwhere \\(A \\in \\mathbb{R}^{n\\times n}.\\)\nThis is a linear difference equation.\n\nExample.\nHere is our warm-up problem.\nWe are interested in the population of two regions, say the city and the suburbs.\nFix an initial year (say 2000) and let\n\\[ {\\bf x_0} = \\left[\\begin{array}{cc}\\text{population of the city in 2000}\\\\\\text{population of the suburbs in 2000}\\end{array}\\right].\\]\n\nThen\n\\[ {\\bf x_1} = \\left[\\begin{array}{cc}\\text{population of the city in 2001}\\\\\\text{population of the suburbs in 2001}\\end{array}\\right],\\]\n\\[{\\bf x_2} = \\left[\\begin{array}{cc}\\text{population of the city in 2002}\\\\\\text{population of the suburbs in 2002}\\end{array}\\right],\\]\n\\[\\dots \\text{etc.}\\]\n\n\nWe only concern ourselves with movements of people between the two regions. * no immigration, emigration, birth, death, etc.\n\n\nWe assume that measurements have shown the following pattern:\nin any given year,\n\n5% of the people in the city move to the suburbs, and\n3% of the people in the suburbs move to the city.\n\n\n\nYou can think of this as:\n\\[\\begin{array}{rcc}&\\text{From City}&\\text{From Suburbs}\\\\\\text{To City}& .95&.03\\\\\\text{To Suburbs}&.05&.97\\end{array}\\]\n\n\nThen we can capture this update rule as a matrix:\n\\[A = \\left[\\begin{array}{rr}.95&.03\\\\.05&.97\\end{array}\\right].\\]\n\n\nWe can see that this is correct by verifying that:\n\\[\\left[\\begin{array}{cc}\\text{city pop. in 2001}\\\\\\text{suburb pop. in 2001}\\end{array}\\right] =\\left[\\begin{array}{rr}.95&.03\\\\.05&.97\\end{array}\\right] \\left[\\begin{array}{cc}\\text{city pop. in 2000}\\\\\\text{suburb pop. in 2000}\\end{array}\\right].\\]",
    "crumbs": [
      "Markov Chains"
    ]
  },
  {
    "objectID": "L11MarkovChains.html#markov-chains",
    "href": "L11MarkovChains.html#markov-chains",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\n\nAndrei Markov, 1856 - 1922, St Petersburg.\n\n\n\nAAMarkov by unknown; originally uploaded to en-wikipedia by en:User:Mhym - From the official web site of the Russian Academy of Sciences: http://www.ras.ru/win/db/show_per.asp?P=.id-53175.ln-en.dl-.pr-inf.uk-0. Licensed under Public Domain via Wikimedia Commons.\nMarkov was part of the great tradition of mathematics in Russia.\nMarkov started out working in number theory but then got interested in probability. He enjoyed poetry and the great Russian poet Pushkin. Markov studied the sequence of letters found in the text of Eugene Onegin, in particular the sequence of consonants and vowels.\nHe sought a way to describe patterns in sequences, such as text like Eugene Onegin. This eventually led to the idea of a system in which one transitions between states, and the probability of going to another state depends only on the current state.\nHence, Markov pioneered the study of systems in which the future state of the system depends only on the present state in a random fashion. This has turned out to be a terrifically useful idea. For example, it is the starting point for analysis of the movement of stock prices, and the dynamics of animal populations.\nThese have since been termed “Markov Chains.”\nMarkov chains are essential tools in understanding, explaining, and predicting phenomena in computer science, physics, biology, economics, and finance.\nToday we will study an application of linear algebra. You will see how the concepts we use, such as vectors and matrices, get applied to a particular problem.\nMany applications in computing are concerned with how a system behaves over time.\nThink of a Web server that is processing requests for Web pages, or network that is moving packets from place to place.\nWe would like to describe how systems like these operate, and analyze them to understand their performance limits.\n\nA typical way to model a time-evolving system is:\n\ndefine some vector that describes the state of the system, and\nformulate a rule that specifies how to compute the next state of the system based on the current state of the system.\n\n\nSo we would say that the state of the system at time \\(k\\) is a vector \\({\\bf x_k} \\in \\mathbb{R}^n,\\) and\n\\[{\\bf x_{k+1}} = T({\\bf x_k}),\\;\\;\\;\\text{for time}\\;k=0,1,2...\\]\nwhere \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n.\\)\n\n\nThe vector \\({\\bf x_k}\\) is called the state vector.\n\n\nOf course, we are going to be particularly interested in the case where \\(T\\) is a linear transformation.\nThen we know that we can write the difference equation as:\n\\[ {\\bf x_{k+1}} = A{\\bf x_k},\\]\nwhere \\(A \\in \\mathbb{R}^{n\\times n}.\\)\nThis is a linear difference equation.\n\nExample.\nHere is our warm-up problem.\nWe are interested in the population of two regions, say the city and the suburbs.\nFix an initial year (say 2000) and let\n\\[ {\\bf x_0} = \\left[\\begin{array}{cc}\\text{population of the city in 2000}\\\\\\text{population of the suburbs in 2000}\\end{array}\\right].\\]\n\nThen\n\\[ {\\bf x_1} = \\left[\\begin{array}{cc}\\text{population of the city in 2001}\\\\\\text{population of the suburbs in 2001}\\end{array}\\right],\\]\n\\[{\\bf x_2} = \\left[\\begin{array}{cc}\\text{population of the city in 2002}\\\\\\text{population of the suburbs in 2002}\\end{array}\\right],\\]\n\\[\\dots \\text{etc.}\\]\n\n\nWe only concern ourselves with movements of people between the two regions. * no immigration, emigration, birth, death, etc.\n\n\nWe assume that measurements have shown the following pattern:\nin any given year,\n\n5% of the people in the city move to the suburbs, and\n3% of the people in the suburbs move to the city.\n\n\n\nYou can think of this as:\n\\[\\begin{array}{rcc}&\\text{From City}&\\text{From Suburbs}\\\\\\text{To City}& .95&.03\\\\\\text{To Suburbs}&.05&.97\\end{array}\\]\n\n\nThen we can capture this update rule as a matrix:\n\\[A = \\left[\\begin{array}{rr}.95&.03\\\\.05&.97\\end{array}\\right].\\]\n\n\nWe can see that this is correct by verifying that:\n\\[\\left[\\begin{array}{cc}\\text{city pop. in 2001}\\\\\\text{suburb pop. in 2001}\\end{array}\\right] =\\left[\\begin{array}{rr}.95&.03\\\\.05&.97\\end{array}\\right] \\left[\\begin{array}{cc}\\text{city pop. in 2000}\\\\\\text{suburb pop. in 2000}\\end{array}\\right].\\]",
    "crumbs": [
      "Markov Chains"
    ]
  },
  {
    "objectID": "L11MarkovChains.html#markov-chains-1",
    "href": "L11MarkovChains.html#markov-chains-1",
    "title": "Geometric Algorithms",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nLet’s look at \\(A\\) again:\n\\[A = \\left[\\begin{array}{rr}.95&.03\\\\.05&.97\\end{array}\\right].\\]\n\n\nWe note that \\(A\\) has a special property: each of its columns adds up to 1.\nAlso, it would not make sense to have negative entries in \\(A\\).\n\n\nThe reason that columns sum to 1 is that the total number of people in the system is not changing over time.\n\nThis leads to three definitions:\n\nDefinition. A probability vector is a vector of nonnegative entries that sums to 1.\n\n\nDefinition. A stochastic matrix is a square matrix of nonnegative values whose columns each sum to 1.\n\n\nDefinition. A Markov chain is a dynamical system whose state is a probability vector and which evolves according to a stochastic matrix.\nThat is, it is a probability vector \\({\\bf x_0} \\in \\mathbb{R}^n\\) and a stochastic matrix \\(A \\in \\mathbb{R}^{n\\times n}\\) such that\n\\[{\\bf x_{k+1}} = A{\\bf x_k}\\;\\;\\;\\text{for}\\;k = 0,1,2,...\\]\n\nSo we think of a probability vector \\({\\bf x_0}\\) as describing how things are “distributed” across various categories – the fraction of items that are in each category.\n\nAnd we think of the stochastic matrix \\(A\\) as describing how things “redistribute” themselves at each time step.\n\nExample. Suppose that in 2000 the population of the city is 600,000 and the population of the suburbs is 400,000. What is the distribution of the population in 2001? In 2002? In 2020?\n\nSolution. First, we convert the population counts to a probability vector, in which elements sum to 1.\nThis is done by simply normalizing by the sum of the vector elements.\n\n600,000 + 400,000 = 1,000,000.\n\n\n\n\\[\\frac{1}{1,000,000}\\left[\\begin{array}{rr}600,000\\\\400,000\\end{array}\\right] = \\left[\\begin{array}{rr}0.60\\\\0.40\\end{array}\\right].\\]\n\n\nThen the distribution of population in 2001 is:\n\\[ {\\bf x_{1}} = A{\\bf x_0} = \\left[\\begin{array}{rr}.95&.03\\\\.05&.97\\end{array}\\right]\\left[\\begin{array}{rr}0.60\\\\0.40\\end{array}\\right] = \\left[\\begin{array}{rr}0.582\\\\0.418\\end{array}\\right].\\]\n\n\nAnd the distribution of the population in 2002 is:\n\\[ {\\bf x_{2}} = A{\\bf x_1} = \\left[\\begin{array}{rr}.95&.03\\\\.05&.97\\end{array}\\right]\\left[\\begin{array}{rr}0.582\\\\0.418\\end{array}\\right] = \\left[\\begin{array}{rr}0.565\\\\0.435\\end{array}\\right].\\]\n\n\nNote that another way we could have written this is:\n\\[ {\\bf x_{2}} = A{\\bf x_1} = A(A{\\bf x_0}) = A^2 {\\bf x_0}.\\]\n\n\nTo answer the question for 2020, i.e., \\(k=20,\\) we note that\n\\[{\\bf x_{20}} = \\overbrace{A\\cdots A}^{20} {\\bf x_0} = A^{20}{\\bf x_0}.\\]\n\n# stochastic matrix A\nA = np.array(\n    [[0.95,0.03],\n     [0.05,0.97]])\n#\n# initial state vector x_0\nx_0 = np.array([0.60,0.40])\n#\n# compute A^20\nA_20 = np.linalg.matrix_power(A, 20)\n#\n# compute x_20\nx_20 = A_20 @ x_0\nprint(x_20)\n\n[0.417456 0.582544]\n\n\n\n\nSo we find that after 20 years, only 42% of the population will remain in the city.",
    "crumbs": [
      "Markov Chains"
    ]
  },
  {
    "objectID": "L11MarkovChains.html#predicting-the-distant-future",
    "href": "L11MarkovChains.html#predicting-the-distant-future",
    "title": "Geometric Algorithms",
    "section": "Predicting the Distant Future",
    "text": "Predicting the Distant Future\n\nWe noticed that the population of the city is going down. Will everyone eventually live in the suburbs?\n\n\nA important question about a Markov Chain is: what will happen in the distant future?\nFor example, what happens to the population distribution in our example “in the long run?”\n\n\nRather than answering that question right now, we’ll take a more interesting example.\n\nSuppose we have a system whose state transition is described by the stochastic matrix\n\\[P = \\left[\\begin{array}{rrr}.5&.2&.3\\\\.3&.8&.3\\\\.2&0&.4\\end{array}\\right]\\]\nand which starts in the state\n\\[{\\bf x_0} = \\left[\\begin{array}{r}1\\\\0\\\\0\\end{array}\\right].\\]\n\nConsider the Markov Chain defined by \\(P\\) and \\({\\bf x_0}\\), that is, the chain defined as\n\\[{\\bf x_{k+1}} = P{\\bf x_k}\\;\\;\\;\\text{for}\\;k=0,1,2...\\]\n\n\nWhat happens to the system as time passes?\nLet’s compute the state vectors \\({\\bf x_1},\\dots,{\\bf x_{15}}\\) to find out.\n\n\n\n# stochastic matrix A\nA = np.array(\n    [[.5, .2, .3],\n     [.3, .8, .3],\n     [.2,  0, .4]])\n#\n# initial state vector\nx = np.array([1,0,0])\n#\n# array to hold each future state vector\nxs = np.zeros((15,3))\n# \n# compute future state vectors\nfor i in range(15):\n    xs[i] = x\n    print(f'x({i}) = {x}')\n    x = A @ x\n\nx(0) = [1 0 0]\nx(1) = [0.5 0.3 0.2]\nx(2) = [0.37 0.45 0.18]\nx(3) = [0.329 0.525 0.146]\nx(4) = [0.3133 0.5625 0.1242]\nx(5) = [0.30641 0.58125 0.11234]\nx(6) = [0.303157 0.590625 0.106218]\nx(7) = [0.3015689 0.5953125 0.1031186]\nx(8) = [0.30078253 0.59765625 0.10156122]\nx(9) = [0.30039088 0.59882813 0.10078099]\nx(10) = [0.30019536 0.59941406 0.10039057]\nx(11) = [0.30009767 0.59970703 0.1001953 ]\nx(12) = [0.30004883 0.59985352 0.10009765]\nx(13) = [0.30002441 0.59992676 0.10004883]\nx(14) = [0.30001221 0.59996338 0.10002441]\n\n\n\n\nWhat is going on here? Let’s look at these values graphically.\n\n\n\n\n\n\n\n\n\n\n\nBased on visual inspection, these vectors seem to be approaching\n\\[{\\bf q} = \\left[\\begin{array}{r}.3\\\\.6\\\\.1\\end{array}\\right].\\]\nThe components of \\({\\bf x_k}\\) don’t seem to be changing much past about \\(k = 10.\\)\n\n\nIn fact, we can confirm that the this system would be stable at \\(\\left[\\begin{array}{r}.3\\\\.6\\\\.1\\end{array}\\right]\\) by noting that:\n\\[\\left[\\begin{array}{rrr}.5&.2&.3\\\\.3&.8&.3\\\\.2&0&.4\\end{array}\\right]\\left[\\begin{array}{r}.3\\\\.6\\\\.1\\end{array}\\right] = \\left[\\begin{array}{r}.15+.12+.03\\\\.09+.48+.03\\\\.06+0+.04\\end{array}\\right] = \\left[\\begin{array}{r}.3\\\\.6\\\\.1\\end{array}\\right].\\]\n\n\nThis calculation is exact. So it seems that:\n\nthe sequence of vectors is approaching \\(\\left[\\begin{array}{r}.3\\\\.6\\\\.1\\end{array}\\right]\\) as a limit, and\nwhen and if they get to that point, they will stabilize there.",
    "crumbs": [
      "Markov Chains"
    ]
  },
  {
    "objectID": "L11MarkovChains.html#steady-state-vectors",
    "href": "L11MarkovChains.html#steady-state-vectors",
    "title": "Geometric Algorithms",
    "section": "Steady-State Vectors",
    "text": "Steady-State Vectors\n\nThis convergence to a “steady state” is quite remarkable. Is this a general phenomenon?\n\n\nDefinition. If \\(P\\) is a stochastic matrix, then a steady-state vector (or equilibrium vector) for \\(P\\) is a probability vector \\(\\bf q\\) such that:\n\\[P{\\bf q} = {\\bf q}.\\]\n\n\nIt can be shown that every stochastic matrix has at least one steady-state vector.\n(We’ll study this more closely in a later lecture.)\n\nExample.\n\n\\(\\left[\\begin{array}{r}.3\\\\.6\\\\.1\\end{array}\\right]\\) is a steady-state vector for \\(\\left[\\begin{array}{rrr}.5&.2&.3\\\\.3&.8&.3\\\\.2&0&.4\\end{array}\\right]\\)\n\nbecause\n\\[ \\left[\\begin{array}{rrr}.5&.2&.3\\\\.3&.8&.3\\\\.2&0&.4\\end{array}\\right]\\left[\\begin{array}{r}.3\\\\.6\\\\.1\\end{array}\\right] = \\left[\\begin{array}{r}.3\\\\.6\\\\.1\\end{array}\\right] \\]\nExample.\nRecalling the population-movement example above.\n\nThe probability vector \\({\\bf q} = \\left[\\begin{array}{r}.375\\\\.625\\end{array}\\right]\\) is a steady-state vector for the population migration matrix \\(A\\), because\n\\[A{\\bf q} = \\left[\\begin{array}{rr}.95&.03\\\\.05&.97\\end{array}\\right]\\left[\\begin{array}{r}.375\\\\.625\\end{array}\\right] = \\left[\\begin{array}{r}.35625+.01875\\\\.01875+.60625\\end{array}\\right] = \\left[\\begin{array}{r}.375\\\\.625\\end{array}\\right] = {\\bf q}.\\]\n\n\nTo interpret this:\n\nif the total population of the region is 1 million,\nthen if there are 375,000 persons in the city and 625,000 persons in the suburbs,\nthe populations of both the city and the suburbs would stabilize – they would stay the same in all future years.",
    "crumbs": [
      "Markov Chains"
    ]
  },
  {
    "objectID": "L11MarkovChains.html#finding-the-steady-state",
    "href": "L11MarkovChains.html#finding-the-steady-state",
    "title": "Geometric Algorithms",
    "section": "Finding the Steady State",
    "text": "Finding the Steady State\nOK, so it seems that the two Markov Chains we have studied so far each have a steady state. This leads to two questions:\n\n\nCan we compute the steady state?\n\nSo far we have guessed what the steady state is, and then checked. Can we compute the steady state directly?\n\n\n\n\n\nHow do we know if:\n\na Markov Chain has a unique steady state, and\nwhether it will always converge to that steady state?\n\n\n\nLet’s start by thinking about how to compute the steady-state directly.\n\nExample. Let \\(P = \\left[\\begin{array}{rr}.6&.3\\\\.4&.7\\end{array}\\right].\\) Find a steady-state vector for \\(P\\).\n\n\nSolution. Let’s simply solve the equation \\(P{\\bf x} = {\\bf x}.\\)\n\n\n\\[P{\\bf x} = {\\bf x}\\]\n\n\n\\[P{\\bf x} -{\\bf x} = {\\bf 0}\\]\n\n\n\\[P{\\bf x} -I{\\bf x} = {\\bf 0}\\]\n\n\n\\[(P-I){\\bf x} = {\\bf 0}\\]\n\n\nNow, \\(P-I\\) is a matrix, so this is a linear system that we can solve.\n\n\n\\[P-I = \\left[\\begin{array}{rr}.6&.3\\\\.4&.7\\end{array}\\right] - \\left[\\begin{array}{rr}1&0\\\\0&1\\end{array}\\right] = \\left[\\begin{array}{rr}-.4&.3\\\\.4&-.3\\end{array}\\right].\\]\n\n\nTo find all solutions of \\((P-I){\\bf x} = {\\bf 0},\\) we row reduce the augmented matrix:\n\\[\\left[\\begin{array}{rrr}-.4&.3&0\\\\.4&-.3&0\\end{array}\\right] \\sim \\left[\\begin{array}{rrr}-.4&.3&0\\\\0&0&0\\end{array}\\right] \\sim \\left[\\begin{array}{rrr}1&-3/4&0\\\\0&0&0\\end{array}\\right].\\]\n\n\nSo \\(x_1 = \\frac{3}{4}x_2\\) and \\(x_2\\) is free. The general solution is \\(\\left[\\begin{array}{c}\\frac{3}{4}x_2\\\\x_2\\end{array}\\right].\\)\n\n\nThis means that there are an infinite set of solutions. Which one are we interested in?\n\nRemember that our vectors \\({\\bf x}\\) are probability vectors. So we are interested in the solution in which the vector elements are nonnegative and sum to 1.\n\nThe simple way to find this is to take any solution, and divide it by the sum of the entries (so that the sum then adds to 1.)\nLet’s choose \\(x_2 = 1\\), so the specific solution is:\n\\[\\left[\\begin{array}{r}\\frac{3}{4}\\\\1\\end{array}\\right].\\]\n\n\nNormalizing this by the sum of the entries (\\(\\frac{7}{4}\\)) we get:\n\\[{\\bf q} = \\left[\\begin{array}{r}\\frac{3}{7}\\\\\\frac{4}{7}\\end{array}\\right].\\]\n\nSo, we have found how to solve a Markov Chain for its steady state:\n\nSolve the linear system \\((P-I){\\bf x} = {\\bf 0}.\\)\n\nThe system will have an infinite number of solutions, with one free variable. Obtain a general solution.\nPick any specific solution (choose any value for the free variable), and normalize it so the entries add up to 1.",
    "crumbs": [
      "Markov Chains"
    ]
  },
  {
    "objectID": "L11MarkovChains.html#existence-of-and-convergence-to-steady-state",
    "href": "L11MarkovChains.html#existence-of-and-convergence-to-steady-state",
    "title": "Geometric Algorithms",
    "section": "Existence of, and Convergence to, Steady State",
    "text": "Existence of, and Convergence to, Steady State\nFinally: when does a system have a unique solution that is a probability vector, and how do we know it will converge to that vector?\n\nOf course, a linear system in general might have no solutions, or it might have a unique solution that is not a probability vector.\nSo what we are asking is, when does a system defined by a Markov Chain have an infinite set of solutions, so that we can find one of them that is a probability vector?\n\n\nDefinition. We say that a stochastic matrix \\(P\\) is regular if some matrix power \\(P^k\\) contains only strictly positive entries.\n\n\nFor\n\\[P =\\left[\\begin{array}{rrr}.5&.2&.3\\\\.3&.8&.3\\\\.2&0&.4\\end{array}\\right],\\]\nWe note that \\(P\\) does not have every entry strictly positive.\n\n\nHowever:\n\\[P^2 = \\left[\\begin{array}{rrr}.37&.26&.33\\\\.45&.70&.45\\\\.18&.04&.22\\end{array}\\right].\\]\nSince every entry in \\(P^2\\) is positive, \\(P\\) is a regular stochastic matrix.\n\nTheorem. If \\(P\\) is an \\(n\\times n\\) regular stochastic matrix, then \\(P\\) has a unique steady-state vector \\({\\bf q}.\\) Further, if \\({\\bf x_0}\\) is any initial state and \\({\\bf x_{k+1}} = P{\\bf x_k}\\) for \\(k = 0, 1, 2,\\dots,\\) then the Markov Chain \\(\\{{\\bf x_k}\\}\\) converges to \\({\\bf q}\\) as \\(k \\rightarrow \\infty.\\)\n\nNote the phrase “any initial state.”\nThis is a remarkable property of a Markov Chain: it converges to its steady-state vector no matter what state the chain starts in.\nWe say that the long-term behavior of the chain has “no memory of the starting state.”\n\nExample. Consider a computer system that consists of a disk, a CPU, and a network interface.\n\nA set of jobs are loaded into the system. Each job makes requests for service from each of the components of the system.\nAfter receiving service, the job then next requests service from the same or a different component, and so on.\n\n\nJobs move between system components according to the following diagram.\nFor example, after receiving service from the Disk, 70% of jobs return to the disk for another unit of service; 20% request service from the network interface; and 10% request service from the CPU.\n\n\n\nAssume the system runs for a long time. Determine whether the system will stabilize, and if so, find the fraction of jobs that are using each device once stabilized. Which device is busiest? Least busy?\n\n\nSolution. From the diagram, the movement of jobs among components is given by:\n\\[ \\begin{array}{rccc}&\\text{From Disk}&\\text{From Net}&\\text{From CPU}\\\\\\text{To Disk}&0.70&0.10&0.30\\\\\\text{To Net}&0.20&0.80&0.30\\\\\\text{To CPU}&0.10&0.10&0.40\\end{array}\\]\n\n\nThis corresponds to the stochastic matrix:\n\\[P = \\left[\\begin{array}{rrr}0.70&0.10&0.30\\\\0.20&0.80&0.30\\\\0.10&0.10&0.40\\end{array}\\right].\\]\n\n\nFirst of all, this is a regular matrix because \\(P\\) has all strictly positive entries. So it has a unique steady-state vector.\n\n\nNext, we find the steady state of the system by solving \\((P-I){\\bf x} = {\\bf 0}:\\)\n\\[[P-I\\; {\\bf 0}] = \\left[\\begin{array}{rrrr}-0.30&0.10&0.30&0\\\\0.20&-0.20&0.30&0\\\\0.10&0.10&-0.60&0\\end{array}\\right] \\sim \\left[\\begin{array}{rrrr}1&0&-9/4&0\\\\0&1&-15/4&0\\\\0&0&0&0\\end{array}\\right].\\]\n\n\nThus the general solution of \\((P-I){\\bf x} = {\\bf 0}\\) is\n\\[x_1 = \\frac{9}{4} x_3, \\;\\;\\; x_2 = \\frac{15}{4} x_3,\\;\\;\\; x_3\\;\\text{free.}\\]\nSince \\(x_3\\) is free (there are infinitely many solutions) we can find a solution whose entries sum to 1. This is:\n\\[{\\bf q} \\approx \\left[\\begin{array}{r}.32\\\\.54\\\\.14\\end{array}\\right].\\]\n\n\nSo we see that in the long run,\n\nabout 32% of the jobs will be using the disk,\nabout 54% will be using the network interface, and\nabout 14% will be using the CPU.\n\n\n\nAgain, the important fact here is that we did not have to concern ourselves with the state in which that the system started.\nThe influence of the starting state is eventually lost!\n\n\nLet’s demonstrate that computationally. Let’s say that at the start, all the jobs happened to be using the CPU. Then:\n\\[{\\bf x}_0 = \\left[\\begin{array}{r}0\\\\0\\\\1\\end{array}\\right].\\]\n\n\nThen let’s look at how the three elements of the \\({\\bf x}\\) vector evolve with time, by computing \\(P{\\bf x}_0\\), \\(P^2{\\bf x}_0\\), \\(P^3{\\bf x}_0\\), etc.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice how the system starts in the state \\(\\left[\\begin{array}{r}0\\\\0\\\\1\\end{array}\\right]\\),\nbut quickly (within about 10 steps) reaches the equilibrium state that we predicted:\n\\[\\left[\\begin{array}{r}.32\\\\.54\\\\.14\\end{array}\\right].\\]\n\n\nNow let’s compare what happens if the system starts in a different state, say \\(\\left[\\begin{array}{r}1\\\\0\\\\0\\end{array}\\right]\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis shows graphically that even though the system started in a very different state, it quickly converges to the steady state regardless of the starting state.\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nAlthough we have been showing each component converging separately, in fact the entire state vector of the system can be thought of as evolving in space.\nThis figure shows the movement of the state vector starting from six different initial conditions, and shows how regardless of the starting state, the eventual state is the same.",
    "crumbs": [
      "Markov Chains"
    ]
  },
  {
    "objectID": "L11MarkovChains.html#summary",
    "href": "L11MarkovChains.html#summary",
    "title": "Geometric Algorithms",
    "section": "Summary",
    "text": "Summary\n\nMany phenomena can be describe using Markov’s idea:\n\nThere are “states”, and\nTransition between states only depends on the current state.\n\nExamples: population movement, jobs in a computer, consonants/vowels in a text…\nSuch a system can be expressed in terms of a stochastic matrix and a probability vector.\nEvolution of the system in time is described by matrix multiplication.\nUsing linear algebraic tools we can predict the steady state of such a system!",
    "crumbs": [
      "Markov Chains"
    ]
  },
  {
    "objectID": "L15DimensionRank.html",
    "href": "L15DimensionRank.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\nImage credit: “Frans Hals - Portret van René Descartes” by After Frans Hals (1582/1583–1666) - André Hatala [e.a.] (1997) De eeuw van Rembrandt, Bruxelles: Crédit communal de Belgique, ISBN 2-908388-32-4.. Licensed under Public Domain via Wikimedia Commons.\n\n\nRene Descartes (1596-1650) was a French philosopher, mathematician, and writer. He is often credited with developing the idea of a coordinate system, although versions of coordinate systems had been seen in Greek mathematics since 300BC.\nAs a young man, Descartes had health problems and generally stayed in bed late each day. The story goes that one day as he lay in bed, he observed a fly on the ceiling of his room. He thought about how to describe the movement of the fly, and realized that he could completely describe it by measuring its distance from the walls of the room. This gave birth to the so-called Cartesian coordinate system.\nWhat is certain is that Descartes championed the idea that geometric problems could be cast into algebraic form and solved in that fashion.\nThis was an important shift in thinking; the mathematical tradition begun by the Greeks held that geometry, as practiced by Euclid with compass and straightedge, was a more fundamental approach. For example, the problem of constructing a regular hexagon was one that the Greeks had studied and solved using non-numeric methods.\n\n\nImage credit: by Aldoaldoz - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=10023563\n\n\n\n\nDescartes would have argued that a hexagon could be constructed exactly by simply computing the coordinates of its vertices.\nThe study of curves and shapes in algebraic form laid important groundwork for calculus, and Newton was strongly influenced by Descartes’ ideas.\n\nWhy is a coordinate system so useful?\nThe value of a coordinate system is that it gives a unique name to each point in the plane (or in any vector space).\nNow, here is a question: what if the walls of Descartes’ room had not been square?\n… in other words, the corners were not perpendicular?\nWould his system still have worked? Would he still have been able to precisely specify the path of the fly?\nWe’ll explore this question today and use it to further deepen our understanding of linear operators.",
    "crumbs": [
      "Dimension and Rank"
    ]
  },
  {
    "objectID": "L15DimensionRank.html#dimension-and-rank",
    "href": "L15DimensionRank.html#dimension-and-rank",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\nImage credit: “Frans Hals - Portret van René Descartes” by After Frans Hals (1582/1583–1666) - André Hatala [e.a.] (1997) De eeuw van Rembrandt, Bruxelles: Crédit communal de Belgique, ISBN 2-908388-32-4.. Licensed under Public Domain via Wikimedia Commons.\n\n\nRene Descartes (1596-1650) was a French philosopher, mathematician, and writer. He is often credited with developing the idea of a coordinate system, although versions of coordinate systems had been seen in Greek mathematics since 300BC.\nAs a young man, Descartes had health problems and generally stayed in bed late each day. The story goes that one day as he lay in bed, he observed a fly on the ceiling of his room. He thought about how to describe the movement of the fly, and realized that he could completely describe it by measuring its distance from the walls of the room. This gave birth to the so-called Cartesian coordinate system.\nWhat is certain is that Descartes championed the idea that geometric problems could be cast into algebraic form and solved in that fashion.\nThis was an important shift in thinking; the mathematical tradition begun by the Greeks held that geometry, as practiced by Euclid with compass and straightedge, was a more fundamental approach. For example, the problem of constructing a regular hexagon was one that the Greeks had studied and solved using non-numeric methods.\n\n\nImage credit: by Aldoaldoz - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=10023563\n\n\n\n\nDescartes would have argued that a hexagon could be constructed exactly by simply computing the coordinates of its vertices.\nThe study of curves and shapes in algebraic form laid important groundwork for calculus, and Newton was strongly influenced by Descartes’ ideas.\n\nWhy is a coordinate system so useful?\nThe value of a coordinate system is that it gives a unique name to each point in the plane (or in any vector space).\nNow, here is a question: what if the walls of Descartes’ room had not been square?\n… in other words, the corners were not perpendicular?\nWould his system still have worked? Would he still have been able to precisely specify the path of the fly?\nWe’ll explore this question today and use it to further deepen our understanding of linear operators.",
    "crumbs": [
      "Dimension and Rank"
    ]
  },
  {
    "objectID": "L15DimensionRank.html#coordinate-systems",
    "href": "L15DimensionRank.html#coordinate-systems",
    "title": "Geometric Algorithms",
    "section": "Coordinate Systems",
    "text": "Coordinate Systems\n\nIn the last lecture we developed the idea of a basis – a minimal spanning set for a subspace \\(H\\).\n\n\nToday we’ll emphasize this aspect: a key value of a basis is that\n\na basis provides a coordinate system for H.\n\n\n\nIn other words: if we are given a basis for \\(H\\), then each vector in \\(H\\) can be written in only one way as a linear combination of the basis vectors.\n\n\nLet’s see this convincingly.\nSuppose \\(\\mathcal{B}\\ = \\{\\mathbf{b}_1,\\dots,\\mathbf{b}_p\\}\\) is a basis for \\(H,\\) and suppose a vector \\(\\mathbf{x}\\) in \\(H\\) can be generated in two ways, say\n\\[\\mathbf{x} = c_1\\mathbf{b}_1+\\cdots+c_p\\mathbf{b}_p\\]\n\nand\n\n\\[\\mathbf{x} = d_1\\mathbf{b}_1+\\cdots+d_p\\mathbf{b}_p.\\]\n\n\nThen, subtracting gives\n\\[{\\bf 0} = \\mathbf{x} - \\mathbf{x} = (c_1-d_1)\\mathbf{b}_1+\\cdots+(c_p-d_p)\\mathbf{b}_p.\\]\n\n\nNow, since \\(\\mathcal{B}\\) is a basis, we know that the vectors \\(\\{\\mathbf{b}_1\\dots\\mathbf{b}_p\\}\\) are linearly independent.\n\n\nSo by the definition of linear independence, the weights in the above expression must all be zero.\nThat is, \\(c_j = d_j\\) for all \\(j\\).\n… which shows that the two representations must be the same.\n\nDefinition. Suppose the set \\(\\mathcal{B}\\ = \\{\\mathbf{b}_1,\\dots,\\mathbf{b}_p\\}\\) is a basis for the subspace \\(H\\).\nFor each \\(\\mathbf{x}\\) in \\(H\\), the coordinates of \\(\\mathbf{x}\\) relative to the basis \\(\\mathcal{B}\\) are the weights \\(c_1,\\dots,c_p\\) such that \\(\\mathbf{x} = c_1\\mathbf{b}_1+\\cdots+c_p\\mathbf{b}_p\\).\n\nThe vector in \\(\\mathbb{R}^p\\)\n\\[[\\mathbf{x}]_\\mathcal{B} = \\begin{bmatrix}c_1\\\\\\vdots\\\\c_p\\end{bmatrix}\\]\nis called the coordinate vector of \\(\\mathbf{x}\\) (relative to \\(\\mathcal{B}\\)) or the \\(\\mathcal{B}\\)-coordinate vector of \\(\\mathbf{x}\\).\n\nHere is an example in \\(\\mathbb{R}^2\\):\nLet’s look at the point \\(\\begin{bmatrix}1\\\\6\\end{bmatrix}\\).\n\n\n\n\n\n\nNow we’ll use a new basis:\n\\[ \\mathcal{B} = \\left\\{\\begin{bmatrix}1\\\\0\\end{bmatrix}, \\begin{bmatrix}1\\\\2\\end{bmatrix}\\right\\} \\]\n\n\nNotice that the location of \\(\\mathbf{x}\\) relative to the origin does not change.\nHowever, using the \\(\\mathcal{B}\\)-basis, it has different coordinates.\nThe new coordinates are \\([\\mathbf{x}]_\\mathcal{B} = \\begin{bmatrix}-2\\\\3\\end{bmatrix}\\).\n\n\n\nFinding the Coordinates of a Point in a Basis.\nOK. Now, let’s say we are given a particular basis. How do we find the coordinates of a point in that basis?\n\nLet’s consider a specific example.\nLet \\(\\mathbf{v}_1 = \\begin{bmatrix}3\\\\6\\\\2\\end{bmatrix}, \\mathbf{v}_2 = \\begin{bmatrix}-1\\\\0\\\\1\\end{bmatrix}, \\mathbf{x} = \\begin{bmatrix}3\\\\12\\\\7\\end{bmatrix},\\) and \\(\\mathcal{B} = \\{\\mathbf{v}_1,\\mathbf{v}_2\\}.\\)\nThen \\(\\mathcal{B}\\) is a basis for \\(H\\) = Span\\(\\{\\mathbf{v}_1,\\mathbf{v}_2\\}\\) because \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) are linearly independent.\n\n\nProblem: Determine if \\(\\mathbf{x}\\) is in \\(H\\), and if it is, find the coordinate vector of \\(\\mathbf{x}\\) relative to \\(\\mathcal{B}.\\)\n\n\nSolution. If \\(\\mathbf{x}\\) is in \\(H,\\) then the following vector equation is consistent:\n\\[c_1\\begin{bmatrix}3\\\\6\\\\2\\end{bmatrix} + c_2\\begin{bmatrix}-1\\\\0\\\\1\\end{bmatrix} = \\begin{bmatrix}3\\\\12\\\\7\\end{bmatrix}.\\]\n\n\nThe scalars \\(c_1\\) and \\(c_2,\\) if they exist, are the \\(\\mathcal{B}\\)-coordinates of \\(\\mathbf{x}.\\)\n\n\nRow operations show that\n\\[\\begin{bmatrix}3&-1&3\\\\6&0&12\\\\2&1&7\\end{bmatrix} \\sim \\begin{bmatrix}1&0&2\\\\0&1&3\\\\0&0&0\\end{bmatrix}.\\]\n\n\nThe reduced row echelon form shows that the system is consistent, so \\(\\mathbf{x}\\) is in \\(H\\).\nFurthermore, it shows that \\(c_1 = 2\\) and \\(c_2 = 3,\\)\nso \\([\\mathbf{x}]_\\mathcal{B} = \\begin{bmatrix}2\\\\3\\end{bmatrix}.\\)\n\n\nIn this example, the basis \\(\\mathcal{B}\\) determines a coordinate system on \\(H\\), which can be visualized like this:\n\n\n\n\n\n\n\n\n\n\n\n\nIsomorphism\nAnother important idea is that, although points in \\(H\\) are in \\(\\mathbb{R}^3\\), they are completely determined by their coordinate vectors, which belong to \\(\\mathbb{R}^2.\\)\nIn our example, \\(\\begin{bmatrix}3\\\\12\\\\7\\end{bmatrix}\\mapsto\\begin{bmatrix}2\\\\3\\end{bmatrix}\\).\n\nWe can see that the grid in the figure above makes \\(H\\) “look like” \\(\\mathbb{R}^2.\\)\n\n\n\nNote that a one-to-one correspondence is a function that is both one-to-one and onto – in other words, a bijection.\n\nThe correspondence \\(\\mathbf{x} \\mapsto [\\mathbf{x}]_\\mathcal{B}\\) is one-to-one correspondence between \\(H\\) and \\(\\mathbb{R}^2\\) that preserves linear combinations.\nIn other words, if \\(\\mathbf{a} = \\mathbf{b} + \\mathbf{c}\\) when \\(\\mathbf{a}, \\mathbf{b}, \\mathbf{c} \\in H\\),\nthen \\([\\mathbf{a}]_\\mathcal{B} = [\\mathbf{b}]_\\mathcal{B} + [\\mathbf{c}]_\\mathcal{B}\\) with \\([\\mathbf{a}]_\\mathcal{B}, [\\mathbf{b}]_\\mathcal{B}, [\\mathbf{c}]_\\mathcal{B} \\in \\mathbb{R}^2\\)\n\n\nWhen we have a one-to-one correspondence between two subspaces that preserves linear combinations, we call such a correspondence an isomorphism, and we say that \\(H\\) is isomorphic to \\(\\mathbb{R}^2.\\)\n\n\nIn general, if \\(\\mathcal{B}\\ = \\{\\mathbf{b}_1,\\dots,\\mathbf{b}_p\\}\\) is a basis for \\(H\\), then the mapping \\(\\mathbf{x} \\mapsto [\\mathbf{x}]_\\mathcal{B}\\) is a one-to-one correspondence that makes \\(H\\) look and act the same as \\(\\mathbb{R}^p.\\)\nThis is even through the vectors in \\(H\\) themselves may have more than \\(p\\) entries.",
    "crumbs": [
      "Dimension and Rank"
    ]
  },
  {
    "objectID": "L15DimensionRank.html#the-dimension-of-a-subspace",
    "href": "L15DimensionRank.html#the-dimension-of-a-subspace",
    "title": "Geometric Algorithms",
    "section": "The Dimension of a Subspace",
    "text": "The Dimension of a Subspace\n\n\nHere is an informal proof: Since \\(H\\) has a basis of \\(p\\) vectors, \\(H\\) is isomorphic to \\(\\mathbb{R}^p\\). Any set consisting of more than \\(p\\) vectors in \\(\\mathbb{R}^p\\) must be dependent. Recall that an isomorphism preserves vector relationships (sums, linear combinations, etc). So by the nature of an isomorphism, we can establish that any set of more than \\(p\\) vectors in \\(H\\) must be dependent. So any basis for \\(H\\) must have \\(p\\) vectors.\n\nIt can be shown that if a subspace \\(H\\) has a basis of \\(p\\) vectors, then every basis of \\(H\\) must consist of exactly \\(p\\) vectors.\n\n\nThat is, for a given \\(H\\), the number \\(p\\) is a special number.\n\n\nThus we can make this definition:\nDefinition. The dimension of a nonzero subspace \\(H,\\) denoted by \\(\\dim H,\\) is the number of vectors in any basis for \\(H.\\)\nThe dimension of the zero subspace \\(\\{{\\bf 0}\\}\\) is defined to be zero.\n\n\nSo now we can say with precision things we’ve previous said informally.\nFor example, a plane through \\({\\bf 0}\\) is two-dimensional, and a line through \\({\\bf 0}\\) is one-dimensional.\n\n\nQuestion: What is the dimension of a line not through the origin?\n\n\nAnswer: It is undefined, because a line not through the the origin is not a subspace, so cannot have a basis, so does not have a dimension.\n\n\nDimension of the Null Space\nAt the end of the last lecture we looked at this matrix:\n\\[A = \\begin{bmatrix}-3&6&-1&1&-7\\\\1&-2&2&3&-1\\\\2&-4&5&8&-4\\end{bmatrix}\\]\nWe determined that its null space had a basis consisting of 3 vectors.\nSo the dimension of \\(A\\)’s null space (ie, \\(\\dim\\operatorname{Nul} A\\)) is 3.\n\nRemember that we constructed an explicit description of the null space of this matrix, as:\n\\[ =\nx_2\\begin{bmatrix}2\\\\1\\\\0\\\\0\\\\0\\end{bmatrix}+x_4\\begin{bmatrix}1\\\\0\\\\-2\\\\1\\\\0\\end{bmatrix}+x_5\\begin{bmatrix}-3\\\\0\\\\2\\\\0\\\\1\\end{bmatrix} \\]\nEach basis vector corresponds to a free variable in the equation \\(A\\mathbf{x} = {\\bf 0}.\\)\nSo, to find the dimension of \\(\\operatorname{Nul}\\ A,\\) simply identify and count the number of free variables in \\(A\\mathbf{x} = {\\bf 0}.\\)\n\n\n\nMatrix Rank\nDefinition. The rank of a matrix, denoted by \\(\\operatorname{Rank} A,\\) is the dimension of the column space of \\(A\\).\n\nSince the pivot columns of \\(A\\) form a basis for \\(\\operatorname{Col} A,\\) the rank of \\(A\\) is just the number of pivot columns in \\(A\\).\n\n\nExample. Determine the rank of the matrix\n\\[A = \\begin{bmatrix}2&5&-3&-4&8\\\\4&7&-4&-3&9\\\\6&9&-5&2&4\\\\0&-9&6&5&-6\\end{bmatrix}.\\]\n\n\nSolution Reduce \\(A\\) to an echelon form:\n\\[A = \\begin{bmatrix}2&5&-3&-4&8\\\\0&-3&2&5&-7\\\\0&-6&4&14&-20\\\\0&-9&6&5&-6\\end{bmatrix}\\sim\\cdots\\sim\\begin{bmatrix}2&5&-3&-4&8\\\\0&-3&2&5&-7\\\\0&0&0&4&-6\\\\0&0&0&0&0\\end{bmatrix}.\\]\nThe matrix \\(A\\) has 3 pivot columns, so \\(\\operatorname{Rank} A = 3.\\)",
    "crumbs": [
      "Dimension and Rank"
    ]
  },
  {
    "objectID": "L15DimensionRank.html#the-rank-theorem",
    "href": "L15DimensionRank.html#the-rank-theorem",
    "title": "Geometric Algorithms",
    "section": "The Rank Theorem",
    "text": "The Rank Theorem\n\nConsider a matrix \\(A.\\)\nIn the last lecture we showed the following: one can construct a basis for \\(\\operatorname{Nul} A\\) using the columns corresponding to free variables in the solution of \\(A\\mathbf{x} = {\\bf 0}.\\)\n\n\nThis shows that \\(\\dim\\operatorname{Nul} A\\) = the number of free variables in \\(A\\mathbf{x} = {\\bf 0},\\)\nwhich is the number of non-pivot columns in \\(A\\).\n\n\nWe also saw that the number of columns in any basis for \\(\\operatorname{Col}\\ A\\) is the number of pivot columns.\n\n\nSo we can now make this important connection:\n\\[\n\\begin{array}{rcl}\n\\dim\\operatorname{Nul} A + \\dim\\operatorname{Col} A &= &\\text{number of non-pivot columns of $A$}\\\\ && + \\;\\;\\text{number of pivot columns of $A$}\\\\\n&= &\\text{number of columns of $A$}.\n\\end{array}\n\\]\n\nThese considerations lead to the following theorem:\nTheorem. If a matrix \\(A\\) has \\(n\\) columns, then \\(\\operatorname{Rank} A + \\dim\\operatorname{Nul} A = n\\).\n\nThis is a terrifically important fact!\nHere is an intuitive way to understand it. Let’s think about a matrix \\(A\\) and the associated linear transformation \\(T(x) = Ax\\).\n\n\nIf the matrix \\(A\\) has \\(n\\) columns, then \\(A\\)’s column space could have dimension as high as \\(n\\).\nIn other words, \\(T\\)’s range could have dimension as high as \\(n\\).\n\n\nHowever, if \\(A\\) “throws away” a nullspace of dimension \\(p\\), then that reduces the columnspace of \\(A\\) to \\(n-p\\).\nMeaning, the dimension of \\(T\\)’s range is reduced to \\(n-p\\).\n\n\nExtending the Invertible Matrix Theorem\nThe above arguments show that when \\(A\\) has \\(n\\) columns, then the “larger” that the column space is, the “smaller” that the null space is.\n(Where “larger” means “has more dimensions.”)\n\nThis is particularly important when \\(A\\) is square \\((n\\times n)\\).\nLet’s consider the extreme, in which the column space of \\(A\\) has maximum dimension – i.e., \\(\\dim\\operatorname{Col}\\ A= n.\\)\n\n\nRecall that the IMT said that an \\(n\\times n\\) matrix is invertible if and only if its columns are linearly independent, and if and only if its columns span \\(\\mathbb{R}^n.\\)\n\n\nHence we now can see that an \\(n\\times n\\) matrix is invertible if and only if the columns of \\(A\\) form a basis for \\(\\mathbb{R}^n.\\)\n\n\nThis leads to the following facts, which further extend the IMT:\n\n\nLet \\(A\\) be an \\(n\\times n\\) matrix. Then the following statements are each equivalent to the statement that \\(A\\) is an invertible matrix:\n\nThe columns of \\(A\\) form a basis for \\(\\mathbb{R}^n.\\)\n\\(\\operatorname{Col} A = \\mathbb{R}^n.\\)\n\\(\\dim\\operatorname{Col} A = n.\\)\n\\(\\operatorname{Rank} A = n.\\)\n\\(\\operatorname{Nul} A = \\{{\\bf 0}\\}.\\)\n\\(\\dim\\operatorname{Nul} A = 0.\\)",
    "crumbs": [
      "Dimension and Rank"
    ]
  },
  {
    "objectID": "L09MatrixOperations.html",
    "href": "L09MatrixOperations.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nToday we will talk about multiplying matrices:\n\nHow do you multiply matrices?\nWhat does the product of two matrices mean?\nWhat algebraic rules apply to matrix multiplication?\nWhat is the computational cost of matrix multiplication?\n\n\nEarly in his life, Arthur Cayley practiced law to support his passion for mathematics. During his time as a practicing lawyer, he published over 200 papers on mathematics. Finally at the age of 42 he got a position at Cambridge University and took a big pay cut so he could become a full-time mathematician. He said he never regretted the choice. Cayley often said, “I love my subject.”\nIn 1855-1857 Cayley formed the theory of matrices, and came up with a way to multiply matrices. As we’ll see, it is not the most obvious thing to do, but it was quickly realized that it was the “right” way. Cayley came about this idea by first thinking about linear transformations, and how to compose linear transformations. So that’s where we’ll start.",
    "crumbs": [
      "Matrix Algebra"
    ]
  },
  {
    "objectID": "L09MatrixOperations.html#matrix-algebra",
    "href": "L09MatrixOperations.html#matrix-algebra",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nToday we will talk about multiplying matrices:\n\nHow do you multiply matrices?\nWhat does the product of two matrices mean?\nWhat algebraic rules apply to matrix multiplication?\nWhat is the computational cost of matrix multiplication?\n\n\nEarly in his life, Arthur Cayley practiced law to support his passion for mathematics. During his time as a practicing lawyer, he published over 200 papers on mathematics. Finally at the age of 42 he got a position at Cambridge University and took a big pay cut so he could become a full-time mathematician. He said he never regretted the choice. Cayley often said, “I love my subject.”\nIn 1855-1857 Cayley formed the theory of matrices, and came up with a way to multiply matrices. As we’ll see, it is not the most obvious thing to do, but it was quickly realized that it was the “right” way. Cayley came about this idea by first thinking about linear transformations, and how to compose linear transformations. So that’s where we’ll start.",
    "crumbs": [
      "Matrix Algebra"
    ]
  },
  {
    "objectID": "L09MatrixOperations.html#composing-linear-transformations",
    "href": "L09MatrixOperations.html#composing-linear-transformations",
    "title": "Geometric Algorithms",
    "section": "Composing Linear Transformations",
    "text": "Composing Linear Transformations\nLet’s start our discussion by recalling the linear transformation we called reflection through the origin.\n\nHere is a picture of what this transformation does to a shape:\n\n\n\n\n\n\n\n\n\n\n\nWe determined that the matrix \\(C = \\left[\\begin{array}{rr}-1&0\\\\0&-1\\end{array}\\right]\\) implements this linear transformation.\n\n\nBut notice that we could have accomplished this another way:\n\nFirst reflect through the \\(x_1\\) axis\nThen reflect through the \\(x_2\\) axis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we saw, to reflect a point through the \\(x_1\\) axis, we multiply it by matrix \\(A = \\left[\\begin{array}{rr}1&0\\\\0&-1\\end{array}\\right]\\).\nLikewise, to reflect a point through the \\(x_2\\) axis, we multiply it by matrix \\(B = \\left[\\begin{array}{rr}-1&0\\\\0&1\\end{array}\\right]\\).\n\n\nSo, another way to reflect point \\({\\bf u}\\) through the origin would be:\n\n\\({\\bf v} = A{\\bf u}\\)\nFollowed by \\({\\bf w} = B{\\bf v}.\\)\n\n\n\nIn other words, \\({\\bf w} = B(A{\\bf u}).\\)\n\n\nWhat we are doing here is called composing transformations.\nIn a composition of transformations, we take the output of one transformation as input for another transformation.\n\n\nNow, here is the key point:\nIt is clear that \\(B(A{\\bf x})\\) and \\(C{\\bf x}\\) are the same linear transformation.\nSo, using \\(C\\) we can go directly to the solution using one multiplication, rather than having to multiply twice (once for \\(A\\) and once for \\(B\\)).\n\nSo a natural question is: given \\(A\\) and \\(B\\), could we find \\(C\\) directly?\nIn other words, for any \\(A\\) and \\(B\\), could we find \\(C\\) such that:\n\\[ A(B{\\bf x}) = C{\\bf x}? \\]\n\nLet’s determine how to find \\(C\\) given \\(A\\) and \\(B.\\)\n\n\nIf \\(A\\) is \\(m \\times n\\), \\(B\\) is \\(n \\times p\\), and \\({\\bf x} \\in \\mathbb{R}^p,\\) denote the columns of \\(B\\) by \\({\\bf b_1},\\dots,{\\bf b_p},\\) and the entries in \\({\\bf x}\\) by \\(x_1, \\dots, x_p.\\)\n\n\nThen:\n\\[ B{\\bf x} = x_1{\\bf b_1} + \\dots + x_p {\\bf b_p}. \\]\n\n\nand:\n\\[A(B{\\bf x}) = A(x_1{\\bf b_1} + \\dots + x_p {\\bf b_p})\\]\n\n\nSince matrix-vector multiplication is a linear transformation:\n\\[ = x_1A{\\bf b_1} + \\dots + x_pA{\\bf b_p}. \\]\n\n\nSo the vector \\(A(B{\\bf x})\\) is a linear combination of the vectors \\(A{\\bf b_1}, \\dots, A{\\bf b_p},\\) using the entries in \\({\\bf x}\\) as weights.\n\n\nA linear combination of vectors is the same as a matrix-vector multiplication. In matrix terms, this linear combination is written:\n\\[ A(B{\\bf x}) = [A{\\bf b_1} \\; \\dots \\; A{\\bf b_p}] {\\bf x}.\\]\n\n\nSo this matrix \\([A{\\bf b_1} \\; \\dots \\; A{\\bf b_p}]\\) is what we are looking for!",
    "crumbs": [
      "Matrix Algebra"
    ]
  },
  {
    "objectID": "L09MatrixOperations.html#matrix-multiplication",
    "href": "L09MatrixOperations.html#matrix-multiplication",
    "title": "Geometric Algorithms",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\n\nDefinition. If \\(A\\) is an \\(m \\times n\\) matrix and \\(B\\) is \\(n \\times p\\) matrix with columns \\({\\bf b_1},\\dots,{\\bf b_p},\\) then the product \\(AB\\) is defined as the \\(m \\times p\\) matrix whose columns are \\(A{\\bf b_1}, \\dots, A{\\bf b_p}.\\) That is,\n\\[ AB = A[{\\bf b_1} \\; \\dots \\; {\\bf b_p}] = [A{\\bf b_1} \\; \\dots \\; A{\\bf b_p}]. \\]\n\n\nThis definition means that for any \\(A\\) and \\(B\\) for which \\(AB\\) is defined, then if \\(C\\) = \\(AB\\),\n\\[ C{\\bf x} = A(B{\\bf x}). \\]\n\n\nThat is: multiplication of matrices  corresponds to composition of linear transformations. \n\n\nNote that when \\(C = AB\\), \\(C{\\bf x}\\) is a vector in the span of the columns of \\(A.\\)\n\nExample. Compute \\(AB\\) where \\(A = \\left[\\begin{array}{rr}2&3\\\\1&-5\\end{array}\\right]\\) and \\(B = \\left[\\begin{array}{rrr}4&3&6\\\\1&-2&3\\end{array}\\right].\\)\n\nSolution. Write \\(B = \\left[{\\bf b_1}\\;{\\bf b_2}\\;{\\bf b_3}\\right],\\) and compute:\n\\[ A{\\bf b_1} = \\left[\\begin{array}{rr}2&3\\\\1&-5\\end{array}\\right]\\left[\\begin{array}{r}4\\\\1\\end{array}\\right],\\;\\;\\;\nA{\\bf b_2} = \\left[\\begin{array}{rr}2&3\\\\1&-5\\end{array}\\right]\\left[\\begin{array}{r}3\\\\-2\\end{array}\\right],\\;\\;\\;\nA{\\bf b_3} = \\left[\\begin{array}{rr}2&3\\\\1&-5\\end{array}\\right]\\left[\\begin{array}{r}6\\\\3\\end{array}\\right],\\]\n\n\n\\[ = \\left[\\begin{array}{r}11\\\\-1\\end{array}\\right]\\;\\;\\;\\left[\\begin{array}{r}0\\\\13\\end{array}\\right]\\;\\;\\;\\left[\\begin{array}{r}21\\\\-9\\end{array}\\right].\\]\n\n\nSo:\n\\[ AB = \\left[A{\\bf b_1}\\;A{\\bf b_2}\\;A{\\bf b_3}\\right] = \\left[\\begin{array}{rrr}11&0&21\\\\-1&13&-9\\end{array}\\right].\\]\n\nExample. Verify that reflection through the \\(x_1\\) axis followed by reflection through the \\(x_2\\) axis is the same as reflection through the origin.\n\n\\[\\left[\\begin{array}{rr}-1&0\\\\0&1\\end{array}\\right]\\left[\\begin{array}{rr}1&0\\\\0&-1\\end{array}\\right] = \\left[\\begin{array}{rr}~&~\\\\~&~\\end{array}\\right].\\]\n\n\n\\[\\left[\\begin{array}{rr}-1&0\\\\0&1\\end{array}\\right]\\left[\\begin{array}{rr}1&0\\\\0&-1\\end{array}\\right] = \\left[\\begin{array}{rr}-1&0\\\\0&-1\\end{array}\\right].\\]\n\n\nNote that this is a valid proof because every linear transformation of vectors is defined by its standard matrix.\n\nExample. If \\(A\\) is a \\(3 \\times 5\\) matrix, and \\(B\\) is a \\(5 \\times 2\\) matrix, what are the sizes of \\(AB\\) and \\(BA\\), if they are defined?\n\n\\[\\begin{array}{cccc}A&B&=&AB\\\\\n3\\times 5&5 \\times 2&& 3 \\times 2\\\\\n\\left[\\begin{array}{rrrrr}*&*&*&*&*\\\\ *&*&*&*&*\\\\ *&*&*&*&*\\end{array}\\right] &\n\\left[\\begin{array}{rr}*&*\\\\ *&*\\\\ *&*\\\\ *&*\\\\ *&*\\end{array}\\right] &\n= &\n\\left[\\begin{array}{rr}*&*\\\\ *&*\\\\ *&*\\end{array}\\right]\n\\end{array}\\]\n\n\nWhat about \\(BA\\)?\n\n\nIt is not defined,  because the number of columns of \\(B\\) does not match the number of rows of \\(A\\).\n\n\nFacts.\nIf \\(A\\) is \\(m\\times n\\), and \\(B\\) is \\(p \\times q\\), then \\(AB\\) is defined if and only if \\(n = p\\). If \\(AB\\) is defined, then it is \\(m \\times q\\).\n\\[\\begin{array}{cccc}A&B&=&AB\\\\\n3\\times \\fbox{5}&\\fbox{5} \\times 2&& 3 \\times 2\\\\\n\\end{array}\\]",
    "crumbs": [
      "Matrix Algebra"
    ]
  },
  {
    "objectID": "L09MatrixOperations.html#the-inner-product-view-of-matrix-multiplication",
    "href": "L09MatrixOperations.html#the-inner-product-view-of-matrix-multiplication",
    "title": "Geometric Algorithms",
    "section": "The Inner Product View of Matrix Multiplication",
    "text": "The Inner Product View of Matrix Multiplication\nRecall that the inner product of two vectors \\({\\bf u}\\) and \\({\\bf v}\\) is \\(\\sum_k u_k v_k.\\)\nAlso recall that one way to define the matrix vector product is \\((A{\\bf x})_i =\\) inner product of \\({\\bf x}\\) and row \\(i\\) of \\(A\\).\n\nThis immediately shows another way to think of matrix multiplication:\n\\((AB)_{ij} =\\) inner product of row \\(i\\) of \\(A\\) and column \\(j\\) of \\(B\\)\n\\((AB)_{ij} = \\sum_k A_{ik}B_{kj}.\\)\n\n\nExample. Start with the same matrices as the last example, \\(A = \\left[\\begin{array}{rr}2&3\\\\1&-5\\end{array}\\right]\\) and \\(B = \\left[\\begin{array}{rrr}4&3&6\\\\1&-2&3\\end{array}\\right].\\) Compute the entry in row 1 and column 3 of \\(C\\).\n\n\n\\[AB = \\left[\\begin{array}{rr}\\fbox{2} & \\fbox{3}\\\\1&-5\\end{array}\\right]\n\\left[\\begin{array}{rrr}4&3&\\fbox{6}\\\\1&-2&\\fbox{3}\\end{array}\\right] =\n\\left[\\begin{array}{rrc}*&*&2(6)+3(3)\\\\ *&*&*\\end{array}\\right] =\n\\left[\\begin{array}{rrr}*&*&21\\\\ *&*&*\\end{array}\\right].\\]\nThis agrees with the result of the last example, and we could reproduce the whole solution by repeating this for each element of the result matrix.",
    "crumbs": [
      "Matrix Algebra"
    ]
  },
  {
    "objectID": "L09MatrixOperations.html#matrix-algebra-1",
    "href": "L09MatrixOperations.html#matrix-algebra-1",
    "title": "Geometric Algorithms",
    "section": "Matrix Algebra",
    "text": "Matrix Algebra\nWe’ve defined multiplication of two matrices. What about addition of two matrices?\n\nThis is straightfoward: if \\(A\\) and \\(B\\) are the same shape, we get \\(A + B\\) by adding the corresponding elements. (Just like adding vectors.)\nThat is,\n\\[(A + B)_{ij} = A_{ij} + B_{ij}.\\]\nIf \\(A\\) and \\(B\\) are not the same shape, \\(A + B\\) is undefined.\n\n\nFurthermore, we define scalar-matrix multiplication just as for vectors:\n\\[ (rA)_{ij} = r(A_{ij}).\\]\n\n\nSo, just as we did for vectors, we can show that the standard properties of addition apply, and that scalar multiplication distributes over addition:\n\n\\(A +  B = B + A\\)\n\\((A + B) + C = A + (B + C)\\)\n\\(A + 0 = A\\)\n\\(r(A + B) = rA + rB\\)\n\\((r + s)A = rA + sA\\)\n\\(r(sA) = (rs)A\\)\n\n\n\nFurthermore, we find that some  (but not all!) of the familiar properties of multiplication apply to matrix multiplication (assume that all sums and products are defined):\n\n\\(A(BC) = (AB)C\\)\n\nmultiplication of matrices is associative\n\n\\(A(B+C) = AB + AC\\)\n\nmultiplication on the left distributes over addition\n\n\\((B+C)A = BA + CA\\)\n\nmultiplication on the right distributes over addition\n\n\\(r(AB) = (rA)B = A(rB)\\)\n\nfor any scalar \\(r\\)\n\n\\(I A = A = AI\\)\n\n\n\nNote that property 1 means that we can write \\(ABC\\) without bothering about parentheses.\n\nNow, here is where things get different!\n\n\nIn general, \\(AB\\) is not equal to \\(BA\\). Multiplication is not commutative!\n\nConsider \\(A = \\left[\\begin{array}{rr}1 & 1\\\\1&1\\end{array}\\right]\\) and \\(B = \\left[\\begin{array}{rr}1 & 1\\\\1&2\\end{array}\\right].\\)\n\n\n\n\n\nIn fact, even if \\(AB\\) is defined, \\(BA\\) may not be defined.\n\n\n\n\n\nOn the other hand, sometimes \\(A\\) and \\(B\\) do commute.\n\nConsider \\(A\\) and \\(B\\) as the reflections through the \\(x_1\\) and \\(x_2\\) axis. Then \\(AB\\) and \\(BA\\) both implement reflection through the origin (i.e., the same transformation.) So in this case \\(AB = BA\\).\n\n\n\n\n\nYou cannot, in general, cancel out matrices in a multiplication. That is, if \\(AC = AB\\), it does not follow that \\(C = B\\). \n\nConsider the case where \\(A\\) is the projection onto one of the axes.\n\n\n\n\n\nIf \\(AB\\) is the zero matrix, you cannot in general conclude that either \\(A\\) or \\(B\\) must be a zero matrix.\n\nConsider \\(A = \\left[\\begin{array}{rr}1 & 0\\\\0&0\\end{array}\\right]\\) and \\(B = \\left[\\begin{array}{rr}0 & 0\\\\0&1\\end{array}\\right].\\)\n\n\n\n\nStudy and remember these rules. You will use them!",
    "crumbs": [
      "Matrix Algebra"
    ]
  },
  {
    "objectID": "L09MatrixOperations.html#powers-of-a-matrix",
    "href": "L09MatrixOperations.html#powers-of-a-matrix",
    "title": "Geometric Algorithms",
    "section": "Powers of a Matrix",
    "text": "Powers of a Matrix\nEquipped now with matrix-matrix multiplication, we can define the powers of a matrix in a straightforward way. For an integer \\(k &gt; 0\\):\n\\[ A^k = \\overbrace{A\\cdots A}^k.\\]\nObviously, \\(A\\) must be a square matrix for \\(A^k\\) to be defined.\n\nWhat should \\(A^0\\) be?\n\\(A^0{\\bf x}\\) should be the result of multiplying \\({\\bf x}\\) with \\(A\\) zero times. So we define \\(A^0 = I\\).",
    "crumbs": [
      "Matrix Algebra"
    ]
  },
  {
    "objectID": "L09MatrixOperations.html#the-transpose-of-a-matrix",
    "href": "L09MatrixOperations.html#the-transpose-of-a-matrix",
    "title": "Geometric Algorithms",
    "section": "The Transpose of a Matrix",
    "text": "The Transpose of a Matrix\nGiven an \\(m \\times n\\) matrix \\(A,\\) the transpose of \\(A\\) is the matrix we get by interchanging its rows and columns.\nIt is denoted \\(A^T\\). Its shape is \\(n \\times m\\).\n\nFor example, if:\n\\[\n\\begin{array}{ccc}\nA = \\left[\\begin{array}{rr}a&b\\\\c&d\\end{array}\\right],&\nB = \\left[\\begin{array}{rr}-5&2\\\\1&-3\\\\0&4\\end{array}\\right],&\nC = \\left[\\begin{array}{rrrr}1&1&1&1\\\\-3&5&-2&7\\end{array}\\right]\n\\end{array}\n\\]\nThen:\n\\[\n\\begin{array}{ccc}\nA^T = \\left[\\begin{array}{rr}a&c\\\\b&d\\end{array}\\right],&\nB^T = \\left[\\begin{array}{rrr}-5&1&0\\\\2&-3&4\\end{array}\\right],&\nC^T = \\left[\\begin{array}{rr}1&-3\\\\1&5\\\\1&-2\\\\1&7\\end{array}\\right]\n\\end{array}\n\\]\n\n\nThe definition can be stated succinctly:\n\\[A^T_{ij} = A_{ji}.\\]\n\n\nRules for Transposes:\n\n\\((A^T)^T = A\\)\n\\((A + B)^T = A^T + B^T\\)\nFor any scalar \\(r\\), \\((rA)^T = r(A^T)\\)\n\\((AB)^T = B^TA^T\\)\n\nThe first three are pretty obvious.\nThe last one is a bit different. Memorize it. You will use it: the transpose of a product is the product of the transposes in reverse order.\n\nQuestion: For a vector in \\({\\bf x} \\in \\mathbb{R}^n\\), what is \\({\\bf x}^T\\)?\n\nAnswer: For the purposes of the definition, we treat \\({\\bf x}\\) as a \\(n \\times 1\\) matrix. So its transpose is an \\(1\\times n\\) matrix, i.e., a matrix with a single row.\n\n\nQuestion: For two vectors \\({\\bf x}\\) and \\({\\bf y}\\), what is \\({\\bf x}^T {\\bf y}\\)?\n\n\nAnswer: By the definition of matrix-vector multiplication, \\({\\bf x}^T {\\bf y} = \\sum_{i=1}^n x_i y_i.\\)\nThat is, \\({\\bf x}^T {\\bf y}\\) is the inner product of \\({\\bf x}\\) and \\({\\bf y}\\). This simple construction is a very useful one to remember.",
    "crumbs": [
      "Matrix Algebra"
    ]
  },
  {
    "objectID": "L09MatrixOperations.html#the-computational-viewpoint",
    "href": "L09MatrixOperations.html#the-computational-viewpoint",
    "title": "Geometric Algorithms",
    "section": "The Computational Viewpoint",
    "text": "The Computational Viewpoint\n\nYou recall in the last lecture I said that in Python/numpy:\nC = A @ B\nwas the same as:\nfor i in range(k):\n    C[:,k] = AxIP(A, B[:,k])\n    \nSo now you know: A @ B is really matrix multiplication of A and B. :)\n\n\nMatrix multiplication is a mainstay of computing. Thousands of applications rely heavily on matrix multiplication.\nSome examples include:\n\nComputer graphics and animation\nGoogle’s algorithm for ranking search results\nModeling mechanical structures such as aircraft and buildings\nCompressing and decompressing audio signals\nWeather modeling and prediction\nModeling quantum computing\n\nSo minimizing the time required to do matrix multiplication is immensely important.\n\n\nComplexity\nWhat is the computational complexity of matrix multiplication?\n\nFor two \\(n \\times n\\) matrices, consider the definition that uses inner product:\n\\[ (AB)_{ij} = \\sum_{k=1}^n A_{ik}B_{kj}.\\]\n\n\nSo each element of the product \\(AB\\) requires \\(n\\) multiplications and \\(n\\) additions.\nThere are \\(n^2\\) elements of \\(AB\\), so the overall computation requires\n\\[2n \\cdot n^2 = 2n^3\\]\noperations.\n\n\nThat’s not particularly good news; for two matrices of size 10,000 \\(\\times\\) 10,000 (which is not particularly large in practice), this is 2 trillion operations (2 teraflops).\n\n\nWhat is the computational complexity of matrix-vector multiplication?\nWe know that matrix-vector multiplication requires \\(n\\) inner products, each of size \\(n\\).\nSo, matrix-vector multiplication requires\n\\[2n^2\\]\noperations.\n\n\n\nOrder matters!\nWhen you look at a mathematical expression involving matrices, think carefully about what it means and how you might efficiently compute it.\n\nFor example:\nWhat is the most efficient way to compute \\(A^2{\\bf x}\\)?\nHere are your choices:\n\n\n\n\nFirst compute \\(A^2\\), then compute \\((A^2){\\bf x}\\)\n\n\n\nFirst compute \\(A{\\bf x}\\), then compute \\(A(A{\\bf x})\\)\n\n\n\n\n\n\n\\[2n^3 + 2n^2\\]\n\n\n\n\\[2 \\cdot 2n^2 = 4n^2\\]\n\n\n\n\nAgain, if we are working with a square matrix with 10,000 rows, then\n\n\n\n\n\\((A^2){\\bf x}\\) requires 2 Trillion flops\n\n\n\n\\(A(A{\\bf x})\\) requires 400 Million flops\n\n\n\n\nWhich would you choose? :)\n\n\n\n\n\nOne Trillion Pennies\n\n\n\n\nOne Million Pennies\n\n\n\n\n\nParallelization\nAlthough matrix multiplication is computationally demanding, it has a wonderful property: it is highly parallel.\nThat is, the computation needed for each element does not require computing the other elements.\n(This is not true, for example, for Gaussian elimination; think about the role of a pivot.)\n\nThis means that if we have multiple processors, and each has access to \\(A\\) and \\(B\\), the work can be divided up very cleanly.\nFor example, let’s say you have \\(n\\) processors. Then each processor can independently compute one column of the result, without needing to know anything about what the other processors are doing.\nSpecifically, processor \\(i\\) can compute its column as \\(A{\\bf b_i}\\).\nIn that case, since all processors are working in parallel, the elapsed time is reduced from \\(2n^3\\) down to \\(2n^2.\\)\n\n\n\nThe Importance of Libraries\nThe pervasive use of matrix multiplication in science and engineering means that very efficient and carefully constructed libraries have been developed for it.\nImportant issues for high performance include:\n\nhow are the matrix elements actually laid out in memory?\nwhat is the order in which matrix elements are accessed?\nwhat are the architectural details of the computer you are using?\n\nmemories, caches, number of processors, etc\n\n\nThe premier library is called LAPACK.\nLAPACK has been developed over the past 40 years and is updated frequently to tune it for new computer hardware.\n\nPython’s “numpy” uses LAPACK under the hood for its matrix computations.\nHence, even though Python is an interpreted language, for doing intensive matrix computations it is very fast, just as fast as compiled code.",
    "crumbs": [
      "Matrix Algebra"
    ]
  },
  {
    "objectID": "L05Axb.html",
    "href": "L05Axb.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\n\n\n\nJ. J. Sylvester\n\n\n\nArthur Cayley\n\n\n\n\nSylvester photo from:http://commons.wikimedia.org/wiki/Image:James_Joseph_Sylvester.jpg, Public Domain, Link\nCayley photo by Herbert Beraud (1845–1896) - http://www-groups.dcs.st-and.ac.uk/~history/PictDisplay/Cayley.html, Public Domain, Link.\nAlthough ways of solving linear systems were known to the ancient Chinese, the 8th century Arabs, and were used to great effect by Gauss in the 1790s, the idea that the information content of a linear system should be captured in a matrix was not developed until J.J. Sylvester in 1850. He gave it the term matrix because he saw it as a sort of “womb” out of which many mathematical objects could be delivered.\nEven then, it took a long time for the concept to emerge that one might want to multiply using matrices – that a matrix was an algebraic object. It was not until 1855 that Arthur Cayley came up with the “right” definition of how to multiply using matrices – the one that we use today.",
    "crumbs": [
      "$A\\mathbf{x} = \\mathbf{b}$"
    ]
  },
  {
    "objectID": "L05Axb.html#amathbfx-mathbfb",
    "href": "L05Axb.html#amathbfx-mathbfb",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\n\n\n\nJ. J. Sylvester\n\n\n\nArthur Cayley\n\n\n\n\nSylvester photo from:http://commons.wikimedia.org/wiki/Image:James_Joseph_Sylvester.jpg, Public Domain, Link\nCayley photo by Herbert Beraud (1845–1896) - http://www-groups.dcs.st-and.ac.uk/~history/PictDisplay/Cayley.html, Public Domain, Link.\nAlthough ways of solving linear systems were known to the ancient Chinese, the 8th century Arabs, and were used to great effect by Gauss in the 1790s, the idea that the information content of a linear system should be captured in a matrix was not developed until J.J. Sylvester in 1850. He gave it the term matrix because he saw it as a sort of “womb” out of which many mathematical objects could be delivered.\nEven then, it took a long time for the concept to emerge that one might want to multiply using matrices – that a matrix was an algebraic object. It was not until 1855 that Arthur Cayley came up with the “right” definition of how to multiply using matrices – the one that we use today.",
    "crumbs": [
      "$A\\mathbf{x} = \\mathbf{b}$"
    ]
  },
  {
    "objectID": "L05Axb.html#multiplying-a-matrix-by-a-vector",
    "href": "L05Axb.html#multiplying-a-matrix-by-a-vector",
    "title": "Geometric Algorithms",
    "section": "Multiplying a Matrix by a Vector",
    "text": "Multiplying a Matrix by a Vector\nLast lecture we studied the vector equation:\n\\[ x_1{\\bf a_1} + x_2{\\bf a_2} + ... + x_n{\\bf a_n} = {\\bf b}.\\]\n\nWe also talked about how one can form a matrix from a set of vectors:\n\\[ A = [{\\bf a_1} \\; {\\bf a_2} \\; ... \\;{\\bf a_n}].\\]\n\n\nIn particular, if we have \\(n\\) vectors \\({\\bf a_1},{\\bf a_2}, ..., {\\bf a_n} \\in \\mathbb{R}^m,\\) then \\(A\\) will be an \\(m \\times n\\) matrix:\n\\[ m\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\n\\overbrace{\\left[\\begin{array}{cccc}\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_1}\\\\\\vdots\\\\\\vdots\\end{array}&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_2}\\\\\\vdots\\\\\\vdots\\end{array}&\\dots&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_n}\\\\\\vdots\\\\\\vdots\\end{array}\\\\\\end{array}\\right]}^{\\large n} \\]\n\nWe’re now going to express a vector equation in a new way. We are going to say that\n\\[x_1{\\bf a_1} + x_2{\\bf a_2} + ... + x_n{\\bf a_n}\\]\nis the same as:\n\\[A{\\bf x}\\]\n\nwhere \\(A = \\left[\\begin{array}{cccc}\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_1}\\\\\\vdots\\\\\\vdots\\end{array}&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_2}\\\\\\vdots\\\\\\vdots\\end{array}&\\dots&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_n}\\\\\\vdots\\\\\\vdots\\end{array}\\\\\\end{array}\\right]\\) and \\({\\bf x} = \\left[\\begin{array}{c}x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{array}\\right].\\)\n\n\nWhat we have done is to define Matrix-vector multiplication.\nThat is, \\(A{\\bf x}\\) denotes multiplying a matrix and a vector, and we define it to be the same as a linear combination of the vectors \\(\\{\\bf a_1, a_2, \\dots, a_n\\}\\) using weights \\(x_1, x_2, \\dots, x_n.\\)\n\nHere is the definition:\nIf \\(A\\) is an \\(m\\times n\\) matrix, with columns \\(\\bf a_1, a_2, \\dots, a_n,\\) and if \\({\\bf x} \\in \\mathbb{R}^n,\\) then the product of \\(A\\) and \\(\\bf x\\), denoted \\(A{\\bf x}\\), is the linear combination of the columns of \\(A\\) using the corresponding entries in \\(x\\) as weights; that is,\n\n\\[ A{\\bf x} = [{\\bf a_1} \\; {\\bf a_2} \\; ... \\;{\\bf a_n}]\\;\\left[\\begin{array}{c}x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{array}\\right] = x_1{\\bf a_1} + x_2{\\bf a_2} + ... + x_n{\\bf a_n}.\\]\n\n\nNotice that if \\(A\\) is an \\(m\\times n\\) matrix, then \\({\\bf x}\\) must be in \\(\\mathbb{R}^n\\) in order for this operation to be defined. That is, the number of columns of \\(A\\) must match the number of rows of \\({\\bf x}.\\)\nIf the number of columns of \\(A\\) does not match the number of rows of \\({\\bf x}\\), then \\(A{\\bf x}\\) has no meaning.\n\n\nMatrix-Vector Multiplication Example\n\\[\\left[\\begin{array}{rrr}1&2&-1\\\\0&-5&3\\end{array}\\right]\\;\\left[\\begin{array}{r}4\\\\3\\\\7\\end{array}\\right] =\\]\n\n\\[ 4\\left[\\begin{array}{r}1\\\\0\\end{array}\\right] + 3\\left[\\begin{array}{r}2\\\\-5\\end{array}\\right] + 7\\left[\\begin{array}{r}-1\\\\3\\end{array}\\right]\n= \\]\n\n\n\\[\\left[\\begin{array}{r}4\\\\0\\end{array}\\right] + \\left[\\begin{array}{r}6\\\\-15\\end{array}\\right]+ \\left[\\begin{array}{r}-7\\\\21\\end{array}\\right] = \\]\n\n\n\\[\\left[\\begin{array}{r}3\\\\6\\end{array}\\right] \\]\n\n\n\nRelationship to Span\nFor \\({\\bf v_1}, {\\bf v_2}, {\\bf v_3} \\in \\mathbb{R}^m,\\) write the linear combination \\(3{\\bf v_1} + 5{\\bf v_2} + 7{\\bf v_3}\\) as a matrix times a vector.\n\n\\[3{\\bf v_1} + 5{\\bf v_2} + 7{\\bf v_3} = [{\\bf v_1} \\; {\\bf v_2} \\; {\\bf v_3}]\\;\\left[\\begin{array}{c}3\\\\5\\\\7\\end{array}\\right] = A{\\bf x}.\\]\n\nNotice that if \\(A = [{\\bf a_1} \\; {\\bf a_2} \\; ... \\;{\\bf a_n}]\\) then \\(A{\\bf x}\\) is some point that lies in Span\\(\\{{\\bf a_1},{\\bf a_2}, \\dots ,{\\bf a_n}\\}\\).\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "$A\\mathbf{x} = \\mathbf{b}$"
    ]
  },
  {
    "objectID": "L05Axb.html#the-matrix-equation",
    "href": "L05Axb.html#the-matrix-equation",
    "title": "Geometric Algorithms",
    "section": "The Matrix Equation",
    "text": "The Matrix Equation\n\nNow let’s write an equation. Let’s start with a linear system:\n\\[\\begin{array}{rcl}x_1+2x_2-x_3&=&4\\\\-5x_2+3x_3&=&1\\end{array}\\]\n\n\nWe saw last lecture that this linear system is equivalent to a vector equation, namely\n\\[x_1\\left[\\begin{array}{r}1\\\\0\\end{array}\\right] + x_2\\left[\\begin{array}{r}2\\\\-5\\end{array}\\right] + x_3\\left[\\begin{array}{r}-1\\\\3\\end{array}\\right] = \\left[\\begin{array}{r}4\\\\1\\end{array}\\right]\\]\n\n\nNow, today we have learned that this vector equation is also equivalent to this matrix equation:\n\\[\\left[\\begin{array}{rrr}1&2&-1\\\\0&-5&3\\end{array}\\right]\\;\\left[\\begin{array}{r}x_1\\\\x_2\\\\x_3\\end{array}\\right] = \\left[\\begin{array}{r}4\\\\1\\end{array}\\right].\\]\n\n\nwhich has the form\n\\[A{\\bf x} = {\\bf b}.\\]\n\n\nNotice how simple it is to go from the linear system to the matrix equation: we put the coefficients of the linear system into \\(A\\), and the right hand side values into \\({\\bf b},\\) and we can then write the matrix equation \\(A{\\bf x} = {\\bf b}\\) immediately.\n\nOK, let’s write this out formally.\nTheorem. If \\(A\\) is an \\(m\\times n\\) matrix, with columns \\(\\bf a_1, a_2, \\dots, a_n,\\) and if \\(\\bf b\\) is in \\(\\mathbb{R}^m\\), the matrix equation\n\\[ A{\\bf x} = {\\bf b}\\]\nhas the same solution set as the vector equation\n\\[ x_1{\\bf a_1} + x_2{\\bf a_2} + ... + x_n{\\bf a_n} = {\\bf b}\\]\nwhich, in turn, has the same solution set as the system of linear equations whose augmented matrix is\n\\[[{\\bf a_1} \\; {\\bf a_2} \\; ... \\;{\\bf a_n}\\;{\\bf b}].\\]\n\nThis is an absolutely key result, because it gives us power to solve problems from three different viewpoints. If we are looking for a solution set \\(x_1, x_2, \\dots, x_n,\\) there are three different questions we can ask:\n\nWhat is the solution set of the linear system \\([{\\bf a_1} \\; {\\bf a_2} \\; ... \\;{\\bf a_n}\\;{\\bf b}]\\)?\nWhat is the solution set of the vector equation \\(x_1{\\bf a_1} + x_2{\\bf a_2} + ... + x_n{\\bf a_n} = {\\bf b}?\\)\nWhat is the solution set of the matrix equation \\(A{\\bf x} = {\\bf b}?\\)\n\nWe now know that all these questions have the same answer. And all of them can be solved by row reducing the augmented matrix \\([{\\bf a_1} \\; {\\bf a_2} \\; ... \\;{\\bf a_n}\\;{\\bf b}].\\)\n\nIn particular, one of the most common questions you will encounter is:\n\ngiven a matrix \\(A\\) and a vector \\({\\bf b}\\), is there an \\({\\bf x}\\) that makes \\(A{\\bf x} = {\\bf b}\\) true?\n\nYou will encounter this form of question quite often in fields like computer graphics, data mining, algorithms, or quantum computing.",
    "crumbs": [
      "$A\\mathbf{x} = \\mathbf{b}$"
    ]
  },
  {
    "objectID": "L05Axb.html#existence-of-solutions",
    "href": "L05Axb.html#existence-of-solutions",
    "title": "Geometric Algorithms",
    "section": "Existence of Solutions",
    "text": "Existence of Solutions\nWe can see that \\(A{\\bf x} = {\\bf b}\\) has a solution if and only if \\(\\bf b\\) is a linear combination of the columns of \\(A\\) – that is, if \\(\\bf b\\) lies in Span\\(\\{{\\bf a_1},{\\bf a_2}, \\dots ,{\\bf a_n}\\}.\\)\nWe can adopt the term “consistent” here too – i.e., is \\(A{\\bf x} = {\\bf b}\\) consistent?\n\nNow we will ask a more general question, which is specifically about the matrix \\(A\\). That is:\n\nDoes \\(A{\\bf x} = {\\bf b}\\) have a solution for any \\({\\bf b}\\)?\n\n\n\nExample.\nLet \\(A = \\left[\\begin{array}{rrr}1&3&4\\\\-4&2&-6\\\\-3&-2&-7\\end{array}\\right]\\) and \\({\\bf b} = \\left[\\begin{array}{r}b_1\\\\b_2\\\\b_3\\end{array}\\right].\\) Is the equation \\(A{\\bf x} = {\\bf b}\\) consistent for all possible \\(b_1, b_2, b_3?\\)\n\n\nWe know how to test for consistency: form the augmented matrix and row reduce it.\n\\[\\left[\\begin{array}{rrrc}1&3&4&b_1\\\\-4&2&-6&b_2\\\\-3&-2&-7&b_3\\end{array}\\right] \\sim \\left[\\begin{array}{rrrc}1&3&4&b_1\\\\0&14&10&b_2+4b_1\\\\0&7&5&b_3+3b_1\\end{array}\\right] \\sim \\left[\\begin{array}{rrrc}1&3&4&b_1\\\\0&14&10&b_2+4b_1\\\\0&0&0&b_3+3b_1-\\frac{1}{2}(b_2 + 4b_1)\\end{array}\\right].\\]\n\n\nThe third entry in the column 4, simplified, is \\(b_1 -\\frac{1}{2}b_2+ b_3.\\) This expression can be nonzero.\nFor example, take \\(b_1 = 1, b_2 = 1, b_3 = 1;\\) then the lower right entry is 1.5. This gives a row of the form \\(0 = k\\) which indicates an inconsistent system.\nSo the answer is “no, \\(A{\\bf x} = {\\bf b}\\) is not consistent for all possible \\(b_1, b_2, b_3.\\)”\n\n\nHere is the echelon form of \\(A\\) (note we are talking only about \\(A\\) here, not the augmented matrix):\n\\[\\left[\\begin{array}{rrr}1&3&4\\\\0&14&10\\\\0&0&0\\end{array}\\right].\\]\nThe equation \\(A{\\bf x} = {\\bf b}\\) fails to be consistent for all \\(\\bf b\\) because the echelon form of \\(A\\) has a row of zeros.\nIf \\(A\\) had a pivot in all three rows, we would not care about the calculations in the augmented column because no matter what is in the augmented column, an echelon form of the augmented matrix could not have a row indicating \\(0 = k\\).\n\nQuestion: OK, so \\(A{\\bf x} = {\\bf b}\\) is not always consistent. When is \\(A{\\bf x} = {\\bf b}\\) consistent?\n\nAnswer: When \\(b_1 -\\frac{1}{2}b_2+ b_3 = 0.\\) This makes the last row of the augmented matrix all zeros.\nThis is a single equation in three unknowns, so it defines a plane through the origin in \\(\\mathbb{R}^3.\\)\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nThe set of points that makes \\(b_1 - \\frac{1}{2}b_2 + b_3 = 0\\) is the set of \\(\\bf b\\) values that makes \\(A{\\bf x} = {\\bf b}\\) consistent. But if \\(A{\\bf x} = {\\bf b}\\) then \\({\\bf b}\\) lies within Span\\(\\{\\bf a_1, a_2, a_3\\}\\).\n\n\nSo we have determined that \\(\\operatorname{Span}\\{\\bf a_1, a_2, a_3\\}\\) is the same point set as \\(b_1 - \\frac{1}{2}b_2 + b_3 = 0\\).\nThat means that \\(\\operatorname{Span}\\{\\bf a_1, a_2, a_3\\}\\) is a plane in this case. In the figure we have plotted \\(\\bf a_1, a_2,\\) and \\(\\bf a_3\\) and you can see that all three points, as well as the origin, lie in the same plane.\n\n\nNow, in the most general case, four points will not line in a plane. In other words, the span of three vectors in \\(\\mathbb{R}^3\\) can be the whole space \\(\\mathbb{R}^3\\).\n\nNow we will tie together all three views of our question.\nTheorem: Let \\(A\\) be an \\(m \\times n\\) matrix. Then the following statements are logically equivalent. That is, for a particular \\(A,\\) either they are all true or they are all false.\n\nFor each \\(\\bf b\\) in \\(\\mathbb{R}^m,\\) the equation \\(A{\\bf x} = {\\bf b}\\) has a solution.\nEach \\(\\bf b\\) in \\(\\mathbb{R}^m\\) is a linear combination of the columns of \\(A\\).\nThe columns of \\(A\\) span \\(\\mathbb{R}^m\\).\n\\(A\\) has a pivot position in every row.\n\n\nThis theorem is very powerful and general.\nNotice that we started by thinking about a matrix equation, but we ended by making statements about \\(A\\) alone.\nThis is the first time we are studying the property of a matrix itself; we will study many properties of matrices on their own as we progress.",
    "crumbs": [
      "$A\\mathbf{x} = \\mathbf{b}$"
    ]
  },
  {
    "objectID": "L05Axb.html#inner-product",
    "href": "L05Axb.html#inner-product",
    "title": "Geometric Algorithms",
    "section": "Inner Product",
    "text": "Inner Product\nThere is a simpler way to compute the matrix-vector product. It uses a concept called the inner product or dot product.\nThe inner product is defined for two sequences of numbers, and returns a single number. For example, the expression\n\\[[3\\;5\\;1] \\left[\\begin{array}{c}2\\\\-1\\\\4\\end{array}\\right]\\]\nis the same as\n\\[(3\\cdot 2) + (5\\cdot -1) + (1 \\cdot 4)\\]\nwhich is the sum of the products of the corresponding entries, and yields the scalar value 5. This is the inner product of the two sequences.\n\nThe general definition of the inner product is:\n\\[[x_1\\;x_2\\;\\dots\\;x_n] \\left[\\begin{array}{c}y_1\\\\y_2\\\\\\vdots\\\\y_n\\end{array}\\right] = \\sum_{i=1}^n x_i y_i\\]\n\nLet’s see how this can be used to think about matrix-vector multiplication.\nConsider the matrix-vector multiplication\n\\[\\left[\\begin{array}{rrr}2&3&4\\\\-1&5&-3\\\\6&-2&8\\end{array}\\right]\\;\\left[\\begin{array}{r}x_1\\\\x_2\\\\x_3\\end{array}\\right]\\]\n\n\\[= x_1\\left[\\begin{array}{r}2\\\\-1\\\\6\\end{array}\\right] + x_2\\left[\\begin{array}{r}3\\\\5\\\\-2\\end{array}\\right] + x_3\\left[\\begin{array}{r}4\\\\-3\\\\8\\end{array}\\right]\\]\n\n\n\\[= \\left[\\begin{array}{r}2x_1\\\\-1x_1\\\\6x_1\\end{array}\\right] + \\left[\\begin{array}{r}3x_2\\\\5x_2\\\\-2x_2\\end{array}\\right] + \\left[\\begin{array}{r}4x_3\\\\-3x_3\\\\8x_3\\end{array}\\right]\\]\n\n\n\\[= \\left[\\begin{array}{r}2x_1+3x_2+4x_3\\\\-1x_1+5x_2-3x_3\\\\6x_1-2x_2+8x_3\\end{array}\\right] \\]\n\n\nNow, what is the first entry in the result? It is the inner product of the first row of \\(A\\) and \\(\\bf x\\). That is,\n\\[[2\\;3\\;4] \\left[\\begin{array}{c}x_1\\\\x_2\\\\x_3\\end{array}\\right] = [2x_1+3x_2+4x_3]\\]\n\n\nThis is a fast and simple way to compute the result of a matrix-vector product. Each element of the result is the inner product of the corresponding row of \\(A\\) and the vector \\(\\bf x\\).\n\n\n\\[ \\left[\\begin{array}{ccc}2&3&4\\\\&&\\\\&&\\end{array}\\right]\\left[\\begin{array}{c}x_1\\\\x_2\\\\x_3\\end{array}\\right] = \\left[\\begin{array}{ccc}2x_1&+3x_2&+4x_3\\\\&&\\\\&&\\end{array}\\right]\\]\n\\[ \\left[\\begin{array}{ccc}&&\\\\-1&5&-3\\\\&&\\end{array}\\right]\\left[\\begin{array}{c}x_1\\\\x_2\\\\x_3\\end{array}\\right] = \\left[\\begin{array}{ccc}&&\\\\-x_1&+5x_2&-3x_3\\\\&&\\end{array}\\right]\\]\n\n\nExample: Matrix-vector multiplication using Inner Products\nHere we compute the same result as before, but now using inner products:\n\\[\\left[\\begin{array}{rrr}1&2&-1\\\\0&-5&3\\end{array}\\right]\\left[\\begin{array}{r}4\\\\3\\\\7\\end{array}\\right] = \\left[\\begin{array}{rrr}1\\cdot 4&+2\\cdot 3&-1\\cdot 7\\\\0\\cdot 4&-5\\cdot 3&+3\\cdot 7\\end{array}\\right] = \\left[\\begin{array}{r}3\\\\6\\end{array}\\right].\\]",
    "crumbs": [
      "$A\\mathbf{x} = \\mathbf{b}$"
    ]
  },
  {
    "objectID": "L05Axb.html#an-interesting-matrix",
    "href": "L05Axb.html#an-interesting-matrix",
    "title": "Geometric Algorithms",
    "section": "An (I)nteresting matrix",
    "text": "An (I)nteresting matrix\nConsider this matrix equation:\n\\[\\left[\\begin{array}{rrr}1&0&0\\\\0&1&0\\\\0&0&1\\end{array}\\right]\\;\\left[\\begin{array}{r}r\\\\s\\\\t\\end{array}\\right] = \\left[\\begin{array}{rrr}1\\cdot r&+0\\cdot s&+0\\cdot t\\\\0\\cdot r&+1\\cdot s&+0\\cdot t\\\\0\\cdot r&+0\\cdot s&+1\\cdot t\\end{array}\\right] = \\left[\\begin{array}{r}r\\\\s\\\\t\\end{array}\\right].\\]\n\nThis sort of matrix – with 1s on the diagonal and 0s everywhere else – is called an identity matrix and is denoted by \\(I\\).\nYou can see that for any \\(\\bf x\\), \\(I\\bf x = x.\\)",
    "crumbs": [
      "$A\\mathbf{x} = \\mathbf{b}$"
    ]
  },
  {
    "objectID": "L05Axb.html#algebraic-properties-of-the-matrix-vector-product",
    "href": "L05Axb.html#algebraic-properties-of-the-matrix-vector-product",
    "title": "Geometric Algorithms",
    "section": "Algebraic Properties of the Matrix-Vector Product",
    "text": "Algebraic Properties of the Matrix-Vector Product\nWe can now begin to lay out the algebra of matrices.\nThat is, we can define rules for valid symbolic manipulation of matrices.\nTheorem. If \\(A\\) is an \\(m \\times n\\) matrix, \\(\\bf u\\) and \\(\\bf v\\) are vectors in \\(\\mathbb{R}^n\\), and \\(c\\) is a scalar, then:\n\n\\(A({\\bf u} + {\\bf v}) = A{\\bf u} + A{\\bf v};\\)\n\\(A(c{\\bf u}) = c(A{\\bf u}).\\)\n\n\nProof: Let us take \\(n=3\\), \\(A = [\\bf a_1\\;a_2\\;a_3],\\) and \\(\\bf u,v \\in \\mathbb{R}^3.\\)\nThen to prove (1.):\n\\[A({\\bf u} + {\\bf v}) = [{\\bf a_1\\;a_2\\;a_3}]\\left[\\begin{array}{r}u_1+v_1\\\\u_2+v_2\\\\u_3+v_3\\end{array}\\right]\\]\n\n\n\\[=(u_1+v_1){\\bf a_1} + (u_2+v_2){\\bf a_2} + (u_3+v_3){\\bf a_3}\\]\n\n\n\\[=(u_1{\\bf a_1} + u_2{\\bf a_2} + u_3{\\bf a_3}) + (v_1{\\bf a_1} + v_2{\\bf a_2} + v_3{\\bf a_3})\\]\n\n\n\\[=A{\\bf u} + A{\\bf v}.\\]\n\n\nAnd to prove (2.):\n\\[A(c{\\bf u}) = [{\\bf a_1\\;a_2\\;a_3}]\\left[\\begin{array}{r}c u_1\\\\c u_2\\\\c u_3\\end{array}\\right]\\]\n\n\n\\[ = (c u_1){\\bf a_1} + (c u_2){\\bf a_2} + (c u_3){\\bf a_3}\\]\n\n\n\\[= c (u_1{\\bf a_1}) + c (u_2{\\bf a_2}) + c (u_3{\\bf a_3})\\]\n\n\n\\[= c (u_1{\\bf a_1} + u_2{\\bf a_2} + u_3{\\bf a_3})\\]\n\n\n\\[= c (A{\\bf u}).\\]",
    "crumbs": [
      "$A\\mathbf{x} = \\mathbf{b}$"
    ]
  },
  {
    "objectID": "L01LinearEquations.html",
    "href": "L01LinearEquations.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\nTraditionally, algebra was the art of solving equations and systems of equations. The word algebra comes form the Arabic al-jabr which means restoration (of broken parts).\nThe term was first used in a mathematical sense by Mohammed al-Khowarizmi (c. 780-850) who worked at the House of Wisdom, an academy established by Caliph al Ma’mum in Baghdad.\nLinear algebra, then, is the art of solving systems of linear equations.\nLinear Algebra with Applications, Bretscher\n\n\n\n\nAl-Khowarizmi gave his name to the algorithm.\nHe wrote a book called ilm al-jabr wa’l-muqābala’ which means “The science of restoring what is missing and equating like with like.”\n\nOur entry point into linear algebra will be solving systems of linear equations.\n\n\n\n\n\n\n\nSource\n\nHere is a famous example of such a problem.\n\n\n\nThe yield of one bundle of inferior rice, two bundles of medium grade rice, and three bundles of superior rice is 39 dou of grain. The yield of one bundle of inferior rice, three bundles of medium grade rice, and two bundles of superior rice is 34 dou. The yield of three bundles of inferior rice, two bundles of medium grain rice, and one bundle of superior rice is 26 dou. What is the yield of one bundle of each grade of rice?\nNine Chapters on the Mathematical Art, c. 200 BCE, China\n\n\n\n\n\n\n\n九章算術細草圖說 by 中國書店海王邨公司 - http://pmgs.kongfz.com/detail/1_158470/. Licensed under Public Domain via Wikimedia Commons.\n\nLet’s denote the unknown quantities as \\(x_1\\), \\(x_2\\), and \\(x_3\\). These are the yields of one bundle of inferior, medium grade, and superior rice, respectively. We can then write the problem as:\n\\[\n\\begin{array}{rcr}\nx_1 + 2 x_2 + 3 x_3 &=& 39\\\\\nx_1 + 3 x_2 + 2 x_3 &=& 34\\\\\n3 x_1 + 2 x_2 + x_3 &=& 26\n\\end{array}\n\\]\nThe problem then is to determine the values of \\(x_1, x_2,\\) and \\(x_3\\).\n\n\nThese are linear equations. A linear equation is one in which no term has power other than 1.\nFor example, there are no terms involving \\(x_1^2\\), or \\(x_1x_2\\), or \\(\\sqrt{x_3}\\).",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "L01LinearEquations.html#linear-equations",
    "href": "L01LinearEquations.html#linear-equations",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\nTraditionally, algebra was the art of solving equations and systems of equations. The word algebra comes form the Arabic al-jabr which means restoration (of broken parts).\nThe term was first used in a mathematical sense by Mohammed al-Khowarizmi (c. 780-850) who worked at the House of Wisdom, an academy established by Caliph al Ma’mum in Baghdad.\nLinear algebra, then, is the art of solving systems of linear equations.\nLinear Algebra with Applications, Bretscher\n\n\n\n\nAl-Khowarizmi gave his name to the algorithm.\nHe wrote a book called ilm al-jabr wa’l-muqābala’ which means “The science of restoring what is missing and equating like with like.”\n\nOur entry point into linear algebra will be solving systems of linear equations.\n\n\n\n\n\n\n\nSource\n\nHere is a famous example of such a problem.\n\n\n\nThe yield of one bundle of inferior rice, two bundles of medium grade rice, and three bundles of superior rice is 39 dou of grain. The yield of one bundle of inferior rice, three bundles of medium grade rice, and two bundles of superior rice is 34 dou. The yield of three bundles of inferior rice, two bundles of medium grain rice, and one bundle of superior rice is 26 dou. What is the yield of one bundle of each grade of rice?\nNine Chapters on the Mathematical Art, c. 200 BCE, China\n\n\n\n\n\n\n\n九章算術細草圖說 by 中國書店海王邨公司 - http://pmgs.kongfz.com/detail/1_158470/. Licensed under Public Domain via Wikimedia Commons.\n\nLet’s denote the unknown quantities as \\(x_1\\), \\(x_2\\), and \\(x_3\\). These are the yields of one bundle of inferior, medium grade, and superior rice, respectively. We can then write the problem as:\n\\[\n\\begin{array}{rcr}\nx_1 + 2 x_2 + 3 x_3 &=& 39\\\\\nx_1 + 3 x_2 + 2 x_3 &=& 34\\\\\n3 x_1 + 2 x_2 + x_3 &=& 26\n\\end{array}\n\\]\nThe problem then is to determine the values of \\(x_1, x_2,\\) and \\(x_3\\).\n\n\nThese are linear equations. A linear equation is one in which no term has power other than 1.\nFor example, there are no terms involving \\(x_1^2\\), or \\(x_1x_2\\), or \\(\\sqrt{x_3}\\).",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "L01LinearEquations.html#basic-definitions",
    "href": "L01LinearEquations.html#basic-definitions",
    "title": "Geometric Algorithms",
    "section": "Basic Definitions",
    "text": "Basic Definitions\n\n\nA linear equation in the variables \\(x_1, \\dots, x_n\\) is an equation that can be written in the form \\[a_1 x_1 + a_2 x_2 + \\dots + a_n x_n = b\\] where \\(b\\) and the coefficients \\(a_1, \\dots, a_n\\) are real or complex numbers that are usually known in advance.\nA system of linear equations (or linear system ) is a collection of one or more linear equations involving the same variables - say \\(x_1, \\dots, x_n\\).\n\n\n\n\nA solution of the system is a list of numbers \\((s_1, s_2, \\dots, s_n)\\) that makes each equation a true statement when the values \\(s_1, s_2, \\dots, s_n\\) are substituted for \\(x_1, x_2, \\dots, x_n,\\) respectively.\nThe set of all possible solutions is called the solution set of the linear system.\nTwo linear systems are called equivalent if they have the same solution set.",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "L01LinearEquations.html#how-many-solutions",
    "href": "L01LinearEquations.html#how-many-solutions",
    "title": "Geometric Algorithms",
    "section": "How Many Solutions?",
    "text": "How Many Solutions?\n\nA system of linear equations has:\n\n\nno solution, or\nexactly one solution, or\ninfinitely many solutions.\n\n\n\n\n\n\nA system of linear equations is said to be consistent if it has either one solution or infinitely many solutions.\nA system of linear equations is said to be inconsistent if it has no solution.",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "L01LinearEquations.html#the-geometry-of-linear-equations",
    "href": "L01LinearEquations.html#the-geometry-of-linear-equations",
    "title": "Geometric Algorithms",
    "section": "The Geometry of Linear Equations",
    "text": "The Geometry of Linear Equations\n\nAny list of numbers \\((s_1, s_2, \\dots, s_n)\\) can be thought of as a point in \\(n\\)-dimensional space.\nWe call that space \\(\\mathbb{R}^n\\).\nSo if we are considering linear equations with \\(n\\) unknowns, the solutions are points in \\(\\mathbb{R}^n\\).\n\n\nNow, any linear equation defines a point set with dimension one less than the space. For example:\n\nif we are in 2-space (2 unknowns), a linear equation defines a line.\nif we are in 3-space (3 unknowns), a linear equation defines a plane.\nin higher dimensions, we refer to all such sets as hyperplanes.\n\nQuestion: why does a linear equation define a point-set of dimension one less than the space?",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "L01LinearEquations.html#some-examples-in-mathbbr2",
    "href": "L01LinearEquations.html#some-examples-in-mathbbr2",
    "title": "Geometric Algorithms",
    "section": "Some Examples in \\(\\mathbb{R}^2\\)",
    "text": "Some Examples in \\(\\mathbb{R}^2\\)\n\nHow many solutions does the linear system have in each case?\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe above system of two equations has exactly one solution.\n\n\n\n\n\n\n\n\n\n\n\nThe above system of two equations has no solutions.\n\n\n\n\n\n\n\n\n\n\n\nThe above system of equations has infinitely many solutions.",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "L01LinearEquations.html#some-examples-in-mathbbr3",
    "href": "L01LinearEquations.html#some-examples-in-mathbbr3",
    "title": "Geometric Algorithms",
    "section": "Some Examples in \\(\\mathbb{R}^3\\)",
    "text": "Some Examples in \\(\\mathbb{R}^3\\)\n\nHow many solutions are there in each of these cases?\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "L01LinearEquations.html#the-matrices-of-a-system",
    "href": "L01LinearEquations.html#the-matrices-of-a-system",
    "title": "Geometric Algorithms",
    "section": "The Matrices of a System",
    "text": "The Matrices of a System\n\nThe essential information of a linear system can be recorded compactly in a rectangular array called a matrix. For the following system of equations,\n\\[\\begin{array}{rcr}\n    x_1 - 2x_2 +x_3 &=& 5\\\\\n    2x_2 - 8x_3 &=& -4\\\\\n    6x_1 +5x_2 +9x_3 &=& -4\n\\end{array}\\]\n\n\nthe matrix\n\\[\\left[\\begin{array}{rrr}\n    1  & -2  & 1 \\\\\n    0 & 2 &  - 8 \\\\\n    6 & 5 &9\n\\end{array}\\right]\\]\nis called the coefficient matrix of the system.\n\nAn augmented matrix of a system consists of the coefficient matrix with an added column containing the constants from the right sides of the equations.\nFor the same system of equations,\n\\[\n\\begin{array}{rcr}\n    x_1 - 2x_2 +x_3 &=& 5\\\\\n    2x_2 - 8x_3 &=& -4\\\\\n    6x_1 +5x_2 +9x_3 &=& -4\n\\end{array}\n\\]\n\nthe matrix\n\\[\\left[\\begin{array}{rrrr}\n    1  & -2  & 1 & 5\\\\\n    0 & 2 &  - 8 & -4\\\\\n    6 & 5 &9 & -4\n\\end{array}\\right]\\]\nis called the augmented matrix of the system.\n\n\nA matrix with \\(m\\) rows and \\(n\\) columns is referred to as ‘’an \\(m \\times n\\) matrix’’ and is an element of the set \\(\\mathbb{R}^{m\\times n}.\\)\n(Note that we always list the number of rows first, then the number of columns.)",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "L01LinearEquations.html#solving-linear-systems",
    "href": "L01LinearEquations.html#solving-linear-systems",
    "title": "Geometric Algorithms",
    "section": "Solving Linear Systems",
    "text": "Solving Linear Systems\n\nTo solve a linear system, we transform it into a new system which is equivalent to the old system, meaning it has the same solution set.\nHowever, in the new system the solution is explicit.\nWe can make these transformations because of three facts.\n\nFact Number 1: Given a set of linear equations, we can add one equation to another without changing the solution set.\n\nBy definition, any solution of the old system makes each old equation true; therefore any solution of the old system makes each new equation true.\n\n\nIt’s also true that any assignment of values to variables that is not a solution of the old system, is not a solution of the new system.\n\n\nFor example, this system:\n\\[\n\\begin{array}{rcr}\n3x_1 + 2x_2 &=& -3\\\\\n-x_1 + 4x_2 &=& 2\\\\\n\\end{array}\n\\]\n\n\n… has the same solution set as:\n\\[\n\\begin{array}{rcr}\n3x_1 + 2x_2 &=& -3\\\\\n2x_1 + 6x_2 &=& -1\\\\\n\\end{array}\n\\]\n\nFact Number 2: Another, more obvious fact is that we can multiply any equation by a constant (not zero) without changing its meaning (and therefore the solution set).\n\nExample:\n\\[\n3x = 2\n\\]\nhas the same solution set as:\n\\[\n9x = 6\n\\]\n\nFact Number 3: And an even more obvious fact is that we can change the order of the equations without changing anything.\n\nTogether, these three rules form a set of tools we can use to solve linear systems. Here is an example.\n\n\nStep 1: Elimination\n\nThe process we’ll describe consists of two steps: Elimination and Backsubstitution.\n\n\nThe goal of elimination is to create a triangular matrix (or system).\nThe basic operation we will repeatedly apply is to add a multiple of one equation (row) to another. We’ll do this with the equations and the matrix side-by-side.\n\n\nHere is the original system:\n\\[\\begin{array}{cc}\n\\begin{array}{rcr}\n    x_1 - 2x_2 +x_3 &=& 5\\\\\n    2x_2 - 8x_3 &=& -4\\\\\n    6x_1 +5x_2 +9x_3 &=& -4\n\\end{array}\n&\n\\;\\;\\;\\;\\;\\;\\left[\\begin{array}{rrrr}\n    1  & -2  & 1 & 5\\\\\n    0 & 2 &  - 8 & -4\\\\\n    6 & 5 &9 & -4\n\\end{array}\\right]\\\\\n\\end{array}\\]\n\n\nTo start the elimination stage, we add -6 times the first equation to the third equation:\n\\[\\begin{array}{rrrrrr}\n    &6x_1& +5x_2& +9x_3& =& -4\\\\\n+  &-6x_1& +12x_2& -6x_3& =& -30\\\\\n\\hline\n   & &      17x_2& +3x_3 &=& -34\\\\\n\\end{array}\\]\n\n\nThis gives us a new system.\n\\[\\begin{array}{cr}\n\\begin{array}{rcr}\n    x_1 - 2x_2 +x_3 &=& 5\\\\\n    2x_2 - 8x_3 &=& -4\\\\\n     17x_2 +3x_3 &=& -34\n\\end{array}\n&\n\\;\\;\\;\\;\\;\\;\\left[\\begin{array}{rrrr}\n    1  & -2  & 1 & 5\\\\\n    0 & 2 &  - 8 & -4\\\\\n    0 & 17 & 3 & -34\n\\end{array}\\right]\\\\\n\\end{array}\n\\]\nNote that this is not the same system of equations, but it is equivalent – it has the same solution set.\n\n\nNext, we multiply the second equation by \\(1/2\\) to get its leading coefficient to be 1:\n\\[\\begin{array}{cr}\n\\begin{array}{rcr}\n    x_1 - 2x_2 +x_3 &=& 5\\\\\n    x_2 - 4x_3 &=& -2\\\\\n     17x_2 +3x_3 &=& -34\n\\end{array}\n&\n\\;\\;\\;\\;\\;\\;\\left[\\begin{array}{rrrr}\n    1  & -2  & 1 & 5\\\\\n    0 & 1 &  - 4 & -2\\\\\n    0 & 17 & 3 & -34\n\\end{array}\\right]\\\\\n\\end{array}\n\\]\n\n\nNext, we multiply the second equation by \\(-17\\) and add it to the third equation:\n\\[\\begin{array}{cr}\n\\begin{array}{rcr}\n    x_1 - 2x_2 +x_3 &=& 5\\\\\n    x_2 - 4x_3 &=& -2\\\\\n         71x_3 &=& 0\n\\end{array}\n&\n\\;\\;\\;\\;\\;\\;\\left[\\begin{array}{rrrr}\n    1  & -2  & 1 & 5\\\\\n    0 & 1 &  - 4 & -2\\\\\n    0 & 0 & 71 & 0\n\\end{array}\n\\right]\\\\\n\\end{array}\n\\]\n\n\nAnd next we can divide the third equation by \\(71\\) to get its leading coefficient equal to 1:\n\\[\\begin{array}{cr}\n\\begin{array}{rcr}\n    x_1 - 2x_2 +x_3 &=& 5\\\\\n    x_2 - 4x_3 &=& -2\\\\\n           x_3 &=& 0\n\\end{array}\n&\n\\;\\;\\;\\;\\;\\;\\left[\\begin{array}{rrrr}\n    1  & -2  & 1 & 5\\\\\n    0 & 1 &  - 4 & -2\\\\\n    0 & 0 &  1 & 0\\\\\n\\end{array}\\right]\\\\\n\\end{array}\n\\]\nWe have now put the system and matrix into triangular form. In a triangular matrix, all values below the diagonal are zero.\n\n\n\nStep 2: Backsubstitution\n\nAt this point, the process shifts to backsubstitution. We now have the value for one variable, and we will substitute it into other equations to simplify them and get values for the other variables.\n\n\nAlthough we think of its as a somewhat different stage, in reality it still comes down to applying the three rules.\n\n\nFirst, we substitute the value of \\(x_3\\) into the equations above it. This is actually multiplying equation 3 by the proper value and adding it to equations above it.\n\\[\\begin{array}{cr}\n\\begin{array}{rcr}\n    x_1 - 2x_2 &=& 5\\\\\n    x_2  &=& -2\\\\\n           x_3 &=& 0\n\\end{array}\n&\n\\;\\;\\;\\;\\;\\;\\left[\\begin{array}{rrrr}\n    1  & -2  & 0 & 5\\\\\n    0 & 1 &  0 & -2\\\\\n    0 & 0 &  1 & 0\n\\end{array}\\right]\\\\\n\\end{array}\n\\]\n\n\nNext, we do the same thing with equation 2, substituting it into equation 1 above it:\n\\[\\begin{array}{cr}\n\\begin{array}{rcr}\n    x_1  &=& 1\\\\\n    x_2  &=& -2\\\\\n           x_3 &=& 0\n\\end{array}\n&\n\\;\\;\\;\\;\\;\\;\\left[\\begin{array}{rrrr}\n    1  & 0  & 0 & 1\\\\\n    0 & 1 &  0 & -2\\\\\n    0 & 0 &  1 & 0\n\\end{array}\\right]\\\\\n\\end{array}\n\\]\n\n\nNow we can read off the solution: it is \\(x_1 = 1\\), \\(x_2 = -2\\), \\(x_3 = 0\\). Notice the particular form of the resulting matrix: ones on the diagonal, zeros above and below each 1.\n\nLet’s get a sense of this process geometrically.\n\nHere are the three starting equations:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s compare the starting point and the finishing point:\n\\[\n\\begin{array}{rcr}\n    x_1 - 2x_2 +x_3 &=& 5\\\\\n    2x_2 - 8x_3 &=& -4\\\\\n    6x_1 +5x_2 +9x_3 &=& -4\n\\end{array}\n\\hspace{0.5in}\n{\\LARGE\\rightarrow}\n\\hspace{0.5in}\n\\begin{array}{rcr}\n    x_1  &=& 1\\\\\n    x_2  &=& -2\\\\\n           x_3 &=& 0\n\\end{array}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\\[\n\\left[\\begin{array}{rrrr}\n    1  & -2  & 1 & 5\\\\\n    0 & 2 &  - 8 & -4\\\\\n    6 & 5 &9 & -4\n\\end{array}\n\\right]\n\\hspace{0.5in}\n{\\LARGE\\rightarrow}\n\\hspace{0.5in}\n\\left[\\begin{array}{rrrr}\n    1  & 0  & 0 & 1\\\\\n    0 & 1 &  0 & -2\\\\\n    0 & 0 &  1 & 0\n\\end{array}\n\\right]\n\\]\nNotice how all the planes have shifted, but they still intersect in the same point. This is the geometric interpretation of equivalent systems.",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "L01LinearEquations.html#verifying-the-solution",
    "href": "L01LinearEquations.html#verifying-the-solution",
    "title": "Geometric Algorithms",
    "section": "Verifying the Solution",
    "text": "Verifying the Solution\n\n\nIt’s important that, once you have solved a system, you verify the solution\ni.e., go back and confirm that what you have computed, in fact meets the original requirements.\n\nSo, in our case here is the original system and its solution:\n\\[\n\\begin{array}{rcr}\n    x_1 - 2x_2 +x_3 &=& 5\\\\\n    2x_2 - 8x_3 &=& -4\\\\\n    6x_1 +5x_2 +9x_3 &=& -4\n\\end{array}\n\\hspace{0.5in}\n{\\LARGE\\rightarrow}\n\\hspace{0.5in}\n\\begin{array}{rcr}\n    x_1  &=& 1\\\\\n    x_2  &=& -2\\\\\n           x_3 &=& 0\n\\end{array}\n\\]\n\n\nWe can verify by substitution:\n\\[\n\\begin{array}{rcr}\n    1 - 2(-2) + 0 &=& 5\\\\\n    2(-2) - 8(0) &=& -4\\\\\n    6(1) +5(-2) +9(0) &=& -4\n\\end{array}\n\\]\nThe solution \\((1, -2, 0)\\) makes each equation true. Confirmed!",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "L01LinearEquations.html#row-equivalence",
    "href": "L01LinearEquations.html#row-equivalence",
    "title": "Geometric Algorithms",
    "section": "Row Equivalence",
    "text": "Row Equivalence\n\nOK, let’s step back and formalize what we have done.\n\n\nElementary Row Operations are the following:\n\n(Replacement) Replace one row by the sum of itself and a multiple of another row.\n(Interchange) Interchange two rows.\n(Scaling) Multiply all entries in a row by a nonzero constant.\n\n\n\nTwo matrices are called row equivalent if there is a sequence of elementary row operations that transforms one matrix into the other.\nIf the augmented matrices of two linear systems are row equivalent, then the two systems have the same solution set.",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "L01LinearEquations.html#fundamental-questions",
    "href": "L01LinearEquations.html#fundamental-questions",
    "title": "Geometric Algorithms",
    "section": "Fundamental Questions",
    "text": "Fundamental Questions\n\nWhen presented with a linear system, we always need to ask two fundamental questions:\n\nIs the system consistent; that is, does at least one solution exist?\nIf a solution exists, is there only one; that is, is the solution unique?\n\nThese really are fundamental; we will see that the answers to these questions have far-reaching implications.",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "L01LinearEquations.html#recognizing-an-inconsistent-system",
    "href": "L01LinearEquations.html#recognizing-an-inconsistent-system",
    "title": "Geometric Algorithms",
    "section": "Recognizing an Inconsistent System",
    "text": "Recognizing an Inconsistent System\n\nConsider the following system:\n\\[\n\\begin{array}{rcr}\nx_2 - 4x_3 &=& 8\\\\\n2x_1 - 3x_2 + 2x_3 &=& 1\\\\\n4x_1 - 8x_2 + 12x_3 &=& 1\n\\end{array}\n\\]\n\n\nwhose augmented matrix is:\n\\[\n\\left[\\begin{array}{rrrr}\n0&1&-4&8\\\\\n2&-3&2&1\\\\\n4&-8&12&1\n\\end{array}\\right]\n\\]\nLet’s apply our row reduction procedure to this matrix.\n\n\nFirst, we’ll interchange rows 1 and 2:\n\\[\n\\left[\\begin{array}{rrrr}\n2&-3&2&1\\\\\n0&1&-4&8\\\\\n4&-8&12&1\n\\end{array}\\right]\n\\]\n\n\nNext, we’ll eliminate the \\(4x_1\\) term in the third equation by adding \\(-2\\) times row 1 to row 3:\n\\[\n\\left[\\begin{array}{rrrr}\n2&-3&2&1\\\\\n0&1&-4&8\\\\\n0&-2&8&-1\n\\end{array}\\right]\n\\]\n\n\nNext, we use the \\(x_2\\) term in the second equation to eliminate the \\(-2x_2\\) term from the third equation (that is, add 2 times row 2 to row 3).\n\\[\n\\left[\\begin{array}{rrrr}\n2&-3&2&1\\\\\n0&1&-4&8\\\\\n0&0&0&15\n\\end{array}\\right]\n\\]\n\n\nThis matrix is now in triangular form.\nWhat does it mean? In particular, what does the last row say?\n\n\nThe last row stands for the equation:\n\\[ 0x_1 + 0x_2 + 0x_3 = 15.\\]\n\n\nClearly, this equation has no solution.\nNow, we know that row reductions never change the solution set of a system. So, the original set of equations also has no solution.\nThe system of equations is inconsistent.\n\n\nWe can see that a system that leads by row reductions to an equation of the form \\(0 = k\\) for some nonzero \\(k\\) must be inconsistent.\nIn fact, we will show later that any inconsistent system will lead by row reductions to an equation of the form \\(0 = k\\) for some nonzero \\(k\\).",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "L01LinearEquations.html#geometric-interpretation-of-inconsistency",
    "href": "L01LinearEquations.html#geometric-interpretation-of-inconsistency",
    "title": "Geometric Algorithms",
    "section": "Geometric Interpretation of Inconsistency",
    "text": "Geometric Interpretation of Inconsistency\n\nHere are our original equations, as hyperplanes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nThe figure illustrates that the two intersection lines are parallel. So there is no point that lies in all the hyperplanes. That is the geometric interpretation of inconsistency.",
    "crumbs": [
      "Linear Equations"
    ]
  },
  {
    "objectID": "postscript.html",
    "href": "postscript.html",
    "title": "Postscript",
    "section": "",
    "text": "Postscript\n\nAs long as Algebra and Geometry have been separated, their progress has been slow and their usages limited; but when these two sciences were reunited, they lent each other mutual strength and walked together with a rapid step towards perfection.\n\nCount Joseph-Louis de Lagrange (1795)\n\nWe know that Descartes’ fusion of algebra and geometry proved no less fruitful for one of these sciences than for the other. For, while on the one hand, geometers learned, through contact with analysis, to give their research a previously unknown generality, analysts, for their part, found powerful help in the images of geometry, both to discover their theorems and to state them in a simple and striking form.\n\nCamille Jordan, Essai sur la géométrie à \\(n\\) dimensions, 1875.\nThis paper introduced the notion of angles between flats, which is equivalent to the statistical technique of Canonical Correlation Analysis.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Postscript"
    ]
  },
  {
    "objectID": "landing_page.html#the-story",
    "href": "landing_page.html#the-story",
    "title": "Preface",
    "section": "The Story",
    "text": "The Story\n\nI have long embraced the belief that every course should be built around a story, a quest to answer certain burning questions.\nDavid Bressoud, mathvalues.org\n\n\n\nWhat is linear algebra really about? This is a great question. My attempt at an answer is this:\nOur shared experience of the world is in three dimensions. In that context humans have acquired innate and learned abilities to think about shapes and spatial relationships. Linear algebra asks: how would all that change if the number of dimensions was unspecified?\nIn that view, an enormously important contribution comes from an algebra in which the dimensionality of objects is unspecified so that concepts become generalized. Another important contribution comes from taking familiar three-dimensional notions and asking what we can say about them, and how we can reason about them, in arbitrary dimension.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "landing_page.html#teaching-approach",
    "href": "landing_page.html#teaching-approach",
    "title": "Preface",
    "section": "Teaching Approach",
    "text": "Teaching Approach\n\nFive-dimensional shapes are hard to visualize – but it doesn’t mean you can’t think about them. Thinking is really the same as seeing.\nWilliam Thurston\n\nThe rationale for the teaching approach used in this course is here. In brief:\nStudents learning Linear Algebra need to develop three modes of thinking. The first is algebraic thinking – how to correctly manipulate symbols in a consistent logical framework, for example to solve equations. The second is geometric thinking: learning to extend familiar two- and three-dimensional concepts to higher dimensions in a rigorous way. The third is computational thinking: understanding the relationship between abstract algebraic machinery and actual computations which arrive at the (hopefully) correct answer to a specific problem in an efficient way.\n\nIt’s in words that the magic is — Abracadabra, Open Sesame, and the rest — but the magic words in one story aren’t magical in the next. The real magic is to understand which words work, and when, and for what; the trick is to learn the trick.\nJohn Barth, Chimera\n\nEach mode of thinking provides a distinct, powerful way of understanding a problem, and so using the full power of linear algebra requires being able to switch between these modes with fluidity. However, these three modes of thinking are quite different, and often students are better at some modes than others. For example, here are three views of matrix-vector multiplication:\n\nJupyter notebooks – including the use of RISE for presentation, Python for computation, and jupyter books for reference – are an ideal teaching environment to take on this trimodal challenge. Hence the goal of these notes is to take advantage of the Jupyter toolchain to interweave these modes on a fine grain, frequently moving from one mode to the other, to constantly reinforce connections between ways of thinking about linear algebra.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "landing_page.html#format",
    "href": "landing_page.html#format",
    "title": "Preface",
    "section": "Format",
    "text": "Format\nThe notes are in the form of Jupyter notebooks. Demos and most figures are included as executable Python code. All course materials are in the github repository here.\nEach of the Chapters is based on a single notebook, and each forms the basis for one lecture (more or less).\nI hope you enjoy this course, whose goal is to prevent you from falling into this trap:\n\nAlgebra is the offer made by the devil to the mathematician. The devil says: I will give you this powerful machine, it will answer any question you like. All you need to do is give me your soul: give up geometry and you will have this marvelous machine.\nSir Michael Atiyah, 2002\n\nAnd to conclude, here is an anonymous course review from Fall 2022:\n\nI thought when the Professors were talking about how linear algebra is “beautiful,” they were exaggerating, but by the end of the course, I understood why this is true.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "L12MatrixFactorizations.html",
    "href": "L12MatrixFactorizations.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nJust as multiplication can be generalized from scalars to matrices, the notion of a factorization can also be generalized from scalars to matrices.\nA factorization of a matrix \\(A\\) is an equation that expresses \\(A\\) as a product of two or more matrices.\n\\[A = BC.\\]\n\nThe essential difference with what we have done so far is that we have been given factors (\\(B\\) and \\(C\\)) and then computed \\(A\\).\nIn a factorization problem, you are given \\(A\\), and you want to find \\(B\\) and \\(C\\) – that meet some conditions.\n\n\nThere are a number of reasons one may want to factor a matrix.\n\nRecasting \\(A\\) into a form that makes computing with \\(A\\) faster.\nRecasting \\(A\\) into a form that makes working with \\(A\\) easier.\nRecasting \\(A\\) into a form that exposes important properties of \\(A\\).\n\n\n\nToday we’ll work with one particular factorization that addesses the first case. Later one we’ll study factorizations that address the other two cases.\n\n\nThe factorization we will study is called the LU Factorization. It is worth studying in its own right, and because it introduces the idea of factorizations, which we will study again later on.",
    "crumbs": [
      "Matrix Factorizations"
    ]
  },
  {
    "objectID": "L12MatrixFactorizations.html#matrix-factorizations",
    "href": "L12MatrixFactorizations.html#matrix-factorizations",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nJust as multiplication can be generalized from scalars to matrices, the notion of a factorization can also be generalized from scalars to matrices.\nA factorization of a matrix \\(A\\) is an equation that expresses \\(A\\) as a product of two or more matrices.\n\\[A = BC.\\]\n\nThe essential difference with what we have done so far is that we have been given factors (\\(B\\) and \\(C\\)) and then computed \\(A\\).\nIn a factorization problem, you are given \\(A\\), and you want to find \\(B\\) and \\(C\\) – that meet some conditions.\n\n\nThere are a number of reasons one may want to factor a matrix.\n\nRecasting \\(A\\) into a form that makes computing with \\(A\\) faster.\nRecasting \\(A\\) into a form that makes working with \\(A\\) easier.\nRecasting \\(A\\) into a form that exposes important properties of \\(A\\).\n\n\n\nToday we’ll work with one particular factorization that addesses the first case. Later one we’ll study factorizations that address the other two cases.\n\n\nThe factorization we will study is called the LU Factorization. It is worth studying in its own right, and because it introduces the idea of factorizations, which we will study again later on.",
    "crumbs": [
      "Matrix Factorizations"
    ]
  },
  {
    "objectID": "L12MatrixFactorizations.html#a-rationale-for-the-lu-factorization",
    "href": "L12MatrixFactorizations.html#a-rationale-for-the-lu-factorization",
    "title": "Geometric Algorithms",
    "section": "A Rationale for the LU Factorization",
    "text": "A Rationale for the LU Factorization\n\nConsider the following problem. You are given \\(A \\in \\mathbb{R}^{n\\times n}\\) and \\(B \\in \\mathbb{R}^{n\\times p}\\).\nYou seek \\(X \\in \\mathbb{R}^{n\\times p}\\) such that:\n\\[ AX = B. \\]\nIn other words, instead of the usual \\(A\\mathbf{x} = \\mathbf{b}\\), now \\(X\\) and \\(B\\) are matrices.\n\n\nBy the rules of matrix multiplication, we can break this problem up.\nLet \\(X = [\\mathbf{x_1} \\mathbf{x_2} \\dots \\mathbf{x_p}],\\) and \\(B = [\\mathbf{b_1} \\mathbf{b_2} \\dots \\mathbf{b_p}]\\).\nThen:\n\\[A{\\bf x_1} = {\\bf b_1}\\] \\[A{\\bf x_2} = {\\bf b_2}\\] \\[\\dots\\] \\[A{\\bf x_p} = {\\bf b_p}\\]\n\n\nIn other words, there are \\(p\\) linear systems to solve.\nEach linear system is conceptually a separate problem.\nNote however that every linear system has the same \\(A\\) matrix.\n\n\nNaturally, you could solve these systems by first computing \\(A^{-1}\\) and then computing:\n\\[{\\bf x_1} = A^{-1}{\\bf b_1}\\] \\[{\\bf x_2} = A^{-1}{\\bf b_2}\\] \\[\\dots\\] \\[{\\bf x_p} = A^{-1}{\\bf b_p}\\]\nOr, more concisely:\n\\[ X = A^{-1}B \\]\n\nWhat is the computational cost of using the matrix inverse to solve \\(AX = B\\)?\nAs discussed earlier the operation count of matrix inversion is \\(\\sim 2n^3\\) flops\n… that is, three times as many as Gaussian Elimination.\nThis cost will dominate the process.\n\nAlternatively, we could perform Gaussian Elimination on each of the separate systems.\nThis is probably worse, because then we have to perform \\(\\sim p \\cdot \\frac{2}{3}n^3\\) flops.\nAssuming \\(p &gt; 3\\), using Gaussian Elimination on each system means doing more work than if we invert \\(A\\).\n\nWhat if we could solve all these systems while performing Gaussian Elimination only once?\nThat would be a win, as it would cut our running time by a factor of 3 compared to using the matrix inverse.\n\nThe LU factorization allows us to do exactly this.\n\n\nToday we will explore the LU factorization. We will see that LU factorization has a close connection to Gaussian Elimination.\nIn fact, I hope that when we are done, you will see Gaussian Elimination in a new way, namely:\n\nGaussian Elimination is really a matrix factorization!\n\n\n\nBefore we start to discuss the LU factorization, we need to introduce a powerful tool for performing factorizations, called elementary matrices.",
    "crumbs": [
      "Matrix Factorizations"
    ]
  },
  {
    "objectID": "L12MatrixFactorizations.html#elementary-matrices",
    "href": "L12MatrixFactorizations.html#elementary-matrices",
    "title": "Geometric Algorithms",
    "section": "Elementary Matrices",
    "text": "Elementary Matrices\n\nRecall from the first and second lectures that the row reduction process consists of repeated applications of elementary row operations:\n\nExchange two rows\nMultiply a row by a constant\nAdd a multiple of one row to another\n\n\n\nNow that we have much more theoretical machinery in our toolbox, we can make an important observation:\nEvery elementary row operation on \\(A\\) can be performed by multiplying \\(A\\) by a suitable matrix.\n\n\nThat is, an elementary row operation is a linear transformation!\n\nFurthermore, the matrices that implement elementary row operations are particularly simple. They are called elementary matrices.\n\nAn elementary matrix is one that is obtained by performing a single elementary row operation on the identity matrix.\n\nExample. Let\n\\[E_1 = \\left[\\begin{array}{rrr}1&0&0\\\\0&1&0\\\\-4&0&1\\end{array}\\right],\\;\\; E_2 = \\left[\\begin{array}{rrr}0&1&0\\\\1&0&0\\\\0&0&1\\end{array}\\right],\\;\\; E_3 = \\left[\\begin{array}{rrr}1&0&0\\\\0&1&0\\\\0&0&5\\end{array}\\right].\\]\nLet’s see what each matrix does to an arbitrary matrix \\(A = \\left[\\begin{array}{rrr}a&b&c\\\\d&e&f\\\\g&h&i\\end{array}\\right].\\)\n\n\\[ E_1A = \\left[\\begin{array}{rrr}a&b&c\\\\d&e&f\\\\g-4a&h-4b&i-4c\\end{array}\\right]. \\]\n\n\n\\[ E_2A = \\left[\\begin{array}{rrr}d&e&f\\\\a&b&c\\\\g&h&i\\end{array}\\right].\\]\n\n\n\\[ E_3A = \\left[\\begin{array}{rrr}a&b&c\\\\d&e&f\\\\5g&5h&5i\\end{array}\\right].\\]\n\n\nClearly, left-multiplication by \\(E_1\\) will add -4 times row 1 to row 3 (for any matrix \\(A\\)).\n\n\nFinding the Elementary Matrix\n\nIn fact, you can look at this as follows.\nAssume that some matrix \\(E\\) exists that implements the operation “add -4 times row 1 to row 3.”\n\n\nNow, for any matrix, \\(EI = E\\) by the definition of \\(I\\).\n\n\nBut note that this equation also says:\n“the matrix (\\(E\\)) that implements the operation ‘add -4 times row 1 to row 3’ is the one you get by performing this operation on \\(I\\)”\n\nThus we have the following:\nFact. If an elementary row operation is performed on an \\(m\\times n\\) matrix \\(A,\\) the resulting matrix can be written as \\(EA\\), where the \\(m\\times m\\) matrix \\(E\\) is created by performing the same row operation on \\(I_m\\).\n\nThis is actually a special case of a general rule we already know:\n\nto compute the standard matrix of a linear transformation, ask what that transformation does to the identity matrix.\n\n\nOne more thing: is an elementary matrix invertible?\nClearly, yes: any row reduction operation can be reversed by another (related) row reduction operation.\nSo every row reduction is an invertible linear transformation – so every elementary matrix is invertible.",
    "crumbs": [
      "Matrix Factorizations"
    ]
  },
  {
    "objectID": "L12MatrixFactorizations.html#the-lu-factorization",
    "href": "L12MatrixFactorizations.html#the-lu-factorization",
    "title": "Geometric Algorithms",
    "section": "The LU Factorization",
    "text": "The LU Factorization\n\nNow, we will introduce the factorization\n\\[ A = LU.\\]\nNote that we don’t need to consider only square matrices \\(A\\).\nLU decomposition (like Gaussian Elimination) works for a matrix \\(A\\) having any shape.\n\nAn LU factorization of \\(A\\) constructs two matrices that have this structure:\n\\[A = \\begin{array}{cc}\n\\left[\\begin{array}{cccc}1&0&0&0\\\\ *&1&0&0\\\\ *&*&1&0\\\\ *&*&*&1\\end{array}\\right]&\n\\left[\\begin{array}{ccccc}\\blacksquare&*&*&*&*\\\\0&\\blacksquare&*&*&*\\\\0&0&0&\\blacksquare&*\\\\0&0&0&0&0\\end{array}\\right]\\\\\nL&U\\\\\n\\end{array}\n\\]\nStars (\\(*\\)) denote arbitrary entries, and blocks (\\(\\blacksquare\\)) denote nonzero entries.\n\nThese two matrices each have a special structure.\n\n\nFirst of all, \\(U\\) is in row echelon form, and it has the same shape as \\(A\\).\nThis is the “upper” matrix (hence its name \\(U\\)).\n\n\nSecond, \\(L\\) is a lower triangular square matrix, and it has 1s on the diagonal.\nThis is called a unit lower triangular matrix (hence its name \\(L\\)).\n\nThe fact that \\(U\\) is in row echelon form may suggest to you (correctly!) that we could get it from \\(A\\) by a sequence of row operations.\nFor now, let us suppose that the row reductions that convert \\(A\\) to \\(U\\) only add a multiple of one row to another row below it.\n\nNow, if you consider an elementary matrix that implements such a row reduction, you will see that it will have 1s on the diagonal, and an additional entry somewhere below the diagonal.\n\n\nFor example, recall \\(E_1\\) above:\n\\[E_1 = \\left[\\begin{array}{rrr}1&0&0\\\\0&1&0\\\\-4&0&1\\end{array}\\right]\\]\n\\[ E_1A = \\left[\\begin{array}{rrr}a&b&c\\\\d&e&f\\\\g-4a&h-4b&i-4c\\end{array}\\right]. \\]\n\n\nIn other words, this sort of elementary matrix would actually be a unit lower triangular matrix.\n\nSo if there is a sequence of row operations that convert \\(A\\) to \\(U\\), then there is a sequence of unit lower triangular elementary matrices \\(E_1, E_2, \\dots, E_p\\) such that\n\\[E_p\\cdots E_2 E_1A = U.\\]\n\nRemember the order that the linear transformations are occurring. It is the same as:\n\\[(E_p (\\cdots (E_2 (E_1A)))) = U.\\]\nBut we can also think of this product as:\n\\[(E_p \\cdots E_1) A = U.\\]\n\n\nNow, we know that all elementary matrices are invertible, and the product of invertible matrices is invertible, so:\n\\[A = (E_p\\cdots E_1)^{-1} U.\\]\nIf we define\n\\[L = (E_p\\cdots E_1)^{-1}\\]\nThen\n\\[ A = LU.\\]\n\n\nIt’s not hard to show that the product of unit lower triangular matrices is unit lower triangular.\nIt’s also true that the inverse of a unit lower triangular matrix is unit lower triangular.\n\n\nSo we can conclude that \\(L,\\) as constructed from \\((E_p\\cdots E_1)^{-1}\\), is unit lower triangular.\nHence, we have defined the LU decomposition based on Gaussian Elimination.\n\nSo: we have rewritten the Gaussian Elimination of \\(A\\) as:\n\\[U = L^{-1}A.\\]\nWe’ve shown that \\(U\\) is in row echelon form, and was obtained from \\(A\\) by a sequence of elementary row operations.\nAnd we’ve shown that the \\(L\\) so defined is unit lower triangular.\n\nLet’s take stock of what this all means: the LU decomposition is a way of capturing the application of Gaussian Elimination to \\(A\\).\nIt incorporates both the process of performing Gaussian Elimination, and the result:\n\\(L^{-1}\\) captures the row reductions that transform \\(A\\) to row echelon form.\n\\(U\\) is the row echelon form of \\(A\\).\n\n\nNote however that we have assumed that the Gaussian Elimination procedure only used certain row reductions – in particular, no row interchanges.\n(We’ll deal with row interchanges later.)\n\n\nFinding \\(L\\)\n\nRecall that the motivation for developing the LU decomposition is that it is more efficient than matrix inversion.\nSo we don’t want to have to invert \\(L^{-1}\\) in the standard way in order to find \\(L\\).\n\n\nThere is an algorithm that does this, and using it one can invert \\(L^{-1}\\) efficiently.\nThe basic idea is that \\(L^{-1}\\) is a sequence of elementary row operations, and the inverse of \\(L^{-1}\\) is the matrix \\(L\\) that gives \\(L^{-1}L = I.\\)\nSo if we examine the sequence of elementary row operations, we can efficiently determine what \\(L\\) will give \\(I\\) under those row operations.\n\n\nThis gives the following algorithm for LU factorization:\n\nReduce \\(A\\) to an echelon form \\(U\\) by a sequence of row replacement operations, if possible.\nPlace entries in \\(L\\) such that the same sequence of row operations reduces \\(L\\) to \\(I\\).\n\n\n\nHere is the key observation: the step of reducing \\(A\\) to echelon form is simply Gaussian Elimination. If the second step can be done efficiently, then the whole LU factorization doesn’t take any longer than Gaussian Elimination itself.\nThe fact is that constructing \\(L\\) can be done efficiently by a simple modification of Gaussian Elimination.\nThis leads to the important conclusion: the operation count of LU decomposition is only \\(\\sim \\frac{2}{3}n^3.\\)",
    "crumbs": [
      "Matrix Factorizations"
    ]
  },
  {
    "objectID": "L12MatrixFactorizations.html#using-the-lu-factorization",
    "href": "L12MatrixFactorizations.html#using-the-lu-factorization",
    "title": "Geometric Algorithms",
    "section": "Using the LU Factorization",
    "text": "Using the LU Factorization\nLet’s return to the motivation for developing the LU factorization.\nWe’ve seen that performing LU decomposition is essentially equivalent to performing Gaussian Elimination, and as such, it doesn’t take time much longer than Gaussian Elimination.\n\nNow we want to show that, using the LU decomposition, that the system \\(A{\\bf x} = {\\bf b}\\) can be solved for any \\({\\bf b}\\) in time that is proportional to \\(n^2\\).\n\n\nInformally, what we are going to do is to use \\(L\\) to do a special, very efficient version of the forward step of Gaussian Elimination, and then use \\(U\\) in the usual way to do backsubstitution.\n\n\nWe can write these two steps concisely as follows:\nWhen \\(A = LU\\), the equation \\(A{\\bf x} = {\\bf b}\\) can be written as \\(L(U{\\bf x}) = {\\bf b}.\\)\nLet’s take this apart, and write \\({\\bf y}\\) for \\(U{\\bf x}\\). Then we can find \\({\\bf x}\\) by solving the pair of equations:\n\\[L{\\bf y} = {\\bf b},\\] \\[U{\\bf x} = {\\bf y}.\\]\n\n\nThe idea is that we first solve \\(L{\\bf y} = {\\bf b}\\) for \\({\\bf y},\\) then solve \\(U{\\bf x} = {\\bf y}\\) for \\({\\bf x}.\\)\nIn a sense, this corresponds to first performing the forward step of Gaussian Elimination (but in a specially streamlined, efficient way) and then performing the backwards (backsubstitution) step in the usual (efficient) fashion.\n\n\n\nImage credit: Linear Algebra and its Applications, by David C. Lay\n\n\n\n\nThe key observation: each equation is fast to solve because \\(L\\) and \\(U\\) are each triangular.\n\nExample.\nGiven the following LU decomposition of \\(A\\):\n\\[A = \\left[\\begin{array}{rrrr}3&-7&-2&2\\\\-3&5&1&0\\\\6&-4&0&-5\\\\-9&5&-5&12\\end{array}\\right] =\n\\left[\\begin{array}{rrrr}1&0&0&0\\\\-1&1&0&0\\\\2&-5&1&0\\\\-3&8&3&1\\end{array}\\right]\n\\left[\\begin{array}{rrrr}3&-7&-2&2\\\\0&-2&-1&2\\\\0&0&-1&1\\\\0&0&0&-1\\end{array}\\right] = LU\\]\n\nUse this LU factorization of \\(A\\) to solve \\(A{\\bf x} = {\\bf b}\\), where \\({\\bf b} = \\left[\\begin{array}{r}-9\\\\5\\\\7\\\\11\\end{array}\\right].\\)\n\n\nSolution. To solve \\(L{\\bf y} = {\\bf b},\\) note that the arithmetic takes place only in the augmented column (column 5). The zeros below each pivot in \\(L\\) are created automatically by the choice of row operations.\n\\[[L\\;\\; {\\bf b}] = \\left[\\begin{array}{rrrrr}1&0&0&0&-9\\\\-1&1&0&0&5\\\\2&-5&1&0&7\\\\-3&8&3&1&11\\end{array}\\right] \\sim\n\\left[\\begin{array}{rrrrr}1&0&0&0&-9\\\\0&1&0&0&-4\\\\0&0&1&0&5\\\\0&0&0&1&1\\end{array}\\right] = [I\\;{\\bf y}].\\]\n\n\nNext, for \\(U{\\bf x} = {\\bf y}\\) (the “backward” phase) the row reduction is again streamlined:\n\\[[U\\;\\;{\\bf y}] =  \\left[\\begin{array}{rrrrr}3&-7&-2&2&-9\\\\0&-2&-1&2&-4\\\\0&0&-1&1&5\\\\0&0&0&-1&1\\end{array}\\right] \\sim\n\\left[\\begin{array}{rrrrr}1&0&0&0&3\\\\0&1&0&0&4\\\\0&0&1&0&-6\\\\0&0&0&1&-1\\end{array}\\right].\\]\n\nOperation Count Analysis.\nBoth the forward and backward phases of solving a system as \\(L(U{\\bf x}) = {\\bf b}\\) have flop counts of \\(\\sim 2n^2\\).\nTherefore to solve \\(A\\mathbf{x} = \\mathbf{b}\\) using \\(LU\\mathbf{x} = \\mathbf{b}\\), the dominating step is doing the factorization \\(A = LU\\).\nWe’ve seen that the factorization step is essentially Gaussian Elimination, and therefore requires \\(\\sim\\frac{2}{3}n^3\\) flops.\n\nNow, to return to our original problem.\nTo solve\n\\[AX = B\\]\nwhere \\(A\\) is \\(n\\times n\\), and both \\(X\\) and \\(B\\) have \\(p\\) columns,\n\nBy matrix inversion of \\(A\\) requires \\(\\sim 2n^3 + 2p n^2\\) flops\nBy LU factorization of \\(A\\) requires \\(\\sim \\frac{2}{3}n^3 + 2p n^2\\) flops\n\n\n\nHence we have found that by using the LU decomposition, one can solve a series of systems all involving the same \\(A\\) in \\(\\sim\\frac{2}{3}n^3\\) flops,\nwhile doing Gaussian Elimination would require \\(\\sim\\frac{2}{3}n^3\\) flops for each system,\nand using the matrix inverse would require \\(\\sim2n^3\\) flops to invert the matrix.",
    "crumbs": [
      "Matrix Factorizations"
    ]
  },
  {
    "objectID": "L12MatrixFactorizations.html#pivoting",
    "href": "L12MatrixFactorizations.html#pivoting",
    "title": "Geometric Algorithms",
    "section": "Pivoting",
    "text": "Pivoting\n\nUp until now we have assumed that the Gaussian Elimination we used to define \\(U\\) only involves adding a multiple of a row to a row below it.\n\n\nHowever, in real situations, we sometime need to exchange two rows.\nOne reason is, as we know, that if the current row has a zero in the pivot position, we need to exchange it with a row that does not have a zero in the pivot position.\nBut there is another, more subtle reason having to do with numerical accuracy.\n\n\nWe make the following observation: in general, we would like to avoid dividing by a small number.\nHere is why. Consider the problem of computing \\(a/b\\) where \\(a\\) and \\(b\\) are scalars.\n\n\nLet’s say there is some small error in the value of \\(a\\), call it \\(\\epsilon.\\)\nThat means that what we are really computing is \\((a+\\epsilon)/b = a/b + \\epsilon/b\\).\nNote that \\(a/b\\) is the correct value, but that what we compute is off by \\(\\epsilon/b\\). Now, if \\(b\\) is a very small number, then the error in the result (\\(\\epsilon/b\\)) will be large.\n\n\nHence we would like to avoid dividing by small numbers whenever possible.\nNow: note that in performing Gaussian Elimination, we divide each row by the value of its pivot.\nWhat this suggests is that we would like to avoid having small pivots.\n\n\nThere is a simple way to address this. In processing any particular row, we can avoid having a small pivot by interchanging the current row with one of those below it.\n\n\nWe would like to exchange the current row with the row that has the largest absolute value of its pivot. This algorithmic technique is called “partial pivoting.”\n\n\nNow, a row interchange is an elementary row operation, and can be implemented by an elementary matrix. This elementary matrix is the identity with its corresponding rows interchanged.\nAn elementary matrix that exchanges rows is called a permutation matrix. The product of permutation matrices is a permutation matrix.\nHence, the net result of all the partial pivoting done during Gaussian Elimination can be expressed in a single permutation matrix \\(P\\).\n\n\nThis means that the final factorization of \\(A\\) is:\n\\[A = P L U.\\]\n\n\nYou can read this equation two ways:\n\n\\(A\\) is the product of a unit lower triangular matrix \\(L\\) and an echelon form matrix \\(U\\), and the rows of their product have been reordered according to the permulation \\(P\\). This is \\(A = P(LU).\\)\n\\(A\\) is the product of a permuted lower triangular matrix \\(PL\\) and an echelon form matrix \\(U\\). This is \\(A = (PL)U.\\)",
    "crumbs": [
      "Matrix Factorizations"
    ]
  },
  {
    "objectID": "L03RowReductions.html",
    "href": "L03RowReductions.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\nIn the last lecture we described a method for solving linear systems, but our description was somewhat informal. Today we’ll formally define Gaussian Elimination , sometimes called Gauss-Jordan Elimination.\n\n\nBased on Bretscher, Linear Algebra , pp 17-18, and the Wikipedia article on Gauss.\nGood additional reading on Gauss’s use of Least Squares and determination of the orbit of Ceres is here.\nCarl Gauss lived from 1777 to 1855, in Germany. He is often called “the greatest mathematician since antiquity.”\nWhen Gauss was around 17 years old, he developed a method for working with inconsistent linear systems, called the method of least squares. A few years later (at the advanced age of 24) he turned his attention to a particular problem in astronomy. In 1801 the Sicilian astronomer Piazzi discovered a (dwarf) planet, which he named Ceres, in honor of the patron goddess of Sicily. Piazzi took measurements of Ceres’ position for 40 nights, but then lost track of it when it passed behind the sun. Piazzi had only tracked Ceres through about 3 degrees of sky. Gauss however then succeeded in calculating the orbit of Ceres, even though the task seemed hopeless on the basis of so few observations. His computations were so accurate that the astronomer Olbers located Ceres again later the same year.\nIn the course of his computations Gauss had to solve systems of 17 linear equations. Since Gauss at first refused to reveal the methods that led to this amazing accomplishment, some even accused him of sorcery. Eight years later, in 1809, Gauss revealed his methods of orbit computation in his book Theoria Motus Corporum Coelestium.\nAlthough Gauss invented this method (which Jordan then popularized), it was a reinvention. As we mentioned in a previous lecture, linear systems were being solved by a similar method in China 2,000 years earlier.",
    "crumbs": [
      "Gaussian Elimination"
    ]
  },
  {
    "objectID": "L03RowReductions.html#gaussian-elimination",
    "href": "L03RowReductions.html#gaussian-elimination",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\nIn the last lecture we described a method for solving linear systems, but our description was somewhat informal. Today we’ll formally define Gaussian Elimination , sometimes called Gauss-Jordan Elimination.\n\n\nBased on Bretscher, Linear Algebra , pp 17-18, and the Wikipedia article on Gauss.\nGood additional reading on Gauss’s use of Least Squares and determination of the orbit of Ceres is here.\nCarl Gauss lived from 1777 to 1855, in Germany. He is often called “the greatest mathematician since antiquity.”\nWhen Gauss was around 17 years old, he developed a method for working with inconsistent linear systems, called the method of least squares. A few years later (at the advanced age of 24) he turned his attention to a particular problem in astronomy. In 1801 the Sicilian astronomer Piazzi discovered a (dwarf) planet, which he named Ceres, in honor of the patron goddess of Sicily. Piazzi took measurements of Ceres’ position for 40 nights, but then lost track of it when it passed behind the sun. Piazzi had only tracked Ceres through about 3 degrees of sky. Gauss however then succeeded in calculating the orbit of Ceres, even though the task seemed hopeless on the basis of so few observations. His computations were so accurate that the astronomer Olbers located Ceres again later the same year.\nIn the course of his computations Gauss had to solve systems of 17 linear equations. Since Gauss at first refused to reveal the methods that led to this amazing accomplishment, some even accused him of sorcery. Eight years later, in 1809, Gauss revealed his methods of orbit computation in his book Theoria Motus Corporum Coelestium.\nAlthough Gauss invented this method (which Jordan then popularized), it was a reinvention. As we mentioned in a previous lecture, linear systems were being solved by a similar method in China 2,000 years earlier.",
    "crumbs": [
      "Gaussian Elimination"
    ]
  },
  {
    "objectID": "L03RowReductions.html#echelon-forms",
    "href": "L03RowReductions.html#echelon-forms",
    "title": "Geometric Algorithms",
    "section": "Echelon Forms",
    "text": "Echelon Forms\nAn echelon is a term used in the military to decribe an arrangement of rows (of troops, or ships, etc) in which each successive row extends further than the row in front of it.\nAt the end of a previous lecture, we had constructed this matrix:\n\\[\n\\left[\\begin{array}{rrrr}\n2&-3&2&1\\\\\n0&1&-4&8\\\\\n0&0&0&15\n\\end{array}\\right]\n\\]\nA leading entry is the first nonzero element in a row. In this matrix, the leading entries have values 2, 1, and 15.\n\nDefinition: A matrix is in echelon form (or row echelon form) if it has the following three properties:\n\n\nAll nonzero rows are above any rows of all zeros.\nEach leading entry of a row is in a column to the right of the leading entry of the row above it.\nAll entries in a column below a leading entry are zeros.\n\n\n\n\nFor example:\n\\[\n\\left[\\begin{array}{cccccccccc}\n0&\\blacksquare&*&*&*&*&*&*&*&*\\\\\n0&0&0&\\blacksquare&*&*&*&*&*&*\\\\\n0&0&0&0&\\blacksquare&*&*&*&*&*\\\\\n0&0&0&0&0&\\blacksquare&*&*&*&*\\\\\n0&0&0&0&0&0&0&0&\\blacksquare&*\\\\\n0&0&0&0&0&0&0&0&0&0\\\\\n\\end{array}\\right]\n\\]\nIn this diagram, the \\(\\blacksquare\\)s are nonzero, and the \\(*\\)s can be any value.\nThis definition is a refinement of the notion of a triangular matrix (or system) that was introduced in the previous lecture.\nThe goal of the first step of Gaussian elimination is to convert the augmented matrix into echelon form.\n\nDefinition: A matrix is in reduced echelon form (or reduced row echelon form) if it is in echelon form, and furthermore:\n\n\nThe leading entry in each nonzero row is 1.\nEach leading 1 is the only nonzero entry in its column.\n\n\n\nFor example:\n\\[\n\\left[\\begin{array}{cccccccccc}\n0&\\fbox{1}&*&0&0&0&*&*&0&*\\\\\n0&0&0&\\fbox{1}&0&0&*&*&0&*\\\\\n0&0&0&0&\\fbox{1}&0&*&*&0&*\\\\\n0&0&0&0&0&\\fbox{1}&*&*&0&*\\\\\n0&0&0&0&0&0&0&0&\\fbox{1}&*\\\\\n0&0&0&0&0&0&0&0&0&0\\\\\n\\end{array}\n\\right]\n\\]\nThe goal of the second step of Gaussian elimination is to convert the matrix into reduced echelon form.",
    "crumbs": [
      "Gaussian Elimination"
    ]
  },
  {
    "objectID": "L03RowReductions.html#properties-of-echelon-forms",
    "href": "L03RowReductions.html#properties-of-echelon-forms",
    "title": "Geometric Algorithms",
    "section": "Properties of Echelon Forms",
    "text": "Properties of Echelon Forms\nAny matrix may be row reduced to an echelon form.\nAn echelon form of a matrix is not unique; for any given matrix, depending on the sequence of row operations, different echelon forms may be produced .\n\nHowever, the reduced echelon form of a matrix is unique.\nTheorem: Each matrix is equivalent to one and only one reduced echelon matrix.\n\n\nThe positions of the leading entries of an echelon matrix and its reduced form are the same. So, by the Theorem, the leading entries of any echelon form of a given matrix are in the same positions. Hence, we give those positions a name: pivots.\nDefinition: A pivot position in a matrix \\(A\\) is the position of a leading 1 in the reduced echelon form of \\(A\\).",
    "crumbs": [
      "Gaussian Elimination"
    ]
  },
  {
    "objectID": "L03RowReductions.html#gaussian-elimination-the-algorithm",
    "href": "L03RowReductions.html#gaussian-elimination-the-algorithm",
    "title": "Geometric Algorithms",
    "section": "Gaussian Elimination: The Algorithm",
    "text": "Gaussian Elimination: The Algorithm\nAs suggested by the last lecture, Gaussian Elimination has two stages. Given an augmented matrix \\(A\\) representing a linear system:\n\nFirst, convert \\(A\\) to one of its echelon forms, say \\(U\\).\nThen, convert \\(U\\) to \\(A\\)’s reduced row echelon form.\n\nEach stage iterates over the rows of \\(A\\), starting with the first row.\nRow Reduction Operations\nBefore stating the algorithm, let’s recall the set of operations that we can perform on rows without changing the solution set:\n\nMultiply a row by a nonzero value.\nAdd a multiple of a row to another row.\nSwap two rows.\n\nGaussian Elimination, Stage 1 (Elimination):\nInput: matrix \\(A\\).\nWe will use \\(i\\) to denote the index of the current row. To start, let \\(i = 1\\). Repeat the following steps:\n\n\nLet \\(j\\) be the position of the leftmost nonzero value in row \\(i\\) or any row below it. If there is no such position, stop.\nIf the value in the \\(j\\)th position in row \\(i\\) is zero, swap this row with a row below it to make the \\(j\\)th position nonzero. This creates a pivot in position \\((i,j)\\).\nUse row reduction operations to create zeros in all positions below the pivot.\nIf any operation creates a row that is all zeros except the last element, the system is inconsistent; stop.\nLet \\(i = i + 1.\\) If \\(i\\) equals the number of rows in \\(A\\), stop. Otherwise start at the first bullet again.\n\n\n\nThe output of this stage is an echelon form of \\(A\\).\n\n\nGaussian Elimination, Stage 2 (Backsubstitution):\nInput: an echelon form of \\(A\\).\nWe start at the top again, so let \\(i = 1\\). Repeat the following steps:\n\n\nIf row \\(i\\) is all zeros, or if \\(i\\) exceeds the number of rows in \\(A\\), stop.\nIf row \\(i\\) has a nonzero pivot value, divide row \\(i\\) by its pivot value. This creates a 1 in the pivot position.\nUse row reduction operations to create zeros in all positions above the pivot.\n\nLet \\(i = i+1\\) and either stop or repeat from the first bullet.\n\n\n\n\nThe output of this stage is the reduced echelon form of \\(A\\).",
    "crumbs": [
      "Gaussian Elimination"
    ]
  },
  {
    "objectID": "L03RowReductions.html#example",
    "href": "L03RowReductions.html#example",
    "title": "Geometric Algorithms",
    "section": "Example",
    "text": "Example\nThe Gaussian Elimination process we’ve described is essentially equivalent to the process described in the first lecture, so we won’t do a lengthy example. Let the input matrix \\(A\\) be\n\\[\\left[\\begin{array}{rrrrrr}\n0 & 3 & -6 & 6 & 4 & -5\\\\\n3 & -7 & 8 & -5 & 8 & 9\\\\\n3 & -9 & 12 & -9 & 6 & 15\n\\end{array}\\right]\\]\n\nStage 1\nStart with the first row (\\(i = 1\\)). The leftmost nonzero in row 1 and below is in position 1. But since it’s not in row 1, we need to swap. We’ll swap rows 1 and 3 (we could have swapped 1 and 2).\n\\[\\left[\\begin{array}{rrrrrr}\n3 & -9 & 12 & -9 & 6 & 15\\\\\n3 & -7 & 8 & -5 & 8 & 9\\\\\n0 & 3 & -6 & 6 & 4 & -5\n\\end{array}\\right]\\]\n\n\nThe pivot is shown in a box. Use row reduction operations to create zeros below the pivot. In this case, that means subtracting row 1 from row 2.\n\\[\\left[\\begin{array}{rrrrrr}\n\\fbox{3} & -9 & 12 & -9 & 6 & 15\\\\\n0 & 2 & -4 & 4 & 2 & -6\\\\\n0 & 3 & -6 & 6 & 4 & -5\n\\end{array}\\right]\\]\n\n\nNow \\(i = 2\\). The pivot is boxed (no need to do any swaps). Use row reduction to create zeros below the pivot. To do so we subtract \\(3/2\\) times row 2 from row 3.\n\\[\\left[\\begin{array}{rrrrrr}\n3 & -9 & 12 & -9 & 6 & 15\\\\\n0 & \\fbox{2} & -4 & 4 & 2 & -6\\\\\n0 & 0 & 0 & 0 & 1 & 4\n\\end{array}\\right]\\]\n\n\nNow \\(i = 3\\). Since it is the last row, we are done with Stage 1. The pivots are marked:\n\\[\\left[\\begin{array}{rrrrrr}\n\\fbox{3} & -9 & 12 & -9 & 6 & 15\\\\\n0 & \\fbox{2} & -4 & 4 & 2 & -6\\\\\n0 & 0 & 0 & 0 & \\fbox{1} & 4\n\\end{array}\\right]\\]\n\n\nStage 2\nStarting again with the first row (\\(i = 1\\)). Divide row 1 by its pivot.\n\\[\\left[\\begin{array}{rrrrrr}\n\\fbox{1} & -3 & 4 & -3 & 2 & 5\\\\\n0 & 2 & -4 & 4 & 2 & -6\\\\\n0 & 0 & 0 & 0 & 1 & 4\n\\end{array}\\right]\\]\n\n\nMoving to the next row (\\(i = 2\\)). Divide row 2 by its pivot.\n\\[\\left[\\begin{array}{rrrrrr}\n1 & -3 & 4 & -3 & 2 & 5\\\\\n0 & \\fbox{1} & -2 & 2 & 1 & -3\\\\\n0 & 0 & 0 & 0 & 1 & 4\n\\end{array}\\right]\\]\n\n\nAnd use row reduction operations to create zeros in all elements above the pivot. In this case, that means adding 3 times row 2 to row 1.\n\\[\\left[\\begin{array}{rrrrrr}\n1 & 0 & -2 & 3 & 5 & -4\\\\\n0 & \\fbox{1} & -2 & 2 & 1 & -3\\\\\n0 & 0 & 0 & 0 & 1 & 4\n\\end{array}\\right]\\]\n\n\nMoving to the next row (\\(i = 3\\)). The pivot is already 1. So we subtract row 3 from row 2, and subtract 5 times row 3 from row 1.\n\\[\\left[\\begin{array}{rrrrrr}\n1 & 0 & -2 & 3 & 0 & -24\\\\\n0 & 1 & -2 & 2 & 0 & -7\\\\\n0 & 0 & 0 & 0 & \\fbox{1} & 4\n\\end{array}\\right]\\]\nAnd we are done.\n\n\nA Note about Inconsistent Systems\n\nNotice that Gaussian Elimination always produces an echelon form for any matrix.\nNow, what kinds of echelon forms are inconsistent?\n\n\nThe only way for an echelon form to be inconsistent, is if it contains a row corresponding to the equation \\(0 = k\\) for some nonzero \\(k\\).\nSince row reductions preserve solution sets, this means that row reducing an inconsistent system will always lead to the equation \\(0 = k\\) for some nonzero \\(k\\).",
    "crumbs": [
      "Gaussian Elimination"
    ]
  },
  {
    "objectID": "L03RowReductions.html#how-many-operations-does-gaussian-elimination-require",
    "href": "L03RowReductions.html#how-many-operations-does-gaussian-elimination-require",
    "title": "Geometric Algorithms",
    "section": "How Many Operations does Gaussian Elimination Require?",
    "text": "How Many Operations does Gaussian Elimination Require?\n\nGaussian Elimination is the first algorithm we have discussed in the course, and as with any algorithm, it is important to assess its cost.\nFirst, however we need to talk about how to assess cost when we are talking about numerical computations.\n\n\nThe Cost of a Numerical Computation\n\nIf you have studied algorithms before, you will have learned about “big-O” notation.\n\n\nWe will NOT use big-O notation in this course!\n\n\nThe reason is that big-O notation does not specify the values of constants.\nHowever, when studying numerical algorithms, constants matter!\nIt will matter to us whether an algorithm takes \\(n\\) versus \\(2n\\) time!\n\nHere is how to assess the computational cost of a numerical algorithm:\nFirst, we need to define our units:\n\nWe will count the number of additions, multiplications, divisions, subtractions, or square roots.\n\n\nThese five operations are required to be implemented by IEEE-754 and so more-or-less have unit cost. That is, in a modern processor each one requires only a single instruction.\nThese five operations are performed on floating point numbers, so they are called flops (floating point operations).\n\nNext, we define how we count operations:\n\nWe will be concerned with the highest-powered term in the expression that counts flops.\n\nThis tells us how the flop count scales for very large inputs.\n\nFor example, let’s say for a problem with input size \\(n\\), an algorithm has flop count\n\\[12n^2 + 3n + 2.\\]\nThen the cost of the algorithm is \\(12n^2\\).\n\n\nThis is a good approximation because \\(12n^2\\) is asymptotically equivalent to the exact flop count:\n\\[ \\lim_{n\\rightarrow\\infty} \\frac{12n^2 + 3n + 2}{12n^2} = 1. \\]\n\n\nWe will use the symbol \\(\\sim\\) to denote this relationship.\nSo we would say that this algorithm has flop count \\(\\sim 12n^2\\).\n\n\n\nThe Cost of Gaussian Elimination\n\nNow, let’s assess the computational cost required to solve a system of \\(n\\) equations in \\(n\\) unknowns using Gaussian Elimination.\n\n\nFor \\(n\\) equations in \\(n\\) unknowns, \\(A\\) is an \\(n \\times (n+1)\\) matrix.\nWe can summarize stage 1 of Gaussian Elimination as, in the worst case:\n\nFor each row \\(i\\) of \\(A\\):\n\nadd a multiple of row \\(i\\) to all rows below it\n\n\n\n\n\n\n\nFor row 1, this becomes \\((n-1) \\cdot 2(n+1)\\) flops.\nThat is, there are \\(n-1\\) rows below row 1, each of those has \\(n+1\\) elements, and each element requires one multiplication and one addition. This is \\(2n^2-2\\) flops for row 1.\n\nWhen operating on row \\(i\\), there are \\(k = n - i + 1\\) unknowns and so there are \\(2k^2 - 2\\) flops required to process the rows below row \\(i\\).\n\n\n\nSo we can see that \\(k\\) ranges from \\(n\\) down to \\(1\\).\nSo, the number of operations required for the Elimination stage is:\n\\[\n\\begin{array}{rcl} \\sum_{k=1}^n (2k^2 - 2) &=& \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\\\\n\\end{array}\n\\]\n\n\\[\n\\begin{array}{rcl}\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;&& 2 \\left(\\sum_{k=1}^n k^2 - \\sum_{k=1}^n 1\\right)\\\\\n&=& 2 \\left(\\frac{n(n+1)(2n+1)}{6} - n\\right)\\\\\n&=& \\frac{2}{3} n^3 + n^2 - \\frac{5}{3} n\n\\end{array}\n\\]\nThe second step above is based on known formulas.\n\n\nWhen \\(n\\) is large, this expression is dominated by \\(\\frac{2}{3} n^3\\).\nThat is,\n\\[ \\lim_{n\\rightarrow\\infty} \\frac{\\frac{2}{3} n^3 + n^2 - \\frac{5}{3} n}{\\frac{2}{3} n^3} = 1 \\]\n\n\n\nWhy is the second stage \\(\\mathbf{O(n^2)}\\)? In the second stage of GE we do a multiplication and an addition for each zero we create.\nIn the worst case, the last column requires creating \\(n-1\\) zeros, the second-last column requires \\(n-2\\) zeros, etc, down to 1.\nSo the number of flops in the second stage is:\n\\[ \\sum_{i=1}^{n-1} 2i \\]\nwhich is equal to\n\\[ 2 \\cdot \\frac{(n-1)n}{2} = n^2 -n \\]\nwhich is \\(\\sim n^2.\\)\n\nNow, the second stage of GE only requires \\(\\sim n^2\\) flops (as we show in the margin),\n… so the whole GE algorithm is dominated by the \\(\\frac{2}{3} n^3\\) flops in the first stage.\n\nSo, we find that:\n\nThe Elimination stage is \\(\\sim \\frac{2}{3} n^3\\).\nThe Backsubstitution stage is \\(\\sim n^2\\).\n\nThus we say that the flop count of Gaussian Elimination is \\(\\sim \\frac{2}{3} n^3\\).",
    "crumbs": [
      "Gaussian Elimination"
    ]
  },
  {
    "objectID": "L03RowReductions.html#using-the-echelon-form",
    "href": "L03RowReductions.html#using-the-echelon-form",
    "title": "Geometric Algorithms",
    "section": "Using the Echelon Form",
    "text": "Using the Echelon Form\nReturning to the fundamental questions about a linear system:\n\nconsistency and\nuniqueness,\n\nwe’ve discussed how the echelon form exposes consistency (by creating an equation \\(0 = k\\) for some nonzero \\(k\\)).\nNow we can also discuss uniqueness.\n\nLet’s assume that the augmented matrix of a system has been transformed into the equivalent reduced echelon form:\n\\[\n\\left[\\begin{array}{rrrr}\n1&0&-5&1\\\\\n0&1&1&4\\\\\n0&0&0&0\n\\end{array}\\right]\n\\]\nThis system is consistent. Is there a unique solution?\n\n\nThe associated system of equations is\n\\[\n\\begin{array}{rrrrr}\nx_1 & & -5x_3 &=& 1\\\\\n&x_2 & +x_3 &=& 4\\\\\n&&0&=&0\\\\\n\\end{array}\n\\]\nVariables \\(x_1\\) and \\(x_2\\) correspond to pivot columns.\nVariables whose column has a pivot are called basic variables.\nVariables without a pivot in their column are called free variables.\nHere, \\(x_3\\) is a free variable, and \\(x_1\\) and \\(x_2\\) are basic variables.\n\n\nWhenever a system is consistent, the solution set can be described explicitly by solving the reduced system of equations for the basic variables in terms of the free variables.\nThis operation is possible because the reduced echelon form places each basic variable in one and only one equation.\n\n\nIn the example, solve the first and second equations for \\(x_1\\) and \\(x_2\\). Ignore the third equation; it offers no restriction on the variables.\n\n\nSo the solution set is:\n\\[\\begin{array}{rl}\nx_1 &= 1 + 5x_3\\\\\nx_2 &= 4 - x_3\\\\\nx_3 &\\text{is free}\n\\end{array}\n\\]\n“\\(x_3\\) is free” means you can choose any value for \\(x_3\\).\nIn other words, there are an inifinite set of solutions to this linear system.\nYou may choose any value of \\(x_3\\) that you like, and for each choice of \\(x_3\\), you can construct a solution for the system.\nFor instance,\n\nwhen \\(x_3 = 0\\), the solution is \\((1,4,0)\\);\nwhen \\(x_3 = 1,\\) the solution is \\((6,3,1)\\);\nand so forth.\n\n\n\nThis way of writing the solution set is called a parametric description.\nThe free variables act as parameters.\nSo: solving a system amounts to either:\n\nfinding a parametric description of the solution set, or\ndetermining that the solution set is empty.",
    "crumbs": [
      "Gaussian Elimination"
    ]
  },
  {
    "objectID": "L03RowReductions.html#geometric-interpretation",
    "href": "L03RowReductions.html#geometric-interpretation",
    "title": "Geometric Algorithms",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\nLet’s consider the geometric interpretation of a system with a free variable.\nConsider again this reduced echelon form:\n\\[\n\\left[\\begin{array}{rrrr}\n1&0&-5&1\\\\\n0&1&1&4\\\\\n0&0&0&0\n\\end{array}\\right]\n\\]\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nSince there is a row of zeros in the reduced echelon form matrix, there are only two equations (rather than three) that determine the solution set.\nTherefore, the equations\n\\[\\begin{array}{rl}\nx_1 &= 1 + 5x_3\\\\\nx_2 &= 4 - x_3\\\\\nx_3 &\\text{is free}\n\\end{array}\n\\]\ndescribe a line in 3-space.\nPut simply, the solution set is one-dimensional (a line) because there is one free variable.",
    "crumbs": [
      "Gaussian Elimination"
    ]
  },
  {
    "objectID": "L07LinearTransformations.html",
    "href": "L07LinearTransformations.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nSo far we’ve been treating the matrix equation\n\\[ A{\\bf x} = {\\bf b}\\]\nas simply another way of writing the vector equation\n\\[ x_1{\\bf a_1} + \\dots + x_n{\\bf a_n} = {\\bf b}.\\]\n\nHowever, we’ll now think of the matrix equation in a new way:\nWe will think of \\(A\\) as “acting on” the vector \\({\\bf x}\\) to create a new vector \\({\\bf b}\\).\n\n\nFor example, let’s let \\(A = \\left[\\begin{array}{rrr}2&1&1\\\\3&1&-1\\end{array}\\right].\\) Then we find:\n\\[ A \\left[\\begin{array}{r}1\\\\-4\\\\-3\\end{array}\\right] = \\left[\\begin{array}{r}-5\\\\2\\end{array}\\right] \\]\n\n\nIn other words, if \\({\\bf x} = \\left[\\begin{array}{r}1\\\\-4\\\\-3\\end{array}\\right]\\) and \\({\\bf b} = \\left[\\begin{array}{r}-5\\\\2\\end{array}\\right]\\), then \\(A\\) transforms \\({\\bf x}\\) into \\({\\bf b}\\).\n\n\n\n\n\nNotice what \\(A\\) has done: it took a vector in \\(\\mathbb{R}^3\\) and transformed it into a vector in \\(\\mathbb{R}^2\\).\n\n\nHow does this fact relate to the shape of \\(A\\)?\n\\(A\\) is \\(2 \\times 3\\) — that is, \\(A \\in \\mathbb{R}^{2\\times 3}\\).\n\n\nThis gives a new way of thinking about solving \\(A{\\bf x} = {\\bf b}\\).\nTo solve \\(A{\\bf x} = {\\bf b}\\), we must “search for” the vector(s) \\({\\bf x}\\) in \\(\\mathbb{R}^3\\) that are transformed into \\({\\bf b}\\) in \\(\\mathbb{R}^2\\) under the “action” of \\(A\\).\n\n\nFor a different \\(A\\), the mapping might be from \\(\\mathbb{R}^1\\) to \\(\\mathbb{R}^3\\):\n\n\n\n\n\nWhat would the shape of \\(A\\) be in the above case?\n\n\nSince \\(A\\) maps from \\(\\mathbb{R}^1\\) to \\(\\mathbb{R}^3\\), \\(A \\in \\mathbb{R}^{3\\times 1}\\).\nThat is, \\(A\\) has 3 rows and 1 column.\n\n\nIn another case, \\(A\\) could be a square \\(2\\times 2\\) matrix.\nThen, it would map from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\):\n\n\n\n\n\nWe have moved out of the familiar world of functions of one variable: we are now thinking about functions that transform a vector into a vector.\nOr, put another way, functions that transform multiple variables into multiple variables.",
    "crumbs": [
      "Linear Transformations"
    ]
  },
  {
    "objectID": "L07LinearTransformations.html#linear-transformations",
    "href": "L07LinearTransformations.html#linear-transformations",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nSo far we’ve been treating the matrix equation\n\\[ A{\\bf x} = {\\bf b}\\]\nas simply another way of writing the vector equation\n\\[ x_1{\\bf a_1} + \\dots + x_n{\\bf a_n} = {\\bf b}.\\]\n\nHowever, we’ll now think of the matrix equation in a new way:\nWe will think of \\(A\\) as “acting on” the vector \\({\\bf x}\\) to create a new vector \\({\\bf b}\\).\n\n\nFor example, let’s let \\(A = \\left[\\begin{array}{rrr}2&1&1\\\\3&1&-1\\end{array}\\right].\\) Then we find:\n\\[ A \\left[\\begin{array}{r}1\\\\-4\\\\-3\\end{array}\\right] = \\left[\\begin{array}{r}-5\\\\2\\end{array}\\right] \\]\n\n\nIn other words, if \\({\\bf x} = \\left[\\begin{array}{r}1\\\\-4\\\\-3\\end{array}\\right]\\) and \\({\\bf b} = \\left[\\begin{array}{r}-5\\\\2\\end{array}\\right]\\), then \\(A\\) transforms \\({\\bf x}\\) into \\({\\bf b}\\).\n\n\n\n\n\nNotice what \\(A\\) has done: it took a vector in \\(\\mathbb{R}^3\\) and transformed it into a vector in \\(\\mathbb{R}^2\\).\n\n\nHow does this fact relate to the shape of \\(A\\)?\n\\(A\\) is \\(2 \\times 3\\) — that is, \\(A \\in \\mathbb{R}^{2\\times 3}\\).\n\n\nThis gives a new way of thinking about solving \\(A{\\bf x} = {\\bf b}\\).\nTo solve \\(A{\\bf x} = {\\bf b}\\), we must “search for” the vector(s) \\({\\bf x}\\) in \\(\\mathbb{R}^3\\) that are transformed into \\({\\bf b}\\) in \\(\\mathbb{R}^2\\) under the “action” of \\(A\\).\n\n\nFor a different \\(A\\), the mapping might be from \\(\\mathbb{R}^1\\) to \\(\\mathbb{R}^3\\):\n\n\n\n\n\nWhat would the shape of \\(A\\) be in the above case?\n\n\nSince \\(A\\) maps from \\(\\mathbb{R}^1\\) to \\(\\mathbb{R}^3\\), \\(A \\in \\mathbb{R}^{3\\times 1}\\).\nThat is, \\(A\\) has 3 rows and 1 column.\n\n\nIn another case, \\(A\\) could be a square \\(2\\times 2\\) matrix.\nThen, it would map from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\):\n\n\n\n\n\nWe have moved out of the familiar world of functions of one variable: we are now thinking about functions that transform a vector into a vector.\nOr, put another way, functions that transform multiple variables into multiple variables.",
    "crumbs": [
      "Linear Transformations"
    ]
  },
  {
    "objectID": "L07LinearTransformations.html#transformations",
    "href": "L07LinearTransformations.html#transformations",
    "title": "Geometric Algorithms",
    "section": "Transformations",
    "text": "Transformations\n\nSome terminology:\nA transformation (or function or mapping) \\(T\\) from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\) is a rule that assigns to each vector \\({\\bf x}\\) in \\(\\mathbb{R}^n\\) a vector \\(T({\\bf x})\\) in \\(\\mathbb{R}^m\\).\n\n\nThe set \\(\\mathbb{R}^n\\) is called the domain of \\(T\\), and \\(\\mathbb{R}^m\\) is called the codomain of \\(T\\).\nThe notation:\n\\[ T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\]\nindicates that the domain of \\(T\\) is \\(\\mathbb{R}^n\\) and the codomain is \\(\\mathbb{R}^m\\).\n\n\nFor \\(\\bf x\\) in \\(\\mathbb{R}^n,\\) the vector \\(T({\\bf x})\\) is called the image of \\(\\bf x\\) (under \\(T\\)).\nThe set of all images \\(T({\\bf x})\\) is called the range of \\(T\\).\n\n\n\nHere, the green plane is the set of all points that are possible outputs of \\(T\\) for some input \\(\\mathbf{x}\\).\nSo in this example:\n\nThe domain of \\(T\\) is \\(\\mathbb{R}^2\\)\nThe codomain of \\(T\\) is \\(\\mathbb{R}^3\\)\nThe range of \\(T\\) is the green plane.\n\n\nLet’s do an example. Let’s say I have these points in \\(\\mathbb{R}^2\\):\n\\[ \\left[\\begin{array}{r}0\\\\1\\end{array}\\right],\\left[\\begin{array}{r}1\\\\1\\end{array}\\right],\\left[\\begin{array}{r}1\\\\0\\end{array}\\right],\\left[\\begin{array}{r}0\\\\0\\end{array}\\right]\\]\nWhere are these points located?\n\n\n\n[[0 1 1 0]\n [1 1 0 0]]\n\n\n\n\n\n\n\n\n\n\n\nNow let’s transform each of these points according to the following rule. Let\n\\[ A = \\left[\\begin{array}{rr}1&1.5\\\\0&1\\end{array}\\right]. \\]\nWe define \\(T({\\bf x}) = A{\\bf x}\\). Then we have\n\\[ T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2.\\]\n\n\nWhat is the image of each of these points under \\(T\\)?\n\\[ A\\left[\\begin{array}{r}0\\\\1\\end{array}\\right] = \\left[\\begin{array}{r}1.5\\\\1\\end{array}\\right]\\]\n\\[ A\\left[\\begin{array}{r}1\\\\1\\end{array}\\right] = \\left[\\begin{array}{r}2.5\\\\1\\end{array}\\right]\\]\n\\[ A\\left[\\begin{array}{r}1\\\\0\\end{array}\\right] = \\left[\\begin{array}{r}1\\\\0\\end{array}\\right]\\]\n\\[ A\\left[\\begin{array}{r}0\\\\0\\end{array}\\right] = \\left[\\begin{array}{r}0\\\\0\\end{array}\\right]\\]\n\n\n\n\nsquare = \n[[0 1 1 0]\n [1 1 0 0]]\nA matrix = \n[[1.  1.5]\n [0.  1. ]]\ntransformed square = \n[[1.5 2.5 1.  0. ]\n [1.  1.  0.  0. ]]\n\n\n\n\n\n\n\n\n\n\n\nThis sort of transformation, where points are successively slid sideways, is called a shear transformation.",
    "crumbs": [
      "Linear Transformations"
    ]
  },
  {
    "objectID": "L07LinearTransformations.html#linear-transformations-1",
    "href": "L07LinearTransformations.html#linear-transformations-1",
    "title": "Geometric Algorithms",
    "section": "Linear Transformations",
    "text": "Linear Transformations\nBy the properties of matrix-vector multiplication, we know that the transformation \\({\\bf x} \\mapsto A{\\bf x}\\) has the properties that\n\\[ A({\\bf u} + {\\bf v}) = A{\\bf u} + A{\\bf v} \\;\\;\\;\\text{and}\\;\\;\\; A(c{\\bf u}) = cA{\\bf u}\\]\nfor all \\(\\bf u, v\\) in \\(\\mathbb{R}^n\\) and all scalars \\(c\\).\n\nWe are now ready to define one of the most fundamental concepts in the course: the concept of a linear transformation.\n(You are now finding out why the subject is called linear algebra!)\n\nDefinition. A transformation \\(T\\) is linear if:\n\n\\(T({\\bf u} + {\\bf v}) = T({\\bf u}) + T({\\bf v}) \\;\\;\\;\\) for all \\(\\bf u, v\\) in the domain of \\(T\\); and\n\\(T(c{\\bf u}) = cT({\\bf u}) \\;\\;\\;\\) for all scalars \\(c\\) and all \\(\\bf u\\) in the domain of \\(T\\).\n\n\nTo fully grasp the significance of what a linear transformation is, don’t think of just matrix-vector multiplication. Think of \\(T\\) as a function in more general terms.\n\n\nThe definition above captures a lot of transformations that are not matrix-vector multiplication. For example, think of:\n\\[ T(f) = \\int_0^1 f(t) \\,dt \\]\nIs \\(T\\) a linear transformation?\n\n\nChecking the conditions of our definition:\n\\[ T(f + g) = T(f) + T(g) \\]\nin other words:\n\\[  \\int_0^1 f(t) + g(t) \\,dt = \\int_0^1 f(t) \\,dt + \\int_0^1 g(t) \\,dt\\]\n\n\nand also:\n\\[ T(c \\cdot f) = c \\cdot T(f) \\]\n(check that yourself)\n\n\nWhat about:\n\\[ T(f) = \\frac{d f(t)}{dt} \\]\nIs \\(T\\) a linear transformation?\n\n\nWhat about:\n\\[ T(x) = e^x \\]\nIs \\(T\\) a linear transformation?",
    "crumbs": [
      "Linear Transformations"
    ]
  },
  {
    "objectID": "L07LinearTransformations.html#properties-of-linear-transformations",
    "href": "L07LinearTransformations.html#properties-of-linear-transformations",
    "title": "Geometric Algorithms",
    "section": "Properties of Linear Transformations",
    "text": "Properties of Linear Transformations\nA key aspect of a linear transformation is that it preserves the operations of vector addition and scalar multiplication.\n\nFor example: for vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), one can either:\n\nTransform them both according to \\(T()\\), then add them, or:\nAdd them, and then transform the result according to \\(T()\\).\n\nOne gets the same result either way. The transformation does not affect the addition.\n\n\nThis leads to two important facts.\nIf \\(T\\) is a linear transformation, then\n\\[ T({\\mathbf 0}) = {\\mathbf 0} \\]\nand\n\\[ T(c\\mathbf{u} + d\\mathbf{v}) = cT(\\mathbf{u}) + dT(\\mathbf{v}) \\]\n\n\nIn fact, if a transformation satisfies the second equation for all \\(\\mathbf{u}, \\mathbf{v}\\) and \\(c, d,\\) then it must be a linear transformation.\nBoth of the rules defining a linear transformation derive from this single equation.\n\nExample.\nGiven a scalar \\(r\\), define \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) by \\(T(\\mathbf{x}) = r\\mathbf{x}\\).\n(\\(T\\) is called a contraction when \\(0\\leq r \\leq 1\\) and a dilation when \\(r &gt; 1\\).)\nLet \\(r = 3\\), and show that \\(T\\) is a linear transformation.\n\nSolution.\nLet \\(\\mathbf{u}, \\mathbf{v}\\) be in \\(\\mathbb{R}^2\\) and let \\(c, d\\) be scalars. Then\n\\[\nT(c\\mathbf{u} + d\\mathbf{v}) = 3(c\\mathbf{u} + d\\mathbf{v})\n\\]\n\n\n\\[ = 3c\\mathbf{u} + 3d\\mathbf{v} \\]\n\n\n\\[ = c(3\\mathbf{u}) + d(3\\mathbf{v}) \\]\n\n\n\\[ = cT(\\mathbf{u}) + dT(\\mathbf{v}) \\]\n\n\nThus \\(T\\) is a linear transformation because it satisfies the rule \\(T(c\\mathbf{u} + d\\mathbf{v}) = cT(\\mathbf{u}) + dT(\\mathbf{v})\\).\n\nExample.\n\nLet \\(T(\\mathbf{x}) = \\mathbf{x} + \\mathbf{b}\\) for some \\(\\mathbf{b} \\neq 0\\).\nWhat sort of operation does \\(T\\) implement?\n\n\nAnswer: translation.\nIs \\(T\\) a linear transformation?\n\n\nSolution.\nWe only need to compare\n\\[T(\\mathbf{u} + \\mathbf{v})\\]\nto\n\\[T(\\mathbf{u}) + T(\\mathbf{v}).\\]\n\n\nSo:\n\\[T(\\mathbf{u} + \\mathbf{v}) = \\mathbf{u} + \\mathbf{v} + \\mathbf{b}\\]\nand\n\\[T(\\mathbf{u}) + T(\\mathbf{v}) = (\\mathbf{u} + \\mathbf{b}) + (\\mathbf{v} + \\mathbf{b})\\]\n\n\nIf \\(\\mathbf{b} \\neq 0\\), then the above two expressions are not equal.\nSo \\(T\\) is not a linear transformation.\n\n\nA Non-Geometric Example: Manufacturing\n\nA company manufactures two products, B and C. To do so, it requires materials, labor, and overhead.\nFor one dollar’s worth of product B, it spends 45 cents on materials, 25 cents on labor, and 15 cents on overhead.\nFor one dollar’s worth of product C, it spends 40 cents on materials, 30 cents on labor, and 15 cents on overhead.\n\n\nLet us construct a “unit cost” matrix:\n\\[U = \\begin{array}{r}\n\\begin{array}{rrr}\\text{B}&\\;\\;\\;\\;\\text{C}\\;&\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\end{array}\\\\\n\\left[\\begin{array}{rr}.45&.40\\\\.25&.30\\\\.15&.15\\end{array}\\right]\n\\begin{array}{r}\\text{Materials}\\\\\\text{Labor}\\\\\\text{Overhead}\\end{array}\\\\\n\\end{array}\\]\n\n\nLet \\(\\mathbf{x} = \\left[\\begin{array}{r}x_1\\\\x_2\\end{array}\\right]\\) be a production vector, corresponding to \\(x_1\\) dollars of product B and \\(x_2\\) dollars of product C.\nThen define \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\) by\n\\[T(\\mathbf{x}) = U\\mathbf{x} \\]\n\n\n\\[ = x_1 \\left[\\begin{array}{r}.45\\\\.25\\\\.15\\end{array}\\right] + x_2 \\left[\\begin{array}{r}.40\\\\.30\\\\.15\\end{array}\\right]\\]\n\n\n\\[ = \\left[\\begin{array}{r}\\text{Total cost of materials}\\\\\\text{Total cost of labor}\\\\\\text{Total cost of overhead}\\end{array}\\right]\n\\]\n\n\nThe mapping \\(T\\) transforms a list of production quantities into a list of total costs.\n\n\nThe linearity of this mapping is reflected in two ways:\n\nIf production is increased by a factor of, say, 4, ie, from \\(\\mathbf{x}\\) to \\(4\\mathbf{x}\\), then the costs increase by the same factor, from \\(T(\\mathbf{x})\\) to \\(4T(\\mathbf{x})\\).\nIf \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are production vectors, then the total cost vector associated with combined production of \\(\\mathbf{x} + \\mathbf{y}\\) is precisely the sum of the cost vectors \\(T(\\mathbf{x})\\) and \\(T(\\mathbf{y})\\).",
    "crumbs": [
      "Linear Transformations"
    ]
  },
  {
    "objectID": "L25SVD.html",
    "href": "L25SVD.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nToday we’ll begin our study of the most useful matrix decomposition in applied Linear Algebra.\nPretty exciting, eh?\n\n\nThe Singular Value Decomposition is the “Swiss Army Knife” and the “Rolls Royce” of matrix decompositions.\n\n– Diane O’Leary\n\n\nThe singular value decomposition is a matrix factorization.\nNow, the first thing to know is that EVERY matrix has a singular value decomposition.",
    "crumbs": [
      "The Singular Value Decomposition"
    ]
  },
  {
    "objectID": "L25SVD.html#the-singular-value-decomposition",
    "href": "L25SVD.html#the-singular-value-decomposition",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nToday we’ll begin our study of the most useful matrix decomposition in applied Linear Algebra.\nPretty exciting, eh?\n\n\nThe Singular Value Decomposition is the “Swiss Army Knife” and the “Rolls Royce” of matrix decompositions.\n\n– Diane O’Leary\n\n\nThe singular value decomposition is a matrix factorization.\nNow, the first thing to know is that EVERY matrix has a singular value decomposition.",
    "crumbs": [
      "The Singular Value Decomposition"
    ]
  },
  {
    "objectID": "L25SVD.html#maximizing-vert-amathbfxvert",
    "href": "L25SVD.html#maximizing-vert-amathbfxvert",
    "title": "Geometric Algorithms",
    "section": "Maximizing \\(\\Vert A\\mathbf{x}\\Vert\\)",
    "text": "Maximizing \\(\\Vert A\\mathbf{x}\\Vert\\)\n\nThe singular value decomposition (let’s just call it SVD) is based on a very simple question:\n\n\nLet’s say you are given an arbitrary matrix \\(A\\), which does not need to be square.\n\n\nHere is the question:\nAmong all unit vectors, what is the vector \\(\\mathbf{x}\\) that maximizes \\(\\Vert A\\mathbf{x}\\Vert\\)?\n\n\nIn other words, in which direction does \\(A\\) create the largest output vector from a unit input?\n\nTo set the stage to answer this question, let’s review a few facts.\nYou recall that the eigenvalues of a square matrix \\(A\\) measure the amount that \\(A\\) “stretches or shrinks” certain special vectors (the eigenvectors).\n\nFor example, for a square \\(A\\), if \\(A\\mathbf{x} = \\lambda\\mathbf{x}\\) and \\(\\Vert \\mathbf{x}\\Vert = 1,\\) then\n\\[\\Vert A\\mathbf{x}\\Vert = \\Vert\\lambda\\mathbf{x}\\Vert = |\\lambda|\\,\\Vert\\mathbf{x}\\Vert = |\\lambda|.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe largest value of \\(\\Vert A\\mathbf{x}\\Vert\\) is the long axis of the ellipse. Clearly there is some \\(\\mathbf{x}\\) that is mapped to that point by \\(A\\). That \\(\\mathbf{x}\\) is what we want to find.\n\n\nAnd let’s make clear that we can apply this idea to arbitrary (non-square) matrices.\n\n\n\nImage credit: Linear Algebra and its Applications, by David C. Lay\nHere is an example that shows that we can still ask the question of what unit \\(\\mathbf{x}\\) maximizes \\(\\Vert A\\mathbf{x}\\Vert\\) even when \\(A\\) is not square.\nFor example:\nIf \\(A = \\begin{bmatrix}4&11&14\\\\8&7&-2\\end{bmatrix},\\)\nthen the linear transformation \\(\\mathbf{x} \\mapsto A\\mathbf{x}\\) maps the unit sphere \\(\\{\\mathbf{x} : \\Vert \\mathbf{x} \\Vert = 1\\}\\) in \\(\\mathbb{R}^3\\) onto an ellipse in \\(\\mathbb{R}^2\\), as shown here:\n\n\n\\(\\Vert A\\mathbf{x}\\Vert^2\\) is a Quadratic Form\n\nNow, here is a way to answer our question:\nProblem. Find the unit vector \\(\\mathbf{x}\\) at which the length \\(\\Vert A\\mathbf{x}\\Vert\\) is maximized, and compute this maximum length.\n\n\nSolution.\nThe quantity \\(\\Vert A\\mathbf{x}\\Vert^2\\) is maximized at the same \\(\\mathbf{x}\\) that maximizes \\(\\Vert A\\mathbf{x}\\Vert\\), and \\(\\Vert A\\mathbf{x}\\Vert^2\\) is easier to study.\nSo let’s ask to find the unit vector \\(\\mathbf{x}\\) at which \\(\\Vert A\\mathbf{x}\\Vert^2\\) is maximized.\n\n\nObserve that\n\\[ \\Vert A\\mathbf{x}\\Vert^2 = (A\\mathbf{x})^T(A\\mathbf{x}) \\]\n\n\n\\[ = \\mathbf{x}^TA^TA\\mathbf{x} \\]\n\n\n\\[ = \\mathbf{x}^T(A^TA)\\mathbf{x} \\]\n\n\nNow, \\(A^TA\\) is a symmetric matrix.\nSo we see that \\(\\Vert A\\mathbf{x}\\Vert^2 = \\mathbf{x}^TA^TA\\mathbf{x}\\) is a quadratic form!\n… and we are seeking to maximize it subject to the constraint \\(\\Vert \\mathbf{x}\\Vert = 1\\).\n\n\nAs we learned in the last lecture, the maximum value of a quadratic form, subject to the constraint that \\(\\Vert\\mathbf{x}\\Vert = 1\\), is the largest eigenvalue of the symmetric matrix.\nSo the maximum value of \\(\\Vert A\\mathbf{x}\\Vert^2\\) subject to \\(\\Vert\\mathbf{x}\\Vert = 1\\) is \\(\\lambda_1\\), the largest eigenvalue of \\(A^TA\\).\nAlso, the maximum is attained at a unit eigenvector of \\(A^TA\\) corresponding to \\(\\lambda_1\\).\n\nFor the matrix \\(A\\) in the 2 \\(\\times\\) 3 example,\n\\[A^TA = \\begin{bmatrix}4&8\\\\11&7\\\\14&-2\\end{bmatrix} \\,\\begin{bmatrix}4&11&14\\\\8&7&-2\\end{bmatrix} = \\begin{bmatrix}80&100&40\\\\100&170&140\\\\40&140&200\\end{bmatrix}.\\]\n\nThe eigenvalues of \\(A^TA\\) are \\(\\lambda_1 = 360, \\lambda_2 = 90,\\) and \\(\\lambda_3 = 0.\\)\nThe corresponding unit eigenvectors are, respectively,\n\\[\\mathbf{v}_1 = \\begin{bmatrix}1/3\\\\2/3\\\\2/3\\end{bmatrix}, \\mathbf{v}_2 = \\begin{bmatrix}-2/3\\\\-1/3\\\\2/3\\end{bmatrix}, \\mathbf{v}_3 = \\begin{bmatrix}2/3\\\\-2/3\\\\1/3\\end{bmatrix}.  \\]\n\n\nFor \\(\\Vert\\mathbf{x}\\Vert = 1\\), the maximum value of \\(\\Vert A\\mathbf{x}\\Vert\\) is \\(\\Vert A\\mathbf{v}_1\\Vert = \\sqrt{360}.\\)\n\n\nThis example shows that the key to understanding the effect of \\(A\\) on the unit sphere in \\(\\mathbb{R}^3\\) is to examime the quadratic form \\(\\mathbf{x}^T(A^TA)\\mathbf{x}.\\)\n\nWe can also go back to our 2 \\(\\times\\) 2 example.\nLet’s plot the eigenvectors of \\(A^TA\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that the eigenvector corresponding to the largest eigenvalue of \\(A^TA\\) indeed shows us where \\(\\Vert A\\mathbf{x}\\Vert\\) is maximized – where the ellipse is longest.\nAlso, the other eigenvector of \\(A^TA\\) shows us where the ellipse is narrowest.\n\n\nIn fact, the entire geometric behavior of the transformation \\(\\mathbf{x}\\mapsto A\\mathbf{x}\\) is captured by the quadratic form \\(\\mathbf{x}^TA^TA\\mathbf{x}\\).",
    "crumbs": [
      "The Singular Value Decomposition"
    ]
  },
  {
    "objectID": "L25SVD.html#the-singular-values-of-a-matrix",
    "href": "L25SVD.html#the-singular-values-of-a-matrix",
    "title": "Geometric Algorithms",
    "section": "The Singular Values of a Matrix",
    "text": "The Singular Values of a Matrix\n\nLet’s continue to consider \\(A\\) to be an arbitrary \\(m\\times n\\) matrix.\nNotice that even though \\(A\\) is not square in general, \\(A^TA\\) is square and symmetric.\nSo, there is a lot we can say about \\(A^TA\\).\nIn particular, since \\(A^TA\\) is symmetric, it can be orthogonally diagonalized (as we saw in the last lecture).\n\n\nSo let \\(\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\}\\) be an orthonormal basis for \\(\\mathbb{R}^n\\) consisting of eigenvectors of \\(A^TA\\), and let \\(\\lambda_1, \\dots, \\lambda_n\\) be the corresponding eigenvalues of \\(A^TA\\).\n\n\nThen, for any eigenvector \\(\\mathbf{v}_i\\),\n\\[ \\Vert A\\mathbf{v}_i\\Vert^2 = (A\\mathbf{v}_i)^T A\\mathbf{v}_i = \\mathbf{v}_i^T A^TA\\mathbf{v}_i \\]\n\n\n\\[ = \\mathbf{v}_i^T(\\lambda_i)\\mathbf{v}_i \\] (since \\(\\mathbf{v}_i\\) is an eigenvector of \\(A^TA\\))\n\n\n\\[ = \\lambda_i\\] (since \\(\\mathbf{v}_i\\) is a unit vector.)\n\n\nNow any expression \\(\\Vert\\cdot\\Vert^2\\) is nonnegative.\nSo the eigenvalues of \\(A^TA\\) are all nonnegative.\nThat is: \\(A^TA\\) is positive semidefinite.\n\n\nWe can therefore renumber the eigenvalues so that\n\\[\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n \\geq 0.\\]\n\nDefinition. The singular values of \\(A\\) are the square roots of the eigenvalues of \\(A^TA\\). They are denoted by \\(\\sigma_1,\\dots,\\sigma_n,\\) and they are arranged in decreasing order.\nThat is, \\(\\sigma_i = \\sqrt{\\lambda_i}\\) for \\(i = 1,\\dots,n.\\)\n\nBy the above argument, the singular values of \\(A\\) are the lengths of the vectors \\(A\\mathbf{v}_1, \\dots, A\\mathbf{v}_n.\\)\n\n\nThe Eigenvectors of \\(A^TA\\) Lead To an Orthogonal Basis for \\(\\operatorname{Col} A\\)\n\nNow: we know that vectors \\(\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\) are an orthogonal set because they are eigenvectors of the symmetric matrix \\(A^TA\\).\nHowever, it’s also the case that \\(A\\mathbf{v}_1, \\dots, A\\mathbf{v}_n\\) are an orthogonal set.\nThis fact is key to the SVD.\n\n\nIt is not obvious at first that \\(A\\mathbf{v}_1, \\dots, A\\mathbf{v}_n\\) are an orthogonal set!\nBut it is true – let’s prove it (and a bit more).\n\n\n\n\n\n\nTheorem. Suppose \\(\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\}\\) is an orthonormal basis of \\(\\mathbb{R}^n\\) consisting of eigenvectors of \\(A^TA\\), arranged so that the corresponding eigenvalues of \\(A^TA\\) satisfy \\(\\lambda_1 \\geq \\cdots \\geq \\lambda_n,\\) and suppose \\(A\\) has \\(r\\) nonzero singular values.\nThen \\(\\{A\\mathbf{v}_1, \\dots, A\\mathbf{v}_r\\}\\) is an orthogonal basis for \\(\\operatorname{Col}\\ A,\\) and rank \\(A = r\\).\n\nNote how surprising this is: while \\(\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\}\\) are a basis for \\(\\mathbb{R}^n\\), \\(\\operatorname{Col} A\\) is a subspace of \\(\\mathbb{R}^m\\).\nNonetheless,\n\ntwo eigenvectors \\(\\mathbf{v}_i\\) and \\(\\mathbf{v}_j \\in \\mathbb{R}^n\\) are orthogonal, and\ntheir images \\(A\\mathbf{v}_i\\) and \\(A\\mathbf{v}_j \\in \\mathbb{R}^m\\) are also orthogonal.\n\n\n\nProof. What we need to do is establish that \\(\\{A\\mathbf{v}_1, \\dots, A\\mathbf{v}_r\\}\\) is an orthogonal linearly independent set whose span is \\(\\operatorname{Col}A\\).\nBecause \\(\\mathbf{v}_i\\) and \\(\\mathbf{v}_j\\) are orthogonal for \\(i\\neq j\\),\n\\[ (A\\mathbf{v}_i)^T(A\\mathbf{v}_j) = \\mathbf{v}_i^TA^TA\\mathbf{v}_j = \\mathbf{v}_i^T(\\lambda_j \\mathbf{v}_j) = 0.\\]\nSo \\(\\{A\\mathbf{v}_1, \\dots, A\\mathbf{v}_n\\}\\) is an orthogonal set.\n\n\nFurthermore, since the lengths of the vectors \\(A\\mathbf{v}_1, \\dots, A\\mathbf{v}_n\\) are the singular values of \\(A\\), and since there are \\(r\\) nonzero singular values, \\(A\\mathbf{v}_i \\neq {\\mathbf 0}\\) if and only if \\(1 \\leq i \\leq r.\\)\nSo \\(A\\mathbf{v}_1, \\dots, A\\mathbf{v}_r\\) are a linearly independent set (because they are orthogonal and all nonzero), and clearly they are each in \\(\\operatorname{Col}\\ A\\).\n\n\nFinally, we just need to show that \\(\\operatorname{Span}\\{A\\mathbf{v}_1, \\dots, A\\mathbf{v}_r\\}\\) = \\(\\operatorname{Col} A\\).\n\n\nTo do this we’ll show that for any \\(\\mathbf{y}\\) in \\(\\operatorname{Col}\\ A\\), we can write \\(\\mathbf{y}\\) in terms of \\(\\{A\\mathbf{v}_1, \\dots, A\\mathbf{v}_r\\}\\):\nSay \\(\\mathbf{y} = A\\mathbf{x}.\\)\n\n\nBecause \\(\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\}\\) is a basis for \\(\\mathbb{R}^n\\), we can write \\(\\mathbf{x} = c_1\\mathbf{v}_1 + \\dots + c_n\\mathbf{v}_n,\\) so\n\\[\\mathbf{y} = A\\mathbf{x} = c_1A\\mathbf{v}_1 + \\dots + c_rA\\mathbf{v}_r + \\dots + c_nA\\mathbf{v}_n.\\]\n\n\n\\[ = c_1A\\mathbf{v}_1 + \\dots + c_rA\\mathbf{v}_r. \\] (because \\(A\\mathbf{v}_i = {\\mathbf 0}\\) for \\(i &gt; r\\)).\n\n\nIn summary: \\(\\{A\\mathbf{v}_1, \\dots, A\\mathbf{v}_r\\}\\) is an (orthogonal) linearly independent set whose span is \\(\\operatorname{Col} A\\), so it is an (orthogonal) basis for \\(\\operatorname{Col} A\\).\n\n\nNotice that we have also proved that \\(\\operatorname{rank} A = \\dim\\operatorname{Col} A = r.\\)\nIn other words, if \\(A\\) has \\(r\\) nonzero singular values, \\(A\\) has rank \\(r\\).",
    "crumbs": [
      "The Singular Value Decomposition"
    ]
  },
  {
    "objectID": "L25SVD.html#the-singular-value-decomposition-1",
    "href": "L25SVD.html#the-singular-value-decomposition-1",
    "title": "Geometric Algorithms",
    "section": "The Singular Value Decomposition",
    "text": "The Singular Value Decomposition\n\nWhat we have just proved is that the eigenvectors of \\(A^TA\\) are rather special.\nNote that, thinking of \\(A\\) as a linear operator:\n\nits domain is \\(\\mathbb{R}^n\\), and\nits range is \\(\\operatorname{Col} A.\\)\n\n\n\nSo we have just proved that\n\nthe set \\(\\{\\mathbf{v}_i\\}\\) is an orthogonal basis for the domain of \\(A\\), for \\(i = 1,\\dots, n\\) and\nthe set \\(\\{A\\mathbf{v}_i\\}\\) is an orthogonal basis for the range of \\(A\\), for \\(i = 1,\\dots, r\\).\n\nNow we can define the SVD.\n\nTheorem. Let \\(A\\) be an \\(m\\times n\\) matrix with rank \\(r\\). Then there exists an \\(m\\times n\\) matrix \\(\\Sigma\\) whose diagonal entries are the first \\(r\\) singular values of \\(A\\), \\(\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r &gt; 0,\\) and there exists an \\(m\\times m\\) orthogonal matrix \\(U\\) and an \\(n\\times n\\) orthogonal matrix \\(V\\) such that\n\\[ A = U\\Sigma V^T \\]\n\nAny factorization \\(A = U\\Sigma V^T,\\) with \\(U\\) and \\(V\\) orthogonal and \\(\\Sigma\\) a diagonal matrix is called a singular value decomposition (SVD) of \\(A\\).\nThe columns of \\(U\\) are called the left singular vectors and the columns of \\(V\\) are called the right singular vectors of \\(A\\).\n\nAside: regarding the “Rolls Royce” property, consider how elegant this structure is:\n\n\n\\(A\\) is an arbitrary matrix\n\\(U\\) and \\(V\\) are both orthogonal matrices\n\\(\\Sigma\\) is a diagonal matrix\nall singular values are positive or zero\nthere are as many positive singular values as the rank of \\(A\\)\n\n(not part of the theorem but we’ll see it is true)\n\n\n\n\n\n\nWe have built up enough tools now that the proof is quite straightforward.\nProof. Let \\(\\lambda_i\\) and \\(\\mathbf{v}_i\\) be the eigenvalues and eigenvectors of \\(A^TA\\), and \\(\\sigma_i = \\sqrt{\\lambda_i}\\).\n\nThe starting point is to use the fact that we just proved:\n\\(\\{A\\mathbf{v}_1, \\dots, A\\mathbf{v}_r\\}\\) is an orthogonal basis for \\(\\operatorname{Col}\\ A.\\)\n\n\nNext, let us normalize each \\(A\\mathbf{v}_i\\) to obtain an orthonormal basis \\(\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_r\\}\\), where\n\\[ \\mathbf{u}_i = \\frac{1}{\\Vert A\\mathbf{v}_i\\Vert}A\\mathbf{v}_i = \\frac{1}{\\sigma_i}A\\mathbf{v}_i \\]\n\n\nThen\n\\[ A\\mathbf{v}_i = \\sigma_i\\mathbf{u}_i\\;\\;\\;\\;(1 \\leq i \\leq r)\\]\n\n\n\nIf you have an orthogonal basis for a subspace, and you want to extend it to an orthogonal basis for the entire space, you can do it by a straightforward procedure called the Gram-Schmidt process. We aren’t covering it in this course, but that is what you would use here to extend the set \\(\\{\\mathbf{u}_{1} \\dots \\mathbf{u}_r\\}\\) to the set \\(\\{\\mathbf{u}_{1} \\dots \\mathbf{u}_m\\}\\).\n\nNow the rank of \\(A\\) (which is \\(r\\)) may be less than \\(m\\).\nIn that case, add additional orthonormal vectors \\(\\{\\mathbf{u}_{r+1} \\dots \\mathbf{u}_m\\}\\) to the set so that they span \\(\\mathbb{R}^m\\).\n\n\nNow collect the vectors into matrices.\n\\[ U = \\begin{bmatrix}\\mathbf{u}_1&\\cdots&\\mathbf{u}_m\\end{bmatrix}\\]\nand\n\\[ V = \\begin{bmatrix}\\mathbf{v}_1&\\cdots&\\mathbf{v}_n\\end{bmatrix}\\]\nRecall that these matrices are orthogonal because the \\(\\{\\mathbf{v_i}\\}\\) are orthogonal and the \\(\\{A\\mathbf{v_i}\\}\\) are orthogonal, as we previously proved.\n\n\nSo\n\\[ AV = [A\\mathbf{v}_1\\;\\cdots\\;A\\mathbf{v}_r\\;\\overbrace{\\mathbf{0}\\cdots\\mathbf{0}}^{n-r}]\\]\n\n\n\\[ = [\\sigma_1\\mathbf{u}_1\\;\\cdots\\;\\sigma_r\\mathbf{u}_r\\;\\overbrace{\\mathbf{0}\\;\\cdots\\;\\mathbf{0}}^{n-r}] = U\\Sigma. \\]\n(keeping in mind that \\(U\\) is \\(m\\times m\\) and \\(\\Sigma\\) is \\(m\\times n\\).)\n\n\nSo\n\\[ AV = U\\Sigma\\]\n\n\nSo:\n\\[AVV^T = U\\Sigma V^T\\]\nAnd because \\(V\\) is an orthogonal matrix:\n\\[ A = U\\Sigma V^T\\]",
    "crumbs": [
      "The Singular Value Decomposition"
    ]
  },
  {
    "objectID": "L25SVD.html#the-reduced-svd-and-the-pseudoinverse",
    "href": "L25SVD.html#the-reduced-svd-and-the-pseudoinverse",
    "title": "Geometric Algorithms",
    "section": "The Reduced SVD and the Pseudoinverse",
    "text": "The Reduced SVD and the Pseudoinverse\n\nLet’s step back to get a sense of how the SVD decomposes a matrix.\nLet’s say \\(A\\) is \\(m\\times n\\) with \\(m&lt;n\\).\n(The situation when \\(m&gt;n\\) follows similarly).\n\n\nThe SVD looks like this, with singular values on the diagonal of \\(\\Sigma\\):\n\n\n\n\n\nNow, let’s assume that the number of nonzero singular values \\(r\\) is less than \\(m\\).\nAgain, other cases would be similar.\n\n\n\n\n\nIn many cases we are only concerned with representing \\(A\\).\nThat is, we don’t need \\(U\\) or \\(V\\) to be orthogonal (square) matrices.\nThen, to compute \\(A\\), we only need the \\(r\\) leftmost columns of \\(U\\), and the \\(r\\) upper rows of \\(V^T\\).\nThat’s because all the other values on the diagonal of \\(\\Sigma\\) are zero, so they don’t contribute anything to \\(A\\).\n\n\n\n\n\nSo we often work with the reduced SVD of \\(A\\):\n\\(\\small \\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\;\\;\\overbrace{\\left[\\begin{array}{ccc}\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_1}\\\\\\vdots\\\\\\vdots\\end{array}&\\dots&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_n}\\\\\\vdots\\\\\\vdots\\end{array}\\\\\\end{array}\\right]}^{\\Large n} =\n\\overbrace{\\left[\\begin{array}{ccc}\\vdots&&\\vdots\\\\\\vdots&&\\vdots\\\\\\mathbf{u}_1&\\cdots&\\mathbf{u}_r\\\\\\vdots&&\\vdots\\\\\\vdots&&\\vdots\\end{array}\\right]}^{\\large r}\n\\times\n\\left[\\begin{array}{ccc}\\sigma_1& &\\\\&\\ddots&\\\\&&\\sigma_r\\end{array}\\right]\n\\times\n\\left[\\begin{array}{ccc}\\dots&\\mathbf{v}_1&\\dots\\\\&\\vdots&\\\\\\dots&\\mathbf{v}_r&\\dots\\end{array}\\right]\\)\n\n\n\\[\\Large\\overset{m\\,\\times\\, n}{A^{\\vphantom{T}}} = \\overset{m\\,\\times\\, r}{U^{\\vphantom{T}}}\\;\\;\\overset{r\\,\\times\\, r}{\\Sigma^{\\vphantom{T}}}\\;\\;\\overset{r\\,\\times\\, n}{V^T}\\]\n\n\nNote that in the reduced SVD, \\(\\Sigma\\) has all nonzero entries on its diagonal, so it can be inverted.\nHowever, we still have that \\(A = U\\Sigma V^T\\).\n\n\nThe Pseudoinverse\n\nConsider the case where we are working with the reduced SVD of \\(A\\):\n\\[A = U\\Sigma V^T.\\]\nIn the reduced SVD, \\(\\Sigma\\) is invertible (it is a diagonal matrix with all positive entries on the diagonal).\n\n\nUsing this decomposition we can define an important matrix corresponding to \\(A\\).\n\n\n\\[A^+ = V \\Sigma^{-1} U^T\\]\n\n\nThis matrix \\(A^+\\) is called the pseudoinverse of \\(A\\).\n(Sometimes called the Moore-Penrose pseudoinverse).\n\n\nObviously, \\(A\\) cannot have an inverse, because it is not even square (let alone invertible) in general.\nSo why is \\(A^+\\) called the pseudoinverse?\n\n\nLet’s go back to our favorite equation, \\(A\\mathbf{x} = \\mathbf{b}\\), specifically in the case where there are no solutions.\nIn that case, we can find least-squares solutions by finding \\(\\mathbf{\\hat{x}}\\) such that \\(A\\mathbf{\\hat{x}}\\) is the projection of \\(\\mathbf{b}\\) onto \\(\\operatorname{Col}A\\).\nAnd, if \\(A^TA\\) is invertible, that \\(\\mathbf{\\hat{x}}\\) is given by\n\\[ \\mathbf{\\hat{x}} = (A^TA)^{-1}A^T\\mathbf{b}\\]\n\n\nBut, what if \\(A^TA\\) is not invertible?\nThere are still least-square solutions, but now there are an infinite number.\nWhat if we just want to find one of them?\n\n\nWe can do that using the pseudoinverse:\n\\[\\mathbf{\\hat{x}} = A^+ \\mathbf{b}\\]\n\n\nWhy does this work? Let’s see.\nIf we define \\(\\mathbf{\\hat{x}}\\) this way, then what is \\(A\\mathbf{\\hat{x}}\\)?\n\\[A\\mathbf{\\hat{x}} = AA^+ \\mathbf{b}\\]\n\n\n\\[ = (U\\Sigma V^T)(V\\Sigma^{-1}U^T)\\mathbf{b}\\]\n\n\n\\[ = U\\Sigma\\Sigma^{-1}U^T\\mathbf{b}\\]\n\n\n\\[ = UU^T\\mathbf{b}\\]\n\n\nNow, \\(U\\) is an orthonormal basis for \\(\\operatorname{Col}A\\).\nAnd, \\(U^T\\mathbf{b}\\) are the coefficients of the projection of \\(\\mathbf{b}\\) onto each column of \\(U\\), since the columns are unit length.\nSo, \\(UU^T\\mathbf{b}\\) is the projection of \\(\\mathbf{b}\\) onto \\(\\operatorname{Col}A\\).\n\n\n\nTo see why this is true, recall that when we project onto an orthogonal basis, the projection is especially easy.\nLet’s say that \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\) form an orthogonal basis for some subspace. Then the projection of \\(\\mathbf{b}\\) onto that subspace is very simple:\n\\[ \\hat{\\mathbf{b}} = \\frac{\\mathbf{u}_1^T\\mathbf{b}}{\\mathbf{u}_1^T\\mathbf{u}_1}\\mathbf{u}_1 + \\frac{\\mathbf{u}_2^T\\mathbf{b}}{\\mathbf{u}_2^T\\mathbf{u}_2}\\mathbf{u}_2. \\]\nNow, how does this become \\(UU^T\\mathbf{b}\\)? Say that \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\) are the columns of \\(U\\).\n\\(U\\) is an orthonormal matrix, so \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\) are unit vectors, so \\(\\mathbf{u}_1^T\\mathbf{u}_1 = 1\\) and \\(\\mathbf{u}_2^T\\mathbf{u}_2 = 1\\).\nSo\n\\[ \\hat{\\mathbf{b}} = \\mathbf{u}_1^T\\mathbf{b}\\mathbf{u}_1 + \\mathbf{u}_2^T\\mathbf{b}\\mathbf{u}_2. \\]\nNote that \\(\\mathbf{u}_1^T\\mathbf{b}\\) and \\(\\mathbf{u}_2^T\\mathbf{b}\\) are scalar quantities, which we can collect into a vector, that can be expressed as \\(U^T\\mathbf{b}\\).\nAnd multiplying that vector by \\(U\\) gives us the projection above:\n\\[ \\hat{\\mathbf{b}} = \\mathbf{u}_1^T\\mathbf{b}\\mathbf{u}_1 + \\mathbf{u}_2^T\\mathbf{b}\\mathbf{u}_2  = UU^T\\mathbf{b}. \\]\n\nSo, \\(\\mathbf{\\hat{x}} = A^+ \\mathbf{b}\\) is a least squares solution of \\(A\\mathbf{x} = \\mathbf{b}\\),\nbecause \\(A\\mathbf{\\hat{x}}\\) is the projection of \\(\\mathbf{b}\\) onto the column space of \\(A\\).\n\n\nAnd this is true even when \\(A^TA\\) is not invertible,\nie, this formula works for any \\(A\\).\nRemember, any \\(A\\) has an SVD, and so any \\(A\\) has a pseudoinverse!\n\n\nUsing these facts, we can now think of the least squares solution set of \\(A\\mathbf{x} = \\mathbf{b}\\) in a particularly simple way.\nIt is always the case for any least-squares solution \\(\\mathbf{\\hat{x}}\\) that:\n\\[\\mathbf{\\hat{x}} = A^+ \\mathbf{b} + \\mathbf{z}\\]\nwhere \\(\\mathbf{z}\\) is a vector in the nullspace of \\(A\\).\n\n\nHence we can write the set of all least-squares solutions in a particuarly clear way:\n\\[\\{\\hat{\\mathbf{x}}\\} = \\{A^+ \\mathbf{b} + \\mathbf{z}\\,|\\,\\mathbf{z} \\in \\operatorname{Nul} A\\}.\\]\n\n\nWhen \\(A\\) has dependent columns, then it has a nontrivial nullspace, and the above expression gives the infinite set of least-squares solutions.\nWhen \\(A\\) has independent columns, then its nullspace consists only of \\(\\mathbf{0}\\), and the above expression gives the unique least-squares solution.",
    "crumbs": [
      "The Singular Value Decomposition"
    ]
  },
  {
    "objectID": "L10MatrixInverse.html",
    "href": "L10MatrixInverse.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nToday we investigate the idea of the “reciprocal” of a matrix.\n\nFor reasons that will become clear, we will think about this way:\nThe reciprocal of any nonzero number \\(r\\) is its multiplicative inverse.\nThat is, \\(1/r = r^{-1}\\) such that \\(r \\cdot r^{-1} = 1.\\)\nThis gives a way to define what is called the inverse of a matrix.\n\nImportantly: we have to recognize that this inverse does not exist for all matrices.\n\nIt only exists for square matrices\nAnd not even for all square matrices – only those that are “invertible.”\n\nDefinition. An \\(n\\times n\\) matrix \\(A\\) is called invertible if there exists an \\(n\\times n\\) matrix \\(C\\) such that\n\\[ AC = I \\;\\;\\text{ and }\\;\\; CA = I. \\]\n\nIn that case \\(C\\) is called the inverse of \\(A\\).\n\n\nClearly, \\(C\\) must also be square and the same size as \\(A\\).\nThe inverse of \\(A\\) is denoted \\(A^{-1}.\\)\n\n\nA matrix that is not invertible is called a singular matrix.\nA strange term, but you just have to memorize and get used to it.\n\nExample.\nIf \\(A = \\left[\\begin{array}{rr}2&5\\\\-3&-7\\end{array}\\right]\\) and \\(C = \\left[\\begin{array}{rr}-7&-5\\\\3&2\\end{array}\\right]\\), then:\n\n\\[ AC = \\left[\\begin{array}{rr}2&5\\\\-3&-7\\end{array}\\right]\\left[\\begin{array}{rr}-7&-5\\\\3&2\\end{array}\\right] = \\left[\\begin{array}{rr}1&0\\\\0&1\\end{array}\\right],\\]\nand:\n\\[ CA = \\left[\\begin{array}{rr}-7&-5\\\\3&2\\end{array}\\right]\\left[\\begin{array}{rr}2&5\\\\-3&-7\\end{array}\\right] = \\left[\\begin{array}{rr}1&0\\\\0&1\\end{array}\\right],\\]\nso we conclude that \\(C = A^{-1}.\\)\n\nLet’s think about what a matrix inverse does in a linear equation.\n\nTake a standard linear equation:\n\\[ A{\\bf x} = {\\bf b}. \\]\n\n\nThen:\n\\[A^{-1}(A{\\bf x}) = A^{-1}{\\bf b}\\]\n\n\n\\[(A^{-1}A){\\bf x} = A^{-1}{\\bf b}\\]\n\n\n\\[I{\\bf x} = A^{-1}{\\bf b}\\]\n\n\n\\[{\\bf x} = A^{-1}{\\bf b}\\]\n\nTheorem. If \\(A\\) is an invertible \\(n\\times n\\) matrix, then for each \\({\\bf b}\\) in \\(\\mathbb{R}^n,\\) the equation \\(A{\\bf x} = {\\bf b}\\) has the unique solution \\(A^{-1}{\\bf b}.\\)\nProof. Follows directly from the definition of \\(A^{-1}.\\)\n\nThis very simple, powerful theorem gives us a new way to solve a linear system.\n\n\nFurthermore, this theorem connects the matrix inverse to certain kinds of linear systems.\nWe know that not all linear systems of \\(n\\) equations in \\(n\\) variables have a unique solution.\nSuch systems may have no solutions (inconsistent) or an infinite number of solutions.\nBut this theorem says that if \\(A\\) is invertible, then the system has a unique solution.",
    "crumbs": [
      "The Inverse of a Matrix"
    ]
  },
  {
    "objectID": "L10MatrixInverse.html#the-inverse-of-a-matrix",
    "href": "L10MatrixInverse.html#the-inverse-of-a-matrix",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nToday we investigate the idea of the “reciprocal” of a matrix.\n\nFor reasons that will become clear, we will think about this way:\nThe reciprocal of any nonzero number \\(r\\) is its multiplicative inverse.\nThat is, \\(1/r = r^{-1}\\) such that \\(r \\cdot r^{-1} = 1.\\)\nThis gives a way to define what is called the inverse of a matrix.\n\nImportantly: we have to recognize that this inverse does not exist for all matrices.\n\nIt only exists for square matrices\nAnd not even for all square matrices – only those that are “invertible.”\n\nDefinition. An \\(n\\times n\\) matrix \\(A\\) is called invertible if there exists an \\(n\\times n\\) matrix \\(C\\) such that\n\\[ AC = I \\;\\;\\text{ and }\\;\\; CA = I. \\]\n\nIn that case \\(C\\) is called the inverse of \\(A\\).\n\n\nClearly, \\(C\\) must also be square and the same size as \\(A\\).\nThe inverse of \\(A\\) is denoted \\(A^{-1}.\\)\n\n\nA matrix that is not invertible is called a singular matrix.\nA strange term, but you just have to memorize and get used to it.\n\nExample.\nIf \\(A = \\left[\\begin{array}{rr}2&5\\\\-3&-7\\end{array}\\right]\\) and \\(C = \\left[\\begin{array}{rr}-7&-5\\\\3&2\\end{array}\\right]\\), then:\n\n\\[ AC = \\left[\\begin{array}{rr}2&5\\\\-3&-7\\end{array}\\right]\\left[\\begin{array}{rr}-7&-5\\\\3&2\\end{array}\\right] = \\left[\\begin{array}{rr}1&0\\\\0&1\\end{array}\\right],\\]\nand:\n\\[ CA = \\left[\\begin{array}{rr}-7&-5\\\\3&2\\end{array}\\right]\\left[\\begin{array}{rr}2&5\\\\-3&-7\\end{array}\\right] = \\left[\\begin{array}{rr}1&0\\\\0&1\\end{array}\\right],\\]\nso we conclude that \\(C = A^{-1}.\\)\n\nLet’s think about what a matrix inverse does in a linear equation.\n\nTake a standard linear equation:\n\\[ A{\\bf x} = {\\bf b}. \\]\n\n\nThen:\n\\[A^{-1}(A{\\bf x}) = A^{-1}{\\bf b}\\]\n\n\n\\[(A^{-1}A){\\bf x} = A^{-1}{\\bf b}\\]\n\n\n\\[I{\\bf x} = A^{-1}{\\bf b}\\]\n\n\n\\[{\\bf x} = A^{-1}{\\bf b}\\]\n\nTheorem. If \\(A\\) is an invertible \\(n\\times n\\) matrix, then for each \\({\\bf b}\\) in \\(\\mathbb{R}^n,\\) the equation \\(A{\\bf x} = {\\bf b}\\) has the unique solution \\(A^{-1}{\\bf b}.\\)\nProof. Follows directly from the definition of \\(A^{-1}.\\)\n\nThis very simple, powerful theorem gives us a new way to solve a linear system.\n\n\nFurthermore, this theorem connects the matrix inverse to certain kinds of linear systems.\nWe know that not all linear systems of \\(n\\) equations in \\(n\\) variables have a unique solution.\nSuch systems may have no solutions (inconsistent) or an infinite number of solutions.\nBut this theorem says that if \\(A\\) is invertible, then the system has a unique solution.",
    "crumbs": [
      "The Inverse of a Matrix"
    ]
  },
  {
    "objectID": "L10MatrixInverse.html#computing-the-matrix-inverse",
    "href": "L10MatrixInverse.html#computing-the-matrix-inverse",
    "title": "Geometric Algorithms",
    "section": "Computing the Matrix Inverse",
    "text": "Computing the Matrix Inverse\nWonderful - so to solve a linear system, we simply need to compute the inverse of \\(A\\) (if it exists)!\nWell … how do we do that?\n\nThe \\(2\\times 2\\) case\nBefore answering this question for arbitrary matices, I will answer it for the special case of \\(2 \\times 2\\) matrices.\n\nTheorem. Let \\(A\\) = \\(\\left[\\begin{array}{rr}a&b\\\\c&d\\end{array}\\right].\\)\n\nIf \\(ad-bc \\neq 0\\), then \\(A\\) is invertible and \\(A^{-1} = \\frac{1}{ad-bc}\\left[\\begin{array}{rr}d&-b\\\\-c&a\\end{array}\\right].\\)\nIf \\(ad-bc = 0\\), then \\(A\\) is not invertible.\n\n\n\nNotice that this theorem tells us, for \\(2\\times 2\\) matrices, exactly which ones are invertible.\nNamely: those which have \\(ad-bc \\neq 0\\).\nOf course, we recognize the quantity \\(ad-bc\\)!\nIt is the determinant of \\(A\\).\n\nExample. Given a \\(2\\times 2\\) matrix \\(A\\), if the columns of \\(A\\) are linearly dependent, is \\(A\\) invertible?\n\nSolution. If the columns of \\(A\\) are linearly dependent, then at least one of the columns is a multiple of the other.\nLet the multiplier be \\(m.\\)\nThen we can express \\(A\\) as: \\(\\left[\\begin{array}{rr}a&ma\\\\b&mb\\end{array}\\right].\\)\n\n\nThe determinant of \\(A\\) is \\(a(mb) - b(ma) = 0.\\)\nSo a \\(2\\times 2\\) matrix with linearly dependent columns is not invertible.\n\n\n\nMatrices larger than \\(2 \\times 2\\).\n\nOK, now let’s look at a general method for computing the inverse of \\(A\\).\n\n\nRecall our definition of matrix multiplication: \\(AB\\) is the matrix formed by multiplying \\(A\\) times each column of \\(B\\).\n\\[ AB = [A{\\bf b_1} \\; \\dots \\; A{\\bf b_n}]. \\]\n\n\nLet’s look at the equation\n\\[AA^{-1} = I.\\]\n\n\nLet’s call the columns of \\(A^{-1}\\) = \\([{\\bf x_1}, {\\bf x_2}, \\dots, {\\bf x_n}].\\)\nWe know what the columns of \\(I\\) are: \\([{\\bf e_1}, {\\bf e_2}, \\dots, {\\bf e_n}].\\)\n\n\nSo:\n\\[ AA^{-1} = A[{\\bf x_1}, {\\bf x_2}, \\dots, {\\bf x_n}] = [{\\bf e_1}, {\\bf e_2}, \\dots, {\\bf e_n}].\\]\n\n\nNotice that we can break this up into \\(n\\) separate problems:\n\\[ A{\\bf x_1} = {\\bf e_1} \\] \\[ A{\\bf x_2} = {\\bf e_2} \\] \\[ \\vdots \\] \\[ A{\\bf x_n} = {\\bf e_n} \\]\n(This is a common trick … make sure you understand why it works!)\n\n\nSo here is a general way to compute the inverse of \\(A\\):\n\nSolve the linear system \\(A{\\bf x_1} = {\\bf e_1}\\) to get the first column of \\(A^{-1}.\\)\nSolve the linear system \\(A{\\bf x_2} = {\\bf e_2}\\) to get the second column of \\(A^{-1}.\\)\n\\(\\dots\\)\nSolve the linear system \\(A{\\bf x_n} = {\\bf e_n}\\) to get the last column of \\(A^{-1}.\\)\n\n\n\nIf any of the systems are inconsistent or has an infinite solution set, then \\(A^{-1}\\) does not exist.\n\n\nIn fact, the above procedure is equivalent to the following:\n\nConstruct the \\(n \\times 2n\\) matrix \\(B = [A \\;I]\\)\nFind the \\(C\\) = the reduced echelon form of \\(B\\)\nIn the resulting matrix \\(C\\):\n\nif the columns on the left half are \\(I\\), then\n\nthe columns in the right half of \\(C\\) will be \\(A^{-1}\\).\n\nOtherwise, \\(A\\) is not invertible.",
    "crumbs": [
      "The Inverse of a Matrix"
    ]
  },
  {
    "objectID": "L10MatrixInverse.html#the-computational-view",
    "href": "L10MatrixInverse.html#the-computational-view",
    "title": "Geometric Algorithms",
    "section": "The Computational View",
    "text": "The Computational View\n\nThe Operation Count of Matrix Inversion\nThus, when we perform Matrix Inversion on an \\(n\\times n\\) matrix, we are row reducing a \\(n\\times 2n\\) matrix.\nThis increased size results in the operation count of matrix inversion being \\(\\sim 2n^3.\\)\n(To see a derivation of this, check the lecture notes.)\nThis fact will be important!\n\n\nHere is more detail on the operation count of Matrix Inversion.\nTo do Matrix inversion, we perform row reduction on \\([A I]\\) to obtain \\([I A^{-1}]\\) as just described.   Since \\([A I]\\) is \\(n\\times 2n\\), the forward elimination step is \\(\\sim\\frac{5}{3}n^3\\) and the backsubstitution step is \\(\\sim\\frac{1}{3}n^3\\).  \nIn more detail:\nIf you go back to the derivation of the cost of Gaussian Elimination in Lecture 3, you need to extend the diagram.  It is no longer \\(n\\times(n+1)\\) but now is \\(n\\times 2n\\).  Then for the forward elimination phase of matrix inversion, you get:\n\\[ 2\\sum_{k=1}^n (k-1)(k+n) = 2\\sum_{k=1}^n k^2 + (n-1)k - n\\]\nflops.\nIf you expand this out, and use standard formulas for sums (eg, see https://brilliant.org/wiki/sum-of-n-n2-or-n3/), you will get the high order term of \\(\\sim\\frac{5}{3}n^3.\\)\nNow, for the back substitution phase, at the start you have a matrix that is \\([U L]\\) where \\(U\\) is upper triangular and \\(L\\) is lower triangular.  To backsubstitute row \\(k\\) in this matrix, you need\n\\[2 \\sum_{i=1}^{k} i = 2\\frac{(k-1)k}{2}\\]\nflops.   So the total for back substitution is\n\\[ \\sum_{k=1}^n k^2 - k \\] \nwhose highest order term is \\(\\frac{1}{3}n^3\\).  \nSo the total operation count of Matrix Inversion is\n\\[\\sim \\frac{5}{3}n^3 + \\frac{1}{3}n^3 =  2n^3\\]\n\n\nIn Code\nThis general strategy leads to an algorithm for inverting any matrix.\nHowever, in this course I will not ask you invert matrices larger than \\(2\\times 2\\) by hand.\nAny time you need to invert a matrix larger than \\(2\\times 2,\\) you may use a calculator or computer.\n\nTo invert a matrix in Python/numpy, use the function np.linalg.inv(). For example:\n\n\n\nimport numpy as np\nA = np.array(\n    [[ 2.0, 5.0],\n     [-3.0,-7.0]])\nprint('A =\\n',A)\nB = np.linalg.inv(A)\nprint('B = \\n',B)\n\nA =\n [[ 2.  5.]\n [-3. -7.]]\nB = \n [[-7. -5.]\n [ 3.  2.]]\n\n\n\nWhat do you think happens if you call np.linalg.inv() on a matrix that is not invertible?\n\n\nA = np.array([[2.,4.],[2.,4.]])\nnp.linalg.inv(A)\n\n\n---------------------------------------------------------------------------\nLinAlgError                               Traceback (most recent call last)\nCell In[3], line 2\n      1 A = np.array([[2.,4.],[2.,4.]])\n----&gt; 2 np.linalg.inv(A)\n\nFile ~/miniconda3/envs/quarto/lib/python3.12/site-packages/numpy/linalg/_linalg.py:615, in inv(a)\n    612 signature = 'D-&gt;D' if isComplexType(t) else 'd-&gt;d'\n    613 with errstate(call=_raise_linalgerror_singular, invalid='call',\n    614               over='ignore', divide='ignore', under='ignore'):\n--&gt; 615     ainv = _umath_linalg.inv(a, signature=signature)\n    616 return wrap(ainv.astype(result_t, copy=False))\n\nFile ~/miniconda3/envs/quarto/lib/python3.12/site-packages/numpy/linalg/_linalg.py:104, in _raise_linalgerror_singular(err, flag)\n    103 def _raise_linalgerror_singular(err, flag):\n--&gt; 104     raise LinAlgError(\"Singular matrix\")\n\nLinAlgError: Singular matrix\n\n\n\n\n\nThe right way to handle this is:\n\n\n\nA = np.array([[2.,4.],[2.,4.]])\ntry:\n    np.linalg.inv(A)\nexcept np.linalg.LinAlgError:\n    print('Oops, looks like A is singular!')\n\nOops, looks like A is singular!",
    "crumbs": [
      "The Inverse of a Matrix"
    ]
  },
  {
    "objectID": "L10MatrixInverse.html#using-the-matrix-inverse-to-solve-a-linear-system",
    "href": "L10MatrixInverse.html#using-the-matrix-inverse-to-solve-a-linear-system",
    "title": "Geometric Algorithms",
    "section": "Using the Matrix Inverse to Solve a Linear System",
    "text": "Using the Matrix Inverse to Solve a Linear System\n\nSolve the system:\n\\[\\begin{array}{rcl}\n3x_1 +4x_2 &=& 3\\\\\n5x_1 +6x_2 &=& 7\n\\end{array}\\]\n\n\nRewrite this system as \\(A{\\bf x} = {\\bf b}:\\)\n\\[ \\left[\\begin{array}{rr}3&4\\\\5&6\\end{array}\\right] {\\bf x} = \\left[\\begin{array}{r}3\\\\7\\end{array}\\right].\\]\n\n\nThe determinant of \\(A\\) is \\(3(6)-4(5) = -2,\\) which is nonzero, so \\(A\\) has an inverse.\nAccording to our \\(2\\times 2\\) formula, the inverse of \\(A\\) is:\n\\[ A^{-1} = \\frac{1}{-2}\\left[\\begin{array}{rr}6&-4\\\\-5&3\\end{array}\\right] = \\left[\\begin{array}{rr}-3&2\\\\5/2&-3/2\\end{array}\\right].\\]\n\n\nSo the solution is:\n\\[ {\\bf x} = A^{-1}{\\bf b} = \\left[\\begin{array}{rr}-3&2\\\\5/2&-3/2\\end{array}\\right]\\left[\\begin{array}{r}3\\\\7\\end{array}\\right] = \\left[\\begin{array}{r}5\\\\-3\\end{array}\\right].\\]",
    "crumbs": [
      "The Inverse of a Matrix"
    ]
  },
  {
    "objectID": "L10MatrixInverse.html#algebra-of-matrix-inverses",
    "href": "L10MatrixInverse.html#algebra-of-matrix-inverses",
    "title": "Geometric Algorithms",
    "section": "Algebra of Matrix Inverses",
    "text": "Algebra of Matrix Inverses\n\nTheorem.\n\nIf \\(A\\) is an invertible matrix, then \\(A^{-1}\\) is invertible, and\n\n\\[(A^{-1})^{-1} = A.\\]\n\n\n\nIf \\(A\\) is an invertible matrix, then so is \\(A^T,\\) and the inverse of \\(A^T\\) is the transpose of \\(A^{-1}.\\)\n\n\\[(A^T)^{-1} = (A^{-1})^T.\\]\n\n\n\nIf \\(A\\) and \\(B\\) are \\(n\\times n\\) invertible matrices, then so is \\(AB,\\) and the inverse of \\(AB\\) is the product of the inverses of \\(A\\) and \\(B\\) in the reverse order.\n\n\\[(AB)^{-1} = B^{-1}A^{-1}.\\]\n\n\nThe first two are straightforward. Let’s verify the last one because it shows some common calculation patterns:\n\\[(AB)(B^{-1}A^{-1})\\]\n\n\n\\[=A(BB^{-1})A^{-1}\\]\n\n\n\\[=AIA^{-1}\\]\n\n\n\\[=AA^{-1}\\]\n\n\n\\[=I.\\]",
    "crumbs": [
      "The Inverse of a Matrix"
    ]
  },
  {
    "objectID": "L10MatrixInverse.html#the-invertible-matrix-theorem",
    "href": "L10MatrixInverse.html#the-invertible-matrix-theorem",
    "title": "Geometric Algorithms",
    "section": "The Invertible Matrix Theorem",
    "text": "The Invertible Matrix Theorem\nEarlier we saw that if a matrix \\(A\\) is invertible, then \\(A{\\bf x} = {\\bf b}\\) has a unique solution for any \\({\\bf b}\\).\nThis suggests a deep connection between the invertibility of \\(A\\) and the nature of the linear system \\(A{\\bf x} = {\\bf b}.\\)\n\nIn fact, we are now at the point where we can collect together in a fairly complete way much of what we have learned about matrices and linear systems.\nThis remarkable collection of ten interrelated properties is called the Invertible Matrix Theorem (IMT).\n\n\nInvertible Matrix Theorem. Let \\(A\\) be a square \\(n\\times n\\) matrix.\nThen the following statements are equivalent; that is, they are either all true or all false:\n\n\n\n\\(A\\) is an invertible matrix.\n\n\n\n\n\\(A^T\\) is an invertible matrix.\n\nProof by direct construction: \\((A^T)^{-1} = (A^{-1})^T.\\)\n\n\n\n\n\nThe equation \\(A{\\bf x} = {\\bf b}\\) has a unique solution for each \\({\\bf b}\\) in \\(\\mathbb{R}^n.\\)\n\nAs already mentioned, we proved this above.\n\n\n\n\n\nA is row equivalent to the identity matrix.\n\nIf \\(A{\\bf x} = {\\bf b}\\) has a unique solution for any \\({\\bf b},\\) then the reduced row echelon form of \\(A\\) is \\(I\\).\n\n\n\n\n\nA has \\(n\\) pivot positions.\n\nFollows directly from the previous statement.\n\n\n\n\n\nThe equation \\(A{\\bf x} = {\\bf 0}\\) has only the trivial solution.\n\nIf \\(A{\\bf x} = {\\bf b}\\) has a unique solution for any \\({\\bf b},\\) then the unique solution for \\({\\bf b} = {\\bf 0}\\) must be \\({\\bf 0.}\\)\n\n\n\n\n\nThe columns of \\(A\\) form a linearly independent set.\n\nFollows directly the previous statement and the definition of linear independence.\n\n\n\n\n\nThe columns of \\(A\\) span \\(\\mathbb{R}^n.\\)\n\nFor any \\({\\bf b} \\in \\mathbb{R}^n,\\) there is a set of coefficients \\({\\bf x}\\) which can be used to construct \\({\\bf b}\\) from the columns of \\(A.\\)\n\n\n\n\n\nThe linear transformation \\({\\bf x} \\mapsto A{\\bf x}\\) maps \\(\\mathbb{R}^n\\) onto \\(\\mathbb{R}^n.\\)\n\nFollows directly from the previous statement.\n\n\n\n\n\nThe linear transformation \\({\\bf x} \\mapsto A{\\bf x}\\) is one-to-one.\n\nFollows directly from the fact that \\(A{\\bf x} = {\\bf b}\\) has a unique solution for any \\({\\bf b}.\\)\n\n\n\n\nThe arguments above show that if \\(A\\) is invertible, then all the other statements are true.\nIn fact, the converse holds as well: if \\(A\\) is not invertible, then all the other statements are false.\n(We will skip the proof of the converse, but it’s not difficult.)\n\nThis theorem has wide-ranging implications.\nIt divides the set of all \\(n\\times n\\) matrices into two disjoint classes:\n\nthe invertible (nonsingular) matrices, and\nthe noninvertible (singular) matrices.\n\n\nThe power of the IMT lies in the conections it provides among so many important concepts.\nFor example, notice how it connects linear independence of the columns of a matrix \\(A\\) to the existence of solutions to equations of the form \\(A{\\bf x} = {\\bf b}.\\)\nThis allows us to bring many tools to bear as needed to solve a problem.\n\n\nExample.\nDecide if \\(A\\) is invertible:\n\\[A = \\left[\\begin{array}{rrr}1&0&-2\\\\3&1&-2\\\\-5&-1&9\\end{array}\\right].\\]\n\n\nSolution.\n\\[A \\sim \\left[\\begin{array}{rrr}1&0&-2\\\\0&1&4\\\\0&-1&-1\\end{array}\\right] \\sim \\left[\\begin{array}{rrr}1&0&-2\\\\0&1&4\\\\0&0&3\\end{array}\\right].\\]\n\\(A\\) has three pivot positions and hence is invertible, by the IMT.\n\n\nExample.\nDecide if \\(A\\mathbf{x} = \\mathbf{b}\\) has a solution for all \\(\\mathbf{b}\\):\n\\[ A = \\left[\\begin{array}{rr}3 & 7\\\\-6 & -14\\end{array}\\right].\\]\n\n\nSolution.\nThe determinant of \\(A\\) is \\((3 \\cdot -14) - (7 \\cdot -6) = 0\\).\nSo \\(A\\) is not invertible, so \\(A\\mathbf{x} = \\mathbf{b}\\) does not have a solution for all \\(\\mathbf{b}\\).\n\nNote.\nKeep in mind: while the IMT is quite powerful, it does not completely settle issues that arise with respect to \\(A{\\bf x} = {\\bf b}.\\)\nThis is because it only applies to square matrices.\nSo if \\(A\\) is nonsquare, then we can’t use the IMT to conclude anything about the existence or nonexistence of solutions to \\(A{\\bf x} = {\\bf b}.\\)",
    "crumbs": [
      "The Inverse of a Matrix"
    ]
  },
  {
    "objectID": "L10MatrixInverse.html#invertible-linear-transformations",
    "href": "L10MatrixInverse.html#invertible-linear-transformations",
    "title": "Geometric Algorithms",
    "section": "Invertible Linear Transformations",
    "text": "Invertible Linear Transformations\n\n\nA linear transformation \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) is invertible if there exists a function \\(S: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) such that\n\\[ S(T({\\bf x})) = {\\bf x}\\;\\;\\;\\text{for all}\\;{\\bf x}\\in\\mathbb{R}^n,\\]\nand\n\\[ T(S({\\bf x})) = {\\bf x}\\;\\;\\;\\text{for all}\\;{\\bf x}\\in\\mathbb{R}^n.\\]\n\n\nTheorem.\nLet \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) be a linear transformation and let \\(A\\) be the standard matrix for \\(T\\).\nThen \\(T\\) is invertible if and only if \\(A\\) is an invertible matrix.\nIn that case the linear transformation \\(S\\) given by \\(S({\\bf x}) = A^{-1}{\\bf x}\\) is the unique function satisfying the definition.\n\nLet’s look at some invertible and non-invertible linear transformations.\n\n\nsquare = np.array([[0.0,1,1,0],[1,1,0,0]])\nA = np.array(\n    [[0.5, 0], \n     [  0, 1]])\nprint(A)\ndm.plotSetup()\ndm.plotSquare(square)\ndm.plotSquare(A @ square,'r')\nLatex(r'Horizontal Contraction')\n\n[[0.5 0. ]\n [0.  1. ]]\n\n\n&lt;IPython.core.display.Latex object&gt;\n\n\n\n\n\n\n\n\n\n\n\nHere \\(A = \\left[\\begin{array}{rr}0.5&0\\\\0&1\\end{array}\\right].\\) Its determinant is \\(1(0.5)-0(0) = 0.5,\\) so this linear transformation is invertible.\nIts inverse is:\n\\[ \\frac{1}{0.5}\\left[\\begin{array}{rr}1&0\\\\0&0.5\\end{array}\\right] = \\left[\\begin{array}{rr}2&0\\\\0&1\\end{array}\\right].\\]\n\n\nClearly, just as \\(A\\) contracted the \\(x_1\\) direction by 0.5, \\(A^{-1}\\) will expand the \\(x_1\\) direction by 2.\n\n\nA = np.array(\n    [[0,0],\n     [0,1]])\ndm.plotSetup()\ndm.plotSquare(A @ square)\nLatex(r'Projection onto the $x_2$ axis')\n\n&lt;IPython.core.display.Latex object&gt;\n\n\n\n\n\n\n\n\n\n\nHere \\(A = \\left[\\begin{array}{rr}0&0\\\\0&1\\end{array}\\right].\\)\nIts determinant is zero, so this linear transformation is not invertible.\n\n\nBy the IMT, there are many equivalent ways to describe this linear transformation:\n\nThe mapping \\(T\\) is not onto \\(\\mathbb{R}^2.\\) (Only a subset of \\(\\mathbb{R}^2\\) can be output by \\(T\\)).\nThe mapping \\(T\\) is not one-to-one. (There are many values \\({\\bf x}\\) that give the same \\(A{\\bf x}.\\))\n\\(A\\) does not have 2 pivots.\nThe columns of \\(A\\) do not span \\(\\mathbb{R}^2.\\)\n\\(A\\mathbf{x} = 0\\) has a non-trivial solution.\n\n\nHere is another example:\n\n\n\n\n\n\n\n\n\n\nIn this figure, we are looking at how the red points \\((x_1, x_2)\\) are mapped to the green points under the transformation\n\\[ \\left[\\begin{array}{r} x_1 \\\\ x_2 \\end{array}\\right]  \\mapsto \\left[\\begin{array}{rr} 1.04 & 0.52 \\\\ 0.52 & 0.26 \\end{array}\\right]\\left[\\begin{array}{r} x_1 \\\\ x_2 \\end{array}\\right].\\]\n\n\nWe notice a few things:\n\nThe green points appear to lie along a line\nThere are cases where more than one red point maps to the same green point\n\nHow do these two facts relate to:\n\nThe determinant of the matrix?\nThe invertibility of the transformation?",
    "crumbs": [
      "The Inverse of a Matrix"
    ]
  },
  {
    "objectID": "L10MatrixInverse.html#further-reading",
    "href": "L10MatrixInverse.html#further-reading",
    "title": "Geometric Algorithms",
    "section": " Further Reading ",
    "text": "Further Reading \nThis material is not required, but may interest you.\n\nIll-Conditioned Matrices\nThe notion of a matrix inverse has some complications when used in practice.\nAs we’ve noted, numerical computations are not always exact.\nIn particular, we often find that a - b(a/b) does not evaluate to exactly zero on a computer.\nFor similar reasons, a matrix which is actually singular may not appear to be so when used in a computation.\nThis happens because, for example, the determinant does not evaluate to exactly zero, even though it should.\nRecall that when we were implementing Gaussian elimination, we established a rule:\nIf a - b(a/b) &lt; epsilon for sufficiently small epsilon, we would treat that quantity as if it were zero.\nWe need an equivalent rule for matrices, so that we recognize when matrices are “effectively singular.”\nWhen a matrix \\(A\\) is “effectively singular” we should not try to solve \\(A{\\bf x} = {\\bf b}\\).\nThe value we use for this purpose is called the condition number.\nEvery matrix has a condition number.\nThe larger the condition number of a matrix, the closer the matrix is to being singular.\nA singular matrix has an infinite condition number.\nAt the other extreme, the condition number of the identity matrix is 1, which is the smallest possible value.\nHere is the point: a matrix with a very large condition number will behave much like a singular matrix in practice.\nSpecifically: one should not try to solve linear systems by computer when the matrix \\(A\\) has a very large condition number.\nHow large is large?\nIt depends, but as a rule of thumb a condition number of \\(10^8\\) or greater would be considered a large condition number.\nIf a matrix has a large condition number, we might say that it is “effectively singular.”\nThe most common way to put this is that the matrix is “ill-conditioned”.\nA matrix that has a large condition number can behave almost like it is singular.\nWe know that if \\(A\\) is a singular matrix, then \\(A{\\bf x}={\\bf b}\\) does not have a unique solution.\nIf on the other hand \\(A\\) is not singular, but is ill-conditioned, then solving \\(A{\\bf x}={\\bf b}\\) can be very inaccurate.\nA small change in \\({\\bf b}\\) (such as might be introduced by limited precision in your computer) will result in a huge change to the solution, \\({\\bf x}\\).\n\n\nDemonstration of Ill-Conditioning\nHere is a demonstration of why this is a problem.\nHere is a matrix that is singular:\n\\[M = \\left[\\begin{array}{rr}1&2\\\\2&4\\end{array}\\right].\\]\nYou can see that it is singular because the second column is a multiple of the first column, so\n\nthe determinant is zero\nthe columns are linearly dependent\nthere is only one pivot position\netc. (see the IMT!)\n\nHere is a matrix that is almost singular:\n\\[A = \\left[\\begin{array}{ll}1&2.0000000001\\\\2&4\\end{array}\\right].\\]\nThe second column is not a multiple of the first column, so technically this matrix is not singular.\nBut the second column is almost a multiple of the first column.\nThe determinant is -0.0000000002\nYou could say the determinant is “almost zero”.\nThis matrix is ill-conditioned.\nNow let’s solve \\(A{\\bf x} = {\\bf b}\\) using the ill-conditioned matrix \\(A.\\)\nFirst, let’s consider when \\({\\bf b} = \\left[\\begin{array}{r}1\\\\2\\end{array}\\right].\\)\nSolving \\(A{\\bf x} = \\left[\\begin{array}{r}1\\\\2\\end{array}\\right]\\) we get \\({\\bf x} = \\left[\\begin{array}{r}1\\\\0\\end{array}\\right].\\)\nNow, let’s change \\({\\bf b}\\) just a little bit, and solve again.\nLet’s set \\({\\bf b} = \\left[\\begin{array}{l}1\\\\2.01\\end{array}\\right].\\)\nSolving \\(A{\\bf x} = \\left[\\begin{array}{l}1\\\\2.01\\end{array}\\right]\\) we get \\({\\bf x} = \\left[\\begin{array}{r}100000000\\\\-50000000\\end{array}\\right].\\)\nNotice how a small change in \\({\\bf b}\\) resulted in a huge change in \\({\\bf x}.\\)\nThis is very bad!\nIt means that we cannot trust the solution – it could be wildly wrong due to small errors in the input!\nThis is happening because the matrix \\(A\\) is ill-conditioned – it has a large condition number.\nIn fact the condition number of \\(A\\) is about 12,500,000,000.\nNow, this situation would not be a problem … if you were always dealing with exact quantities in your computer.\nBut you are not.\nEvery floating point number has limited precision – a limited number of digits that can be stored.\nAs a result, there can be a small error in the value of any number stored in the computer.\nThis is not normally a problem – you would not typically notice it.\nBut if you are solving a system with a large condition number, the small error in \\({\\bf b}\\) can get expanded in a large error in \\({\\bf x}\\).\nThe error can be so large that the value you get for \\({\\bf x}\\) is completely wrong.\nTo compute the condition number of a matrix A in Python/numpy, use np.linalg.cond(A).\n\nA = np.array([[1, 2.0000000001],[2, 4]])\nnp.linalg.cond(A)\n\nnp.float64(125000556673.43701)",
    "crumbs": [
      "The Inverse of a Matrix"
    ]
  },
  {
    "objectID": "L19PageRank.html",
    "href": "L19PageRank.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Larry Page and Sergey Brin\n\n\n\n\nThe PageRank paper",
    "crumbs": [
      "PageRank"
    ]
  },
  {
    "objectID": "L19PageRank.html#pagerank",
    "href": "L19PageRank.html#pagerank",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Larry Page and Sergey Brin\n\n\n\n\nThe PageRank paper",
    "crumbs": [
      "PageRank"
    ]
  },
  {
    "objectID": "L19PageRank.html#the-history",
    "href": "L19PageRank.html#the-history",
    "title": "Geometric Algorithms",
    "section": "The History",
    "text": "The History\nToday we’ll study an algorithm that is probably important in your life: Google’s PageRank.\n\n\nMany parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nLet’s set the stage.\nThe World Wide Web started becoming widely used in 1994.\nBy 1998 the Web had become an indispensable information resource.\nHowever, the problem of effectively searching the Web for relevant information was not well addressed. A number of large search engines were available, with names that are now forgotten: Alta Vista, Lycos, Excite, and others.\n\nAt present, most of them are no longer in existence, because Google emerged in 1998 and came to dominate Web search almost overnight.\nHow did this happen?\n\nAs background: a typical search engine uses a two-step process to retrieve pages related to a user’s query.\nIn the first step, basic text processing is done to find all documents that contain the query terms. Due to the massive size of the Web, this first step can result in thousands to millions of retrieved pages related to the query.\nSome of these pages are important, but most are not.\n\nThe problem that Google solved better than it competitors is deciding the ordering in which the resulting search results are presented. This is crucial: a user wants to find the “correct” or “best” item at the top of the search results.\nBy displaying the most relevant pages at the top of the list returned each query, Google makes its search results very useful. The algorithm that gave Google this advantage is called PageRank.\n\nThe Insight\nAround 1998, the limitations of standard search engines, which just used term frequency, we becoming apparent. A number of researchers were thinking about using additional sources of information to “rate” pages.\n\nThe key idea that a number of researchers hit on was this: links are endorsements.\nWhen a first page contains a link to a second page, that is an indication that the author of the first page thinks the second page is worth looking at. If the first and second pages both contain the same query terms, it is likely that the second page is an important page with respect to that query term.\n\n\nConsider a set of web pages, for a single query term (say “car manufacturers”) with this linking structure:\n\n\n\nIt may be clear that the links between pages contain useful information. But what is the best way to extract that information in the form of rankings?\n\nHere is the strategy that Brin and Page used:\nFrom “The anatomy of a large-scale hypertextual Web search engine” (1998):\n\n\n\n\nToday we’ll study this algorithm, see how to implement it, and understand that what it is really about is Markov Chains and eigenvectors.",
    "crumbs": [
      "PageRank"
    ]
  },
  {
    "objectID": "L19PageRank.html#random-walks",
    "href": "L19PageRank.html#random-walks",
    "title": "Geometric Algorithms",
    "section": "Random Walks",
    "text": "Random Walks\nWe start with the notion of a random walk.\n\nA random walk is a model of many sorts of processes that occur on graphs.\nLet us fix a graph \\(G\\). A random walk models the movement of an object on this graph.\n\n\nWe assume that the object moves from node to node in \\(G\\), one move per time step \\(t.\\) At time \\(t\\) the object is at node \\(k\\) (say) and at the next time \\(t+1\\) it moves to another node chosen at random from among the outgoing edges.\n\nFor our initial discussion, we will assume that \\(G\\) is the line graph:\n\n\nThis is a graph in which each node is connected to two neighbors. It’s natural to identify the nodes with the integers \\(k = 1,\\dots,n.\\)\n\n\nAn example application of this model would be a waiting line (or ‘queue’) like at a grocery store. The current node corresponds to the number of people in the queue. Given some number of people in the queue, only one of two things happens: either a person leaves the queue or a person joins the queue.\nIn fact, this sort of model is used for jobs running in a CPU – it is studied in CS350.\n\nTo complete the definition, what happens at the endpoints of the graph (nodes 1 and \\(n\\)) must be specified.\n\nOne possibility is for the walker to remain fixed at that location. This is called a random walk with absorbing boundaries.\n\n\nAnother possibility is for the walker to bounce back one unit when an endpoint is reached. This is called a random walk with reflecting boundaries.\n\n\nWe can also set a particular probability \\(1-p\\) of moving “to the right” (from \\(k\\) to \\(k+1\\)) and \\(p\\) of moving “to the left” (from \\(k\\) to \\(k-1\\)).\n\nNow, here is a cool idea:\n\nWe can describe a random walk on \\(G\\) as a Markov chain.\n\n\nThe way to interpret the steady-state of the Markov chain in terms of the random walk is:\nLet the chain (random walk) start in the given state. At some long time in the future, make an observation of the state that the chain is in. Then the steady-state distribution gives, for each state, the probability that the chain is in that state when we make our observation.\n\nAs a reminder, recall these facts about a Markov Chain.\nFor a Markov Chain having transition matrix \\(P\\):\n\nThe largest eigenvalue of \\(P\\) is 1.\nIf \\(P\\) is regular, then\n\nThere is only one eigenvalue equal to 1\nThe chain will converge to the corresponding eigenvector as its unique steady-state.\n\n“\\(P\\) is regular” means that for some \\(k&gt;0\\), all entries in \\(P^k\\) are nonzero.\n\n\nAbsorbing Boundaries\n\nExample. A random walk on \\(\\{0,1,2,3,4\\}\\) with absorbing boundaries has a transition matrix of\n\\[P=\\begin{bmatrix}1&p&0&0&0\\\\0&0&p&0&0\\\\0&1-p&0&p&0\\\\0&0&1-p&0&0\\\\0&0&0&1-p&1\\end{bmatrix}\\]\n\n\n\n\nExample. (“Gambler’s Ruin”). Consider a very simple casino game. A gambler (with some money to lose) flips a coin and calls heads or tails. If the gambler is correct, she wins a dollar. If she is wrong, she loses a dollar. The gambler will quit the game when she has either won \\(n-1\\) dollars or lost all of her money.\n\nSuppose that \\(n=5\\) and the gambler starts with $2. The gambler’s winnings must move up or down one dollar with each coin flip, and once the gambler’s winnings reach 0 or 4, they do not change any more since the gambler has quit the game.\n\n\nSuch a process may be modeled as a random walk on \\(\\{0,1,2,3,4\\}\\) with absorbing boundaries. Since a move up or down is equally likely in this case, \\(p = 1/2\\).\n\n\nThis transition matrix is not regular. Why? Consider column 1.\n\n\nThere is not a unique steady-state to which the chain surely converges.\nHowever, it turns out there are two long-term possibilities for the walker, each corresponding to absorption at one boundary, and the walker will surely get stuck at one or the other.\nIn other words, in this case, there are two different eigenvectors corresponding to the eigenvalue 1. So the dimension of this eigenspace is two.\nIn terms of our problem, this means that the gambler eventually either wins or loses (as opposed to going up and down for an arbitrarily long time).\n\nUsing slightly more advanced methods, we can predict the probabilities of landing in one steady-state or the other.\nFor example, if \\(p=0.45\\), we find that the probability that the gambler will lose all her money to be \\(0.4\\).\n\n\n\np = 0.45\nA = np.array([\n    [1,   p,   0,   0,   0],\n    [0,   0,   p,   0,   0],\n    [0, 1-p,   0,   p,   0],\n    [0,   0, 1-p,   0,   0],\n    [0,   0,   0, 1-p,   1]\n])\n\nB = np.linalg.matrix_power(A, 100)\n\nsteady_state = B @ np.array([0, 0, 1, 0, 0])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Walks on Undirected Graphs\n\nNow let’s consider a random walk on a more interesting graph:\n\n\n\n\n\n\nThis graph is undirected – each edge can be followed in either direction.\nAgain, at each node there is an equal probability of departing to any adjacent node.\n\n\nThe transition matrix associated with a random walk on this graph is\n\\[P = \\begin{bmatrix}\n0&1/3&1/4&0&0&0&0\\\\\n1/2&0&1/4&0&1/2&0&0\\\\\n1/2&1/3&0&1&0&1/3&0\\\\\n0&0&1/4&0&0&0&0\\\\\n0&1/3&0&0&0&1/3&0\\\\\n0&0&1/4&0&1/2&0&1\\\\\n0&0&0&0&0&1/3&0\\end{bmatrix}\\]\nIt turns out that this matrix is regular (\\(P^3\\) has no zero entries.)\nHence, the associated Markov Chain converges to a single steady state.\n\n\nThe eigenvector corresponding to the eigenvalue of 1 is the steady-state of the Markov Chain.\nHence we can find that the steady-state is \\(\\frac{1}{16}\\begin{bmatrix}2\\\\3\\\\4\\\\1\\\\2\\\\3\\\\1\\end{bmatrix}.\\)\nThat is, the probability of bring in node 1 at steady state is 2/16; the probability of being in node 2 is 3/16; the probability of being in node 3 is 4/16, etc.\n\nNow, look at \\(G\\) again, and compare it to its steady-state distribution. Notice anything?\n\n\n\n\n\n\\[\\frac{1}{16}\\begin{bmatrix}2\\\\3\\\\4\\\\1\\\\2\\\\3\\\\1\\end{bmatrix}\\]\n\n\n\nThe steady-state distribution is proportional to the number of edges attached to the node.\nThe number of edges connected to a node is called the node degree.\n\n\nThis is not a coincidence!\nIn fact it can be proved that the steady-state distribution of a random walk on an undirected graph is proportional to node degree.\nThat is, the probability of being at a particular node at steady state is proportional to that node’s degree.\nThis is really amazing!\n\n\n\nRandom Walks on Directed Graphs\n\nMore interesting behavior arises when we walk randomly on a directed graph.\nIn this graph, all edges are “one-way streets” – nodes are joined not by lines but by arrows. The chain can move from vertex to vertex, but only in the directions allowed by the arrows.\n\n\nAn example of a directed graph is\n\n\n\n\n\nThe transition matrix for this graph is:\n\\[P = \\begin{bmatrix}\n0&0&1/3&0&0&0\\\\\n1/2&0&1/3&0&0&0\\\\\n1/2&0&0&0&0&0\\\\\n0&0&0&0&1/2&1\\\\\n0&0&1/3&1/2&0&0\\\\\n0&0&0&1/2&1/2&0\n\\end{bmatrix}\\]\n\n\nWe can conclude that this matrix is not regular. Why?\n\n\nOne reason we can conclude this is the column of zeros (column 2).\nAny power of \\(P\\) will preserve this column of zeros.",
    "crumbs": [
      "PageRank"
    ]
  },
  {
    "objectID": "L19PageRank.html#pagerank-1",
    "href": "L19PageRank.html#pagerank-1",
    "title": "Geometric Algorithms",
    "section": "PageRank",
    "text": "PageRank\nThere are many ways to use link structure to infer which pages are most important.\nThere was a lot of experimentation in the late 1990s with various methods.\n\nHere are some examples of link structures found in the Web.\nEach node is a Web page, and each edge is a link from one Web page to another.\n\n\n\n\n\n\n\n\nWhy did Page and Brin settle on the random walk as the basis for their approach?\nIn fact, the intuiution they started from was simpler:\n\nTheir insight was just to say that a page is ‘important’ if many ‘important’ pages link to it.\n\n\nMore precisely, this definition of ‘importance’ is:\n\\[\\small\\text{Importance of page $i$} = \\\\\n\\sum_j \\text{(Importance of page $j$)}\\cdot\\text{(Probability of going from page $j$ to page $i$.)}\\]\n\n\nThis is a very intuitive definition of importance.\nThere is a bit of a issue however – it is self-referential!\nThe ‘importance’ of a page appears on both sides of the equation.\nHow can we solve this equation to get a fixed ‘importance’ for a given page?\n\nAnswering this question is where the random walk comes in.\n\nLet’s define a vector \\(\\mathbf{x}\\) to hold importance:\n\\[ \\mathbf{x} = \\begin{bmatrix}\\text{importance of page 1}\\\\\\text{importance of page 2}\\\\\\vdots\\\\\\text{importance of page n}\\end{bmatrix} \\]\n\n\nAnd let’s define \\(P\\) as the transition matrix of the Markov Chain defined by the link structure of the pages:\n\\[ P_{ij} = \\text{probability of going from page } j \\text{ to page } i \\text{ in a random walk}\\]\n\n\nWhat Page and Brin observed was that this equation\n\\[\\small\\text{Importance of page $i$} = \\] \\[ \\sum_j \\text{(Importance of page $j$)}\\cdot\\text{(Probability of going from page $j$ to page $i$.)}\\]\n\n\nis the same as:\n\\[\\mathbf{x} = P\\mathbf{x}\\]\n\n\nWhich is the equation defining the steady state of the Markov chain \\(P\\).\n\nNow we are ready to understand what Page and Brin were saying in 1998:\n\nPageRank can be thought of as a model of user behavior. We assume there is a “random surfer” who is given a web page at random and keeps clicking on links, never hitting “back” but eventually gets bored and starts on another random page. The probability that the random surfer visits a page is its PageRank.\n\n\nWhat they are implying is that a random surfer should visit important pages more often and unimportant pages less often.\n\nThe way to interpret this precisely is:\n  1. Form the graph that encodes the connections between Web pages that are retrieved for a particular query.\n\n  2. Construct a Markov chain that corresponds to a random walk on this graph.\n\n\n  3. Rank-order the pages according to their probability in the Markov chain’s steady state.\n\n\n\nThere is a nice visualization here.\n\nSo let’s try to make this work and see what happens.\n\n\nExample.\nAssume a set of Web pages have been selected based on a text query, eg, pages related to “personal 737 jets.”\nThese pages have various links between them, as represented by this graph:\n\n\n\n\nLet’s construct the unique steady-state distribution for a random walk on this graph, if it exists. That is, we will construct the PageRank for this set of Web pages.\n\n\nThe key question we must ask is whether a unique steady state exists.\n\n\nStep 1.\nAssume there are \\(n\\) pages to be ranked. Construct an \\(n\\times n\\) transition matrix for the Markov chain.\nSet the Markov chain transitions so that each outgoing link from a node has equal probability of being taken.\n\nWe have already seen the transition matrix for this graph:\n\\[P = \\begin{bmatrix}\n0&0&1/3&0&0&0\\\\\n1/2&0&1/3&0&0&0\\\\\n1/2&0&0&0&0&0\\\\\n0&0&0&0&1/2&1\\\\\n0&0&1/3&1/2&0&0\\\\\n0&0&0&1/2&1/2&0\n\\end{bmatrix}\\]\nWe have observed that this transition matrix is not regular, because for any \\(P^k, k&gt;0,\\) the second column will be zero.\n\n\nTo address this, let’s ask why it happens.\nThe reason that column 2 of \\(P\\) is zero is that the Web page corresponding to node 2 has no links embedded in it, so there is nowhere to go from this page. Of course this will happen a lot in an arbitrary collection of Web pages.\n\n\nNote that Page and Brin say that the random surfer will occasionally “start on another random page.” In other words, it seems reasonable that when reaching a page with no embedded links, the surfer chooses another page at random.\nSo this motivates the first adjustment to \\(P\\):\n\n\n\nStep 2.\nForm the matrix \\(P'\\) as follows: for each column in \\(P\\) that is entirely zeros, replace it with a column in which each entry is \\(1/n\\).\nIn our example:\n\\[\\tiny P = \\begin{bmatrix}\n0&0&1/3&0&0&0\\\\\n1/2&0&1/3&0&0&0\\\\\n1/2&0&0&0&0&0\\\\\n0&0&0&0&1/2&1\\\\\n0&0&1/3&1/2&0&0\\\\\n0&0&0&1/2&1/2&0\n\\end{bmatrix} \\;\\;{\\rightarrow}\\;\\;\nP' = \\begin{bmatrix}\n0&1/n&1/3&0&0&0\\\\\n1/2&1/n&1/3&0&0&0\\\\\n1/2&1/n&0&0&0&0\\\\\n0&1/n&0&0&1/2&1\\\\\n0&1/n&1/3&1/2&0&0\\\\\n0&1/n&0&1/2&1/2&0\n\\end{bmatrix}\\;\\;=\\;\\;\n\\begin{bmatrix}\n0&1/6&1/3&0&0&0\\\\\n1/2&1/6&1/3&0&0&0\\\\\n1/2&1/6&0&0&0&0\\\\\n0&1/6&0&0&1/2&1\\\\\n0&1/6&1/3&1/2&0&0\\\\\n0&1/6&0&1/2&1/2&0\n\\end{bmatrix}\\]\n\nNonetheless, even after this change, \\(P'\\) can fail to be regular.\nIn other words, for an arbitrary set of web pages, there is no guarantee that their transition matrix will be regular.\n\n\nOnce again, let’s read the words of Page and Brin closely: the surfer “eventually gets bored and starts on another random page.”\n\n\n\nStep 3.\nIn practice this means that there a small probability that the surfer will jump from any page to any other page at random.\nLet’s call this small probability \\(\\alpha.\\)\n\nWe can’t just add \\(\\alpha\\) to every entry in \\(P'\\), because then the columns of the new matrix would not sum to 1.\n\n\nInstead we decrease each entry in \\(P'\\) by a factor of \\((1-\\alpha)\\), and then add \\({\\alpha}/{n}\\) to it.\nSo we compute the final transition matrix \\(P''\\) as:\n\\[P''_{ij} = (1-\\alpha)P'_{ij} + \\frac{\\alpha}{n}.\\]\n\n\nWe can write this as a matrix equation:\n\\[P'' = (1-\\alpha)P' + \\frac{\\alpha}{n} \\mathbf{1}\\]\nwhere \\(\\mathbf{1}\\) is an \\(n\\times n\\) matrix of 1’s.\n\n\nIn our example, let’s say that \\(\\alpha = 1/10\\) (in reality it would be smaller). So \\(\\alpha/n = 1/60.\\)\nThen:\n\\[ \\tiny P' \\begin{bmatrix}\n0&1/6&1/3&0&0&0\\\\\n1/2&1/6&1/3&0&0&0\\\\\n1/2&1/6&0&0&0&0\\\\\n0&1/6&0&0&1/2&1\\\\\n0&1/6&1/3&1/2&0&0\\\\\n0&1/6&0&1/2&1/2&0\n\\end{bmatrix} \\;\\;{\\rightarrow}\\;\\; P'' = \\begin{bmatrix}\n1/60&1/6&19/60&1/60&1/60&1/60\\\\\n7/15&1/6&19/60&1/60&1/60&1/60\\\\\n7/15&1/6&1/60&1/60&1/60&1/60\\\\\n1/60&1/6&1/60&1/60&7/15&11/12\\\\\n1/60&1/6&19/60&7/15&1/60&1/60\\\\\n1/60&1/6&1/60&7/15&7/15&1/60\n\\end{bmatrix}\\]\n\n\nObviously, \\(P''\\) is regular, because all its entries are positive (they are at least \\(\\alpha/n.\\))\n\n\n\\(P''\\) is the Markov Chain that Brin and Page defined, and which is used by PageRank to rank pages in response to a Google search.\n\n\n\nStep 4.\nCompute the steady-state of \\(P''\\), and rank pages according to their magnitude in the resulting vector.\nWe can do this by solving \\(P''\\mathbf{x} = \\mathbf{x}\\), or we can compute the eigenvectors of \\(P''\\) and use the eigenvector that corresponds to \\(\\lambda = 1.\\)\n\nFor the example \\(P''\\), we find that the steady-state vector is:\n\\(\\mathbf{x} = \\begin{bmatrix}0.037\\\\0.054\\\\0.041\\\\0.375\\\\0.206\\\\0.286\\end{bmatrix}\\)\n\n\nSo the final ranking of pages is: 4, 6, 5, 2, 3, 1.\nThis is the order that PageRank would display its results, with page 4 at the top of the list.\n\nLet’s see how to do Step 4 in Python:\n\n\n# Here is the P'' matrix as computed in steps 1 through 3.\nP = np.array([\n[1./60, 1./6, 19./60, 1./60, 1./60,  1./60],\n[7./15, 1./6, 19./60, 1./60, 1./60,  1./60],\n[7./15, 1./6,  1./60, 1./60, 1./60,  1./60],\n[1./60, 1./6,  1./60, 1./60, 7./15, 11./12],\n[1./60, 1./6, 19./60, 7./15, 1./60,  1./60],\n[1./60, 1./6,  1./60, 7./15, 7./15,  1./60]\n])\neigenvalues, eigenvectors = np.linalg.eig(P)\nprint(np.real(eigenvalues))\n\n[ 1.          0.61008601 -0.08958752 -0.37049849 -0.45       -0.45      ]\n\n\n\n\n\n# find the location of the largest eigenvalue (1), \n# by computing the indices that would sort the eigenvalues\n# from smallest to largest\nindices = np.argsort(eigenvalues)\n# and take the index of the largest eigenvalue\nprincipal = indices[-1]\nprint(principal)\n\n0\n\n\n\n\n\n# using the index of the largest eigenvalue, extract\n# the corresponding eigenvector (the steady state vector)\nsteadyState = np.real(eigenvectors[:,principal])\nsteadyState = steadyState/np.sum(steadyState)\nprint(steadyState)\n\n[0.03721197 0.05395735 0.04150565 0.37508082 0.20599833 0.28624589]\n\n\n\n\n\n# find the order of the pages in the steady state vector\n# this function sorts from smallest to largest (reverse of what we want)\nreverseOrder = np.argsort(steadyState)\nprint(reverseOrder)\n\n[0 2 1 4 5 3]\n\n\n\n\n\n# reverse the order to get the most important page first\n# and add one to convert from zero indexing to indexing of example\norder = 1 + reverseOrder[::-1]\nprint('final order = {}'.format(order))\nprint('importance = {}'.format(steadyState[order-1]))\n\nfinal order = [4 6 5 2 3 1]\nimportance = [0.37508082 0.28624589 0.20599833 0.05395735 0.04150565 0.03721197]",
    "crumbs": [
      "PageRank"
    ]
  },
  {
    "objectID": "L19PageRank.html#computing-pagerank-the-power-method",
    "href": "L19PageRank.html#computing-pagerank-the-power-method",
    "title": "Geometric Algorithms",
    "section": "Computing PageRank: the Power Method",
    "text": "Computing PageRank: the Power Method\nFrom a mathematical standpoint, we are done!\n\nHowever, from a Computer Science standpoint, there are still some issues.\n\n\nThe most significant issue is simply this: PageRank results must be provided very quickly. Search engines are in competition and speed is a competitive advantage.\n\n\nHere is an example Google search:\n\n\n\n\n\nNotice that the search returned about 400,000 results!\n\n\nRecall that using Gaussian elimination to solve \\(A\\mathbf{x} = \\mathbf{b}\\) takes \\(\\sim \\frac{2}{3}n^3\\) operations.\n\n\nIn this case, apparently \\(n = 400,000.\\)\nSo computing the PageRank in the straightforward way we’ve described would take about 42,667,000,000,000,000 operations.\nAssuming a 2GHz CPU, that’s on the order of eight months.\n\n\n\n\n((2./3)*(400000**3))/((2*10**9)*(3600*24*30))\n\n8.23045267489712\n\n\n\nWe need a faster way to compute the PageRank!\n\nHere is an important insight: we only need the principal eigenvector. (The one corresponding to \\(\\lambda = 1\\)).\n\nLet’s review how a Markov chain gets to steady state. As we discussed at the end of the lecture on the characteristic equation, the state of the chain at any step \\(k\\) is given by\n\\[{\\bf x_k} = c_1{\\bf v_1}\\lambda_1^k + c_2{\\bf v_2}\\lambda_2^k + \\dots + c_n{\\bf v_n}\\lambda_n^k.\\]\n\n\nLet’s assume that \\(\\lambda_1\\) is the eigenvalue 1. If the chain converges to steady sate, then we know that all eigenvalues other than \\(\\lambda_1\\) are less than 1 in magnitude.\nOf course, if \\(|\\lambda_i| &lt; 1,\\)\n\\[\\lim_{k\\rightarrow\\infty} \\lambda_i^k = 0.\\]\n\n\nSo:\n\\[\\lim_{k\\rightarrow\\infty}{\\bf x_k} = c_1{\\bf v_1}.\\]\nNote that \\(c_1\\) is just a constant that doesn’t affect the relative sizes of the components of \\({\\mathbf{x}_k}\\) in the limit of large \\(k.\\)\n\n\nThis is another way of stating that the Markov chain goes to steady state no matter what the starting state is.\n\nThis observation suggests another way to compute the steady state of the chain:\n\nStart from a random state \\(\\mathbf{x}_0\\).\nCompute \\(\\mathbf{x}_{k+1} = A\\mathbf{x}_k\\) for \\(k = 0,1,2,3,\\dots\\)\n\n\nHow do we know when to stop in Step 2?\nSince we are looking for steady-state, we can stop when the difference between \\(\\mathbf{x}_{k+1}\\) and \\(\\mathbf{x}_k\\) is small.\n\n\nThis is called the power method.\n\n\nWhy is this a better method?\n\n\nKeep in mind that the number of flops in matrix-vector multiplication is \\(\\sim 2n^2\\).\nThis is compared to \\(\\sim \\frac{2}{3}n^3\\) for solving the system (finding the eigenvector directly).\n\n\nLet’s say that after computing\n\\[\\mathbf{x}_1 = A \\mathbf{x}_0\\]\n\\[\\mathbf{x}_2 = A \\mathbf{x}_1\\]\n\\[\\mathbf{x}_3 = A \\mathbf{x}_2\\]\n\\[\\mathbf{x}_4 = A \\mathbf{x}_3\\]\n\\[\\mathbf{x}_5 = A \\mathbf{x}_4\\]\n\\[\\mathbf{x}_6 = A \\mathbf{x}_5\\]\n\\[\\mathbf{x}_7 = A \\mathbf{x}_6\\]\n\\[\\mathbf{x}_8 = A \\mathbf{x}_7\\]\n\\[\\mathbf{x}_9 = A \\mathbf{x}_8\\]\n\\[\\mathbf{x}_{10} = A \\mathbf{x}_9\\]\nwe find that \\(\\mathbf{x}_{10}\\) is sufficiently close to \\(\\mathbf{x}_9.\\)\n\n\nHow much work did we do?\nWe did 10 matrix-vector multiplications, or \\(20n^2\\) flops.\n\n\nSo the power method is\n\\[\\frac{\\frac{2}{3}n^3}{20n^2} = \\frac{n}{30}\\]\ntimes faster than the direct method.\n\n\nFor our example, \\(n/30 = 13,333\\). So this trick reduces the running time from 8 months down to 27 minutes.\n\n\n\n\n20*400000.**2/((2*10**9)*(60))\n\n26.666666666666668\n\n\nThis is an example of an iterative method. Iterative methods are often the preferred approach for solving linear algebra problems in the real world.\n\nOne final thing: how exactly do we decide when to stop iterating in the power method?\n\n\nOne simple way is to add up the differences of the components of \\(\\mathbf{x}_{k+1}-\\mathbf{x}_k\\):\n\\[ s = \\sum_{i=1}^n |\\mathbf{x}_{k+1,i} - \\mathbf{x}_{k,i}| \\]\n\n\nand compare it to the sum of the components of \\(\\mathbf{x}_k\\):\n\\[ d = \\sum_{i=1}^n |\\mathbf{x}_{k,i}| \\]\n\n\nIf \\(s/d\\) is small (say, less than 0.001) then we can conclude that \\(\\mathbf{x}_{k+1}\\) is close enough to \\(\\mathbf{x}_k\\) for us to stop iterating.\n\n\nSo the power method is fast, making it the algorithm of choice for a company like Google. It is also easy to implement, and easy to parallelize across multiple machines.",
    "crumbs": [
      "PageRank"
    ]
  },
  {
    "objectID": "L18Diagonalization.html",
    "href": "L18Diagonalization.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\nAnyone who understands algebraic notation, reads at a glance in an equation results reached arithmetically only with great labour and pains.\n— Antoine-Augustin Cournot, 1838\n\n\nToday we consider an important factorization of a square matrix.\nThis factorization uses eigenvalues and eigenvectors, and makes many problems substantially easier.\nFurthermore, it gives fundamental insight into the properties of a matrix.\n\nGiven a square matrix \\(A\\), the factorization of \\(A\\) is of the form\n\\[A = PDP^{-1}\\]\nwhere \\(D\\) is a diagonal matrix.\n\nRecall from last lecture, that the above equation means that \\(A\\) and \\(D\\) are similar matrices.\nSo this factorization amounts to finding a \\(P\\) that allows us to make \\(A\\) similar to a diagonal matrix.\n\n\nThis factorization allows us to\n\nrepresent \\(A\\) in a form that exposes the properties of \\(A\\),\nrepresent \\(A^k\\) in an easy to use form, and\ncompute \\(A^k\\) quickly for large values of \\(k\\).\n\n\n\nLet’s look at an example to show why this factorization is so important.",
    "crumbs": [
      "Diagonalization"
    ]
  },
  {
    "objectID": "L18Diagonalization.html#diagonalization",
    "href": "L18Diagonalization.html#diagonalization",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\nAnyone who understands algebraic notation, reads at a glance in an equation results reached arithmetically only with great labour and pains.\n— Antoine-Augustin Cournot, 1838\n\n\nToday we consider an important factorization of a square matrix.\nThis factorization uses eigenvalues and eigenvectors, and makes many problems substantially easier.\nFurthermore, it gives fundamental insight into the properties of a matrix.\n\nGiven a square matrix \\(A\\), the factorization of \\(A\\) is of the form\n\\[A = PDP^{-1}\\]\nwhere \\(D\\) is a diagonal matrix.\n\nRecall from last lecture, that the above equation means that \\(A\\) and \\(D\\) are similar matrices.\nSo this factorization amounts to finding a \\(P\\) that allows us to make \\(A\\) similar to a diagonal matrix.\n\n\nThis factorization allows us to\n\nrepresent \\(A\\) in a form that exposes the properties of \\(A\\),\nrepresent \\(A^k\\) in an easy to use form, and\ncompute \\(A^k\\) quickly for large values of \\(k\\).\n\n\n\nLet’s look at an example to show why this factorization is so important.",
    "crumbs": [
      "Diagonalization"
    ]
  },
  {
    "objectID": "L18Diagonalization.html#powers-of-a-diagonal-matrix",
    "href": "L18Diagonalization.html#powers-of-a-diagonal-matrix",
    "title": "Geometric Algorithms",
    "section": "Powers of a Diagonal Matrix",
    "text": "Powers of a Diagonal Matrix\nConsider taking the powers of a diagonal matrix.\nFor example, \\(D = \\begin{bmatrix}5&0\\\\0&3\\end{bmatrix}.\\)\n\nThen note that \\(D^2 = \\begin{bmatrix}5&0\\\\0&3\\end{bmatrix}\\begin{bmatrix}5&0\\\\0&3\\end{bmatrix} = \\begin{bmatrix}5^2&0\\\\0&3^2\\end{bmatrix},\\)\n\n\nAnd \\(D^3 = DD^2 = \\begin{bmatrix}5&0\\\\0&3\\end{bmatrix}\\begin{bmatrix}5^2&0\\\\0&3^2\\end{bmatrix} = \\begin{bmatrix}5^3&0\\\\0&3^3\\end{bmatrix}.\\)\n\n\nSo in general,\n\\[ D^k = \\begin{bmatrix}5^k&0\\\\0&3^k\\end{bmatrix} \\;\\;\\;\\text{for}\\;k\\geq1.\\]\n\n\nExtending to a general matrix \\(A\\)\nNow, consider if \\(A\\) is similar to a diagonal matrix.\nFor example, let \\(A = PDP^{-1}\\) for some invertible \\(P\\) and diagonal \\(D\\).\n\nThen, \\(A^k\\) is also easy to compute.\n\nExample.\nLet \\(A = \\begin{bmatrix}7&2\\\\-4&1\\end{bmatrix}.\\)\nFind a formula for \\(A^k,\\) given that \\(A = PDP^{-1},\\) where\n\\[P = \\begin{bmatrix}1&1\\\\-1&-2\\end{bmatrix}\\;\\text{and}\\;D = \\begin{bmatrix}5&0\\\\0&3\\end{bmatrix}.\\]\n\nSolution.\nThe standard formula for the inverse of a \\(2\\times 2\\) matrix yields\n\\[P^{-1} = \\begin{bmatrix}2&1\\\\-1&-1\\end{bmatrix}\\]\n\n\nThen, by associativity of matrix multiplication,\n\\[A^2 = (PDP^{-1})(PDP^{-1}) \\]\n\n\n\\[= PD(P^{-1}P)DP^{-1}\\]\n\n\n\\[ = PDDP^{-1}\\]\n\n\n\\[ = PD^2P^{-1}\\]\n\n\n\\[ = \\begin{bmatrix}1&1\\\\-1&-2\\end{bmatrix}\\begin{bmatrix}5^2&0\\\\0&3^2\\end{bmatrix}\\begin{bmatrix}2&1\\\\-1&-1\\end{bmatrix}\\]\n\n\nSo in general, for \\(k\\geq 1,\\)\n\\[A^k = PD^kP^{-1}\\]\n\n\n\\[=\\begin{bmatrix}1&1\\\\-1&-2\\end{bmatrix}\\begin{bmatrix}5^k&0\\\\0&3^k\\end{bmatrix}\\begin{bmatrix}2&1\\\\-1&-1\\end{bmatrix}\\]\n\n\n\\[=\\begin{bmatrix}2\\cdot5^k-3^k&5^k-3^k\\\\2\\cdot3^k-2\\cdot5^k&2\\cdot3^k-5^k\\end{bmatrix}\\]\n\nNow, it is important to understand:\n\nThis factorization may not always be possible.\n\n\nHence, we have a definition:\nA square matrix \\(A\\) is said to be diagonalizable if \\(A\\) is similar to a diagonal matrix.\n\n\nThat is, \\(A\\) is diagonalizable if we can find some invertible \\(P\\) such that\n\\[A = PDP^{-1}\\]\nand \\(D\\) is a diagonal matrix.",
    "crumbs": [
      "Diagonalization"
    ]
  },
  {
    "objectID": "L18Diagonalization.html#diagonalization-requires-eigenvectors-and-eigenvalues",
    "href": "L18Diagonalization.html#diagonalization-requires-eigenvectors-and-eigenvalues",
    "title": "Geometric Algorithms",
    "section": "Diagonalization Requires Eigenvectors and Eigenvalues",
    "text": "Diagonalization Requires Eigenvectors and Eigenvalues\nNext we will show that to diagonalize a matrix, one must use the eigenvectors and eigenvalues of \\(A\\).\nTheorem. (The Diagonalization Theorem)\nAn \\(n\\times n\\) matrix \\(A\\) is diagonalizable if and only if \\(A\\) has \\(n\\) linearly independent eigenvectors.\n\nIn fact,\n\\[A = PDP^{-1},\\]\nwith \\(D\\) a diagonal matrix,\nif and only if the columns of \\(P\\) are \\(n\\) linearly independent eigenvectors of \\(A.\\)\n\n\nIn this case, the diagonal entries of \\(D\\) are eigenvalues of \\(A\\) that correspond, respectively, to the eigenvectors in \\(P\\).\n\n\nIn other words, \\(A\\) is diagonalizable if and only if there are enough eigenvectors to form a basis of \\(\\mathbb{R}^n\\).\nWe call such a basis an eigenvector basis or an eigenbasis of \\(\\mathbb{R}^n\\).\n\nProof.\nFirst, we prove the “only if” part: if \\(A\\) is diagonalizable, it has \\(n\\) linearly independent eigenvectors.\n\nObserve that if \\(P\\) is any \\(n\\times n\\) matrix with columns \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_n,\\) then\n\\[AP = A[\\mathbf{v}_1\\;\\mathbf{v}_2\\;\\cdots\\;\\mathbf{v}_n] = [A\\mathbf{v}_1\\;A\\mathbf{v}_2\\;\\cdots\\;A\\mathbf{v}_n]\\]\n\n\nnext, note if \\(D\\) is any diagonal matrix with diagonal entries \\(\\lambda_1,\\dots,\\lambda_n,\\)\n\\[PD = P\\begin{bmatrix}\\lambda_1&0&\\cdots&0\\\\0&\\lambda_2&\\cdots&0\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\0&0&\\cdots&\\lambda_n\\end{bmatrix} = [\\lambda_1\\mathbf{v}_1\\;\\lambda_2\\mathbf{v}_2\\;\\cdots\\;\\lambda_n\\mathbf{v}_n].\\]\n\n\nNow suppose \\(A\\) is diagonalizable and \\(A = PDP^{-1}.\\) Then right-multiplying this relation by \\(P\\), we have\n\\[AP = PD\\]\n\n\nIn this case, the calculations above show that\n\\[[A\\mathbf{v}_1\\;A\\mathbf{v}_2\\;\\cdots\\;A\\mathbf{v}_n] = [\\lambda_1\\mathbf{v}_1\\;\\lambda_2\\mathbf{v}_2\\;\\cdots\\;\\lambda_n\\mathbf{v}_n].\\]\n\n\nEquating columns, we find that\n\\[A\\mathbf{v}_1 = \\lambda_1\\mathbf{v}_1, \\;\\;\\; A\\mathbf{v}_2 = \\lambda_2\\mathbf{v}_2, \\;\\;\\; \\dots, \\;\\;\\; A\\mathbf{v}_n = \\lambda_n\\mathbf{v}_n\\]\n\n\nSince \\(P\\) is invertible, its columns \\(\\mathbf{v}_1, \\dots,\\mathbf{v}_n\\) must be linearly independent.\nAlso, since these columns are nonzero, the equations above show that \\(\\lambda_1, \\dots, \\lambda_n\\) are eigenvalues and \\(\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\) are the corresponding eigenvectors.\nThis proves the “only if” part of the theorem.\n\nThe “if” part of the theorem is: if \\(A\\) has \\(n\\) linearly independent eigenvectors, \\(A\\) is diagonalizable.\nThis is straightforward: given \\(A\\)’s \\(n\\) eigenvectors \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_n,\\) use them to construct the columns of \\(P\\) and use corresponding eigenvalues \\(\\lambda_1, \\dots, \\lambda_n\\) to construct \\(D\\).\n\nUsing the sequence of equations above in reverse order, we can go from\n\\[A\\mathbf{v}_1 = \\lambda_1\\mathbf{v}_1, \\;\\;\\; A\\mathbf{v}_2 = \\lambda_2\\mathbf{v}_2, \\;\\;\\; \\dots, \\;\\;\\; A\\mathbf{v}_n = \\lambda_n\\mathbf{v}_n\\]\nto\n\\[AP = PD.\\]\n\n\nSince the eigenvectors are given as linearly independent, \\(P\\) is invertible and so\n\\[A = PDP^{-1}.\\]\n\nThe takeaway is this:\nEvery \\(n\\times n\\) matrix having \\(n\\) linearly independent eigenvectors can be factored into the product of\n\na matrix \\(P\\),\na diagonal matrix \\(D\\), and\nthe inverse of \\(P\\)\n\n… where \\(P\\) holds the eigenvectors of \\(A\\), and \\(D\\) holds the eigenvalues of \\(A\\).\nThis is the eigendecomposition of \\(A\\).\n(It is quite fundamental!)",
    "crumbs": [
      "Diagonalization"
    ]
  },
  {
    "objectID": "L18Diagonalization.html#diagonalizing-a-matrix",
    "href": "L18Diagonalization.html#diagonalizing-a-matrix",
    "title": "Geometric Algorithms",
    "section": "Diagonalizing a Matrix",
    "text": "Diagonalizing a Matrix\nLet’s put this all together and see how to diagonalize a matrix.\n\n\nFour Steps to Diagonalization\n\n\n\nExample. Diagonalize the following matrix, if possible.\n\\[A = \\begin{bmatrix}1&3&3\\\\-3&-5&-3\\\\3&3&1\\end{bmatrix}\\]\nThat is, find an invertible matrix \\(P\\) and a diagonal matrix \\(D\\) such that \\(A = PDP^{-1}.\\)\n\n\n\nStep 1: Find the eigenvalues of \\(A\\).\nThis is routine for us now. If we are working with \\(2\\times2\\) matrices, we may choose to find the roots of the characteristic polynomial (quadratic). For anything larger we’d use a computer.\n\n\n\nIn this case, the characteristic equation turns out to involve a cubic polynomial that can be factored:\n\\[0 = \\det(A-\\lambda I) \\]\n\\[ = -\\lambda^3 - 3\\lambda^2 + 4\\]\n\\[ = -(\\lambda -1)(\\lambda +2)^2\\]\n\n\nSo the eigenvalues are \\(\\lambda = 1\\) and \\(\\lambda = -2\\) (with multiplicity two).\n\n\n\nStep 2: Find three linearly independent eigenvectors of \\(A\\).\nNote that we need three linearly independent vectors because \\(A\\) is \\(3\\times3.\\)\n\n\n\n\nThis is the step at which, if A cannot be diagonalized, we find out\n\n… because we cannot find 3 linearly independent eigenvectors.\n\n\nUsing our standard method (finding the nullspace of \\(A - \\lambda I\\)) we find a basis for each eigenspace:\nBasis for \\(\\lambda = 1\\): \\(\\mathbf{v}_1 = \\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}.\\)\nBasis for \\(\\lambda = -2\\): \\(\\mathbf{v}_2 = \\begin{bmatrix}-1\\\\1\\\\0\\end{bmatrix}\\) and \\(\\mathbf{v}_3 = \\begin{bmatrix}-1\\\\0\\\\1\\end{bmatrix}.\\)\n\n\nAt this point we must ensure that \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}\\) forms a linearly independent set.\n(These vectors in fact do.)\n\n\n\nStep 3: Construct \\(P\\) from the vectors in Step 2.\nThe order of the vectors is actually not important.\n\\[P = [\\mathbf{v}_1\\;\\mathbf{v}_2\\;\\mathbf{v}_3] = \\begin{bmatrix}1&-1&-1\\\\-1&1&0\\\\1&0&1\\end{bmatrix}.\\]\n\n\n\n\nStep 4: Construct \\(D\\) from the corresponding eigenvalues.\nThe order of eigenvalues must match the order of eigenvectors used in the previous step.\nIf an eigenvalue has multiplicity greater than 1, then repeat it the corresponding number of times.\n\\[D = \\begin{bmatrix}1&0&0\\\\0&-2&0\\\\0&0&-2\\end{bmatrix}.\\]\n\n\n\nAnd we are done. We have diagonalized \\(A\\):\n\\[A =  \\begin{bmatrix}1&3&3\\\\-3&-5&-3\\\\3&3&1\\end{bmatrix} = \\begin{bmatrix}1&-1&-1\\\\-1&1&0\\\\1&0&1\\end{bmatrix}  \\begin{bmatrix}1&0&0\\\\0&-2&0\\\\0&0&-2\\end{bmatrix}\\begin{bmatrix}1&-1&-1\\\\-1&1&0\\\\1&0&1\\end{bmatrix}^{-1}\\]\n\n\nSo, just as a reminder, we can now take powers of \\(A\\) quite efficiently:\n\\[A^{100} = \\begin{bmatrix}1&-1&-1\\\\-1&1&0\\\\1&0&1\\end{bmatrix}  \\begin{bmatrix}1^{100}&0&0\\\\0&(-2)^{100}&0\\\\0&0&(-2)^{100}\\end{bmatrix}\\begin{bmatrix}1&-1&-1\\\\-1&1&0\\\\1&0&1\\end{bmatrix}^{-1}\\]\n\n\nWhen Diagonalization Fails\nLet’s look at an example of how diagonalization can fail.\n\nExample. Diagonalize the following matrix, if possible.\n\\[A = \\begin{bmatrix}2&4&3\\\\-4&-6&-3\\\\3&3&1\\end{bmatrix}.\\]\n\n\nSolution. The characteristic equation of \\(A\\) turns out to be the same as in the last example:\n\\[0 = \\det(A-\\lambda I) = -(\\lambda-1)(\\lambda +2)^2\\]\n\n\nThe eigenvalues are \\(\\lambda = 1\\) and \\(\\lambda = -2.\\) However, it is easy to verify that each eigenspace is only one-dimensional:\nBasis for \\(\\lambda_1 = 1\\): \\(\\mathbf{v}_1 = \\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}.\\)\nBasis for \\(\\lambda_2 = -2\\): \\(\\mathbf{v}_2 = \\begin{bmatrix}-1\\\\1\\\\0\\end{bmatrix}.\\)\n\n\nThere are not other eigenvalues, and every eigenvector of \\(A\\) is a multiple of either \\(\\mathbf{v}_1\\) or \\(\\mathbf{v}_2.\\)\n\n\nHence it is impossible to construct a basis of \\(\\mathbb{R}^3\\) using eigenvectors of \\(A\\).\nSo we conclude that \\(A\\) is not diagonalizable.\n\n\n\nAn Important Case\nThere is an important situation in which we can conclude immediately that \\(A\\) is diagonalizable, without explicitly constructing and testing the eigenspaces of \\(A\\).\n\nTheorem.\nAn \\(n\\times n\\) matrix with \\(n\\) distinct eigenvalues is diagonalizable.\nProof Omitted, but easy.\n\n\nExample.\nDetermine if the following matrix is diagonalizable.\n\\[A = \\begin{bmatrix}5&-8&1\\\\0&0&7\\\\0&0&-2\\end{bmatrix}.\\]\n\n\nSolution.\nIt’s easy! Since \\(A\\) is triangular, its eigenvalues are \\(5, 0,\\) and \\(-2\\). Since \\(A\\) is a \\(3\\times3\\) with 3 distinct eigenvalues, \\(A\\) is diagonalizable.",
    "crumbs": [
      "Diagonalization"
    ]
  },
  {
    "objectID": "L18Diagonalization.html#diagonalization-as-a-change-of-basis",
    "href": "L18Diagonalization.html#diagonalization-as-a-change-of-basis",
    "title": "Geometric Algorithms",
    "section": "Diagonalization as a Change of Basis",
    "text": "Diagonalization as a Change of Basis\nWe can now turn to an understanding of how diagonalization informs us about the properties of \\(A\\).\nLet’s interpret the diagonalization \\(A = PDP^{-1}\\) in terms of how \\(A\\) acts as a linear operator.\n\nWhen thinking of \\(A\\) as a linear operator, diagonalization has a specific interpretation:\n\nDiagonalization separates the influence of each vector component from the others.\n\n\n\nIntuitively, the point to see is that when we multiply a vector \\(\\mathbf{x}\\) by a diagonal matrix \\(D\\), the change to each component of \\(\\mathbf{x}\\) depends only on that component.\nThat is, multiplying by a diagonal matrix simply scales the components of the vector.\n\n\nOn the other hand, when we multiply by a matrix \\(A\\) that has off-diagonal entries, the components of \\(\\mathbf{x}\\) affect each other.\n\n\nSo diagonalizing a matrix allows us to bring intuition to its behavior as as linear operator.\n\n\nInterpreting Diagonalization Geometrically\nWhen we compute \\(P\\mathbf{x},\\) we are taking a vector sum of the columns of \\(P\\):\n\\[P\\mathbf{x} = \\mathbf{p}_1 x_1 + \\mathbf{p}_2 x_2 + \\dots \\mathbf{p}_n x_n.\\]\n\nNow \\(P\\) is invertible, so its columns are a basis for \\(\\mathbb{R}^n\\). Let’s call that basis \\(\\mathcal{B} = \\{\\mathbf{p}_1, \\mathbf{p}_2, \\dots, \\mathbf{p}_n\\}.\\)\nSo, we can think of \\(P\\mathbf{x}\\) as “the point that has coordinates \\(\\mathbf{x}\\) in the basis \\(\\mathcal{B}\\).”\n\n\nOn the other hand, what if we wanted to find the coordinates of a vector in basis \\(\\mathcal{B}\\)?\n\n\nLet’s say we have some \\(\\mathbf{y}\\), and we want to find its coordinates in the basis B.\nSo \\(\\mathbf{y} = P\\mathbf{x}.\\)\n\n\nThen since \\(P\\) is invertible, \\(\\mathbf{x} = P^{-1} \\mathbf{y}.\\)\nThus, \\(P^{-1} \\mathbf{y}\\) is “the coordinates of \\(\\mathbf{y}\\) in the basis \\(\\mathcal{B}.\\)”\n\nSo we can interpret \\(PDP^{-1}\\mathbf{x}\\) as:\n  1. Compute the coordinates of \\(\\mathbf{x}\\) in the basis \\(\\mathcal{B}\\).\nThis is \\(P^{-1}\\mathbf{x}.\\)\n\n  2. Scale those \\(\\mathcal{B}\\)-coordinates according to the diagonal matrix \\(D\\).\nThis is \\(DP^{-1}\\mathbf{x}.\\)\n\n\n  3. Find the point that has those scaled \\(\\mathcal{B}\\)-coordinates.\nThis is \\(PDP^{-1}\\mathbf{x}.\\)\n\nLet’s visualize diagonalization geometrically.\nHere is \\(A\\) transforming a point \\(\\mathbf{x} = \\begin{bmatrix}2.47\\\\1.25\\end{bmatrix}\\).\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let’s compute \\(P^{-1}\\mathbf{x}.\\)\nRemember that the columns of \\(P\\) are the eigenvectors of \\(A\\).\nSo \\(P^{-1}\\mathbf{x}\\) is the coordinates of the point \\(\\mathbf{x}\\) in the eigenvector basis:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe coordinates of \\(\\mathbf{x}\\) in this basis are (2,1).\nIn other words \\(P^{-1}\\mathbf{x} = \\begin{bmatrix}2\\\\1\\end{bmatrix}.\\)\n\n\nNow, we compute \\(DP^{-1}\\mathbf{x}.\\) Since \\(D\\) is diagonal, this is just scaling each of the \\(\\mathcal{B}\\)-coordinates.\nIn this example the eigenvalue corresponding to \\(\\mathbf{p}_1\\) is 2, and the eigenvalue corresponding to \\(\\mathbf{p}_2\\) is 3.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo the coordinates of \\(A\\mathbf{x}\\) in the basis \\(\\mathcal{B}\\) are\n\\[ \\begin{bmatrix}2&0\\\\0&3\\end{bmatrix}\\begin{bmatrix}2\\\\1\\end{bmatrix} = \\begin{bmatrix}4\\\\3\\end{bmatrix}.\\]\n\n\nNow we convert back to the standard basis – that is, we ask which point has coordinates (4,3) in basis \\(\\mathcal{B}.\\)\nWe rely on the fact that if \\(\\mathbf{y}\\) has coordinates \\(\\mathbf{x}\\) in the basis \\(\\mathcal{B}\\), then \\(\\mathbf{y} = P\\mathbf{x}.\\)\nSo\n\\[ A\\mathbf{x} = P\\begin{bmatrix}4\\\\3\\end{bmatrix}\\]\n\\[ = PDP^{-1}\\mathbf{x}.\\]\n\n\n\n\n\n\n\n\n\n\n\nWe find that \\(A\\mathbf{x}\\) = \\(PDP^{-1}\\mathbf{x} = \\begin{bmatrix}5.46\\\\3.35\\end{bmatrix}.\\)\n\nIn conclusion: notice that the transformation \\(\\mathbf{x} \\mapsto A\\mathbf{x}\\) may be a complicated one in which each component of \\(\\mathbf{x}\\) affects each component of \\(A\\mathbf{x}\\).\nHowever, by changing to the basis defined by the eigenspaces of \\(A\\), the action of \\(A\\) becomes simple to understand.\n\nDiagonalization of \\(A\\) changes to a basis in which the action of \\(A\\) is particularly easy to understand and compute with.",
    "crumbs": [
      "Diagonalization"
    ]
  },
  {
    "objectID": "L18Diagonalization.html#exposing-the-behavior-of-a-dynamical-system",
    "href": "L18Diagonalization.html#exposing-the-behavior-of-a-dynamical-system",
    "title": "Geometric Algorithms",
    "section": "Exposing the Behavior of a Dynamical System",
    "text": "Exposing the Behavior of a Dynamical System\n\nAt the beginning of the last lecture, we looked at a number of examples of ‘trajectories’ – the behavior of a dynamic system.\n\n\nHere is a typical example:\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nThis is the behavior of a dynamical system with matrix \\(A\\):\n\\[ \\mathbf{x}_{k+1} = A\\mathbf{x}_k \\]\nwith\n\\[ A = \\begin{bmatrix}\n0.87142857&0.05714286\\\\\n-0.11428571&1.12857143\n\\end{bmatrix}\n\\]\n\n\nLooking at \\(A\\) does not give us much insight into why this dynamical system is showing its particular behavior.\n\n\nNow, let’s diagonalize \\(A\\):\n\\[ A = PDP^{-1} \\]\nand we find that:\n\\[ P = \\begin{bmatrix}\n1&2\\\\\n4&1\n\\end{bmatrix}\\;\\;\\;\\;\n\\text{and}\\;\\;\\;\\;\nD =\n\\begin{bmatrix}\n1.1 & 0\\\\\n0 & 0.9\n\\end{bmatrix}\n\\]\n\n\nThe columns of \\(P\\) are eigenvectors of \\(A\\) that correspond to the eigenvalues in \\(D\\).\nSo the eigenspace corresponding to \\(\\lambda_1 = 1.1\\) is Span\\(\\left\\{\\begin{bmatrix}1\\\\4\\end{bmatrix}\\right\\}\\),\nand the eigenspace corresponding to \\(\\lambda_2 = 0.9\\) is Span\\(\\left\\{\\begin{bmatrix}2\\\\1\\end{bmatrix}\\right\\}\\).\n\n\nLet’s look at these eigenspaces:\n\n\n\n\n\nNow, the behavior of this dynamical system becomes much clearer!\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nThe initial point has a large component in the blue eigenspace, and a small component in the green eigenspace.\nAs the system evolves, the blue component shrinks by a factor of 0.9 on each step,\nand the green component increases by a factor of 1.1 on each step.\nWhich allows us to understand very clearly why the dynamical system shows the behavior it does!",
    "crumbs": [
      "Diagonalization"
    ]
  },
  {
    "objectID": "L06LinearIndependence.html",
    "href": "L06LinearIndependence.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nWe start by returning the question: when does \\(A\\mathbf{x} = \\mathbf{b}\\) have a solution \\(\\mathbf{x}\\)?\nThat is, when is \\(A\\mathbf{x} = \\mathbf{b}\\) consistent?\n\nIn the last lecture, we learned that \\(A{\\bf x} = {\\bf b}\\) is consistent if and only if \\(\\bf b\\) lies in the span of the columns of \\(A.\\)\n\n\nAs an example, we saw for the following matrix \\(A\\):\n\\[A = \\left[\\begin{array}{rrr}1&3&4\\\\-4&2&-6\\\\-3&-2&-7\\end{array}\\right]\\]\n\\(A{\\bf x} = {\\bf b}\\) is not consistent for all \\({\\bf b}\\).\n\n\nWe realized that was because the span of \\(A\\)’s columns is not all of \\(\\mathbb{R}^3\\), but rather only a part of \\(\\mathbb{R}^3\\) – namely, a plane lying within \\(\\mathbb{R}^3\\).\nSo, when \\(\\bf b\\) does not lie in that plane, then \\(A{\\bf x} = {\\bf b}\\) is not consistent and has no solution.\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nAs a reminder, here is the picture of the span of the columns of \\(A\\).\n\n\nClearly, \\(\\bf 0, a_1, a_2,\\) and \\(\\bf a_3\\) have a particular relationship:\nnamely, they all lie within the same plane – even though the vectors are in \\(\\mathbb{R}^3\\).\nThis is a special relationship. It only happens under certain circumstances.\n\n\nThat is, it is not the case in general that four points in \\(\\mathbb{R}^3\\) would lie in the same plane!\nToday we will talk about how to define this relationship precisely for vectors of arbitrary dimension, that is, vectors in \\(\\mathbb{R}^n\\).",
    "crumbs": [
      "Linear Independence"
    ]
  },
  {
    "objectID": "L06LinearIndependence.html#linear-independence",
    "href": "L06LinearIndependence.html#linear-independence",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nWe start by returning the question: when does \\(A\\mathbf{x} = \\mathbf{b}\\) have a solution \\(\\mathbf{x}\\)?\nThat is, when is \\(A\\mathbf{x} = \\mathbf{b}\\) consistent?\n\nIn the last lecture, we learned that \\(A{\\bf x} = {\\bf b}\\) is consistent if and only if \\(\\bf b\\) lies in the span of the columns of \\(A.\\)\n\n\nAs an example, we saw for the following matrix \\(A\\):\n\\[A = \\left[\\begin{array}{rrr}1&3&4\\\\-4&2&-6\\\\-3&-2&-7\\end{array}\\right]\\]\n\\(A{\\bf x} = {\\bf b}\\) is not consistent for all \\({\\bf b}\\).\n\n\nWe realized that was because the span of \\(A\\)’s columns is not all of \\(\\mathbb{R}^3\\), but rather only a part of \\(\\mathbb{R}^3\\) – namely, a plane lying within \\(\\mathbb{R}^3\\).\nSo, when \\(\\bf b\\) does not lie in that plane, then \\(A{\\bf x} = {\\bf b}\\) is not consistent and has no solution.\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nAs a reminder, here is the picture of the span of the columns of \\(A\\).\n\n\nClearly, \\(\\bf 0, a_1, a_2,\\) and \\(\\bf a_3\\) have a particular relationship:\nnamely, they all lie within the same plane – even though the vectors are in \\(\\mathbb{R}^3\\).\nThis is a special relationship. It only happens under certain circumstances.\n\n\nThat is, it is not the case in general that four points in \\(\\mathbb{R}^3\\) would lie in the same plane!\nToday we will talk about how to define this relationship precisely for vectors of arbitrary dimension, that is, vectors in \\(\\mathbb{R}^n\\).",
    "crumbs": [
      "Linear Independence"
    ]
  },
  {
    "objectID": "L06LinearIndependence.html#linear-dependence",
    "href": "L06LinearIndependence.html#linear-dependence",
    "title": "Geometric Algorithms",
    "section": "Linear Dependence",
    "text": "Linear Dependence\n\nThe relationship between these four vectors is called linear dependence.\n\n\nBefore stating the definition, let’s get a sense intuitively of what we want to capture.\nWe make this observation:\n\nthe plane defined by \\(\\mathbf{a_1}, \\mathbf{a_2}, \\mathbf{a_3}\\) happens to include the origin.\n\nThat’s one way of capturing the special relationship among \\({\\bf a_1, a_2,}\\) and \\({\\bf a_3}.\\)\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nHere is the formal definition:\nA set of vectors \\(\\{{\\bf v_1, ..., v_p}\\}\\) all of which are in \\(\\mathbb{R}^n\\) is said to be  linearly dependent  if there exist weights \\(\\{c_1, ..., c_p\\},\\) not all zero, such that\n\\[c_1{\\bf v_1} + ... + c_p{\\bf v_p} = {\\bf 0}.\\]\n\nCan you see how this definition captures our intuition about the special relationship of the four vectors?\n\n\nConversely, the set \\(\\{{\\bf v_1, ..., v_p}\\}\\) is said is said to be  linearly independent  if the vector equation\n\\[c_1{\\bf v_1} + ... + c_p{\\bf v_p} = {\\bf 0}.\\]\nhas only the trivial solution \\(c_1 = 0, ..., c_p = 0\\).\n\n\nA set of nonzero weights that yield zero is called a linear dependence relation among \\(\\{{\\bf v_1, ..., v_p}\\}\\).\nA set of vectors is linearly dependent if and only if it is not linearly independent.",
    "crumbs": [
      "Linear Independence"
    ]
  },
  {
    "objectID": "L06LinearIndependence.html#testing-if-a-set-of-vectors-is-linearly-independent",
    "href": "L06LinearIndependence.html#testing-if-a-set-of-vectors-is-linearly-independent",
    "title": "Geometric Algorithms",
    "section": "Testing if a Set of Vectors is Linearly (In)dependent",
    "text": "Testing if a Set of Vectors is Linearly (In)dependent\n\nLet’s work out how we would test, algebraically, whether a set of vectors is linearly dependent.\nWe’ll use a specific example.\n\n\nLet \\({\\bf v_1} = \\left[\\begin{array}{r}1\\\\2\\\\3\\end{array}\\right], {\\bf v_2} = \\left[\\begin{array}{r}4\\\\5\\\\6\\end{array}\\right],\\) and \\({\\bf v_3} = \\left[\\begin{array}{r}2\\\\1\\\\0\\end{array}\\right].\\)\nLet’s determine (a) if the set \\(\\{{\\bf v_1, v_2, v_3}\\}\\) is linearly independent, and (b) if not, a linear dependence relation among them.\n\n(a). Are \\(\\{{\\bf v_1, v_2, v_3}\\}\\) linearly independent?\n\nWe must determine if there is a nontrivial solution of the vector equation:\n\\[x_1{\\bf v_1} + x_2{\\bf v_2} + x_3{\\bf v_3} = {\\bf 0}.\\]\n\n\nLet’s row reduce the augmented matrix:\n\\[\\left[\\begin{array}{rrrc}1&4&2&0\\\\2&5&1&0\\\\3&6&0&0\\end{array}\\right] \\sim\n\\left[\\begin{array}{rrrc}1&4&2&0\\\\0&-3&-3&0\\\\0&0&0&0\\end{array}\\right].\\]\n\n\nWe can see that \\(x_1\\) and \\(x_2\\) are basic variables, and \\(x_3\\) is free.\nEach nonzero value of \\(x_3\\) determines a nontrivial solution of the vector equation.\nSo \\(\\{{\\bf v_1, v_2, v_3}\\}\\) are linearly dependent.\n\n(b). To find the linear dependence relation among \\({\\bf v_1, v_2,}\\) and \\({\\bf v_3},\\) we continue the row reduction to obtain the reduced echelon form:\n\\[\\left[\\begin{array}{rrrc}1&4&2&0\\\\2&5&1&0\\\\3&6&0&0\\end{array}\\right] \\sim\n\\left[\\begin{array}{rrrc}1&4&2&0\\\\0&-3&-3&0\\\\0&0&0&0\\end{array}\\right] \\sim\n\\left[\\begin{array}{rrrc}1&0&-2&0\\\\0&1&1&0\\\\0&0&0&0\\end{array}\\right]\\]\n\nWhich denotes the system of equations:\n\\[\\begin{array}{rrrcl}x_1&&-2x_3&=&0\\\\&x_2&+x_3&=&0\\\\&&0&=&0\\end{array}\\]\nSo \\(x_1 = 2x_3, x_2 = -x_3,\\) and \\(x_3\\) is free.\n\n\nWe can choose any nonzero value for \\(x_3\\) – say, \\(x_3 = 5\\). Then \\(x_1 = 10\\) and \\(x_2 = -5\\). This gives us the solution:\n\\[ 10{\\bf v_1} - 5{\\bf v_2} + 5{\\bf v_3} = {\\bf 0}.\\]\nThis is one (out of infinitely many) linear dependence relations among \\({\\bf v_1, v_2,}\\) and \\({\\bf v_3}.\\)",
    "crumbs": [
      "Linear Independence"
    ]
  },
  {
    "objectID": "L06LinearIndependence.html#linear-independence-of-matrix-columns",
    "href": "L06LinearIndependence.html#linear-independence-of-matrix-columns",
    "title": "Geometric Algorithms",
    "section": "Linear Independence of Matrix Columns",
    "text": "Linear Independence of Matrix Columns\n\nThe columns of a matrix are a set of vectors, so our definition of linear dependence extends naturally to them.\nIn particular if \\(A = [{\\bf a_1}\\;\\dots\\;{\\bf a_n}]\\) then\n\\[x_1{\\bf a_1} + ... + x_n{\\bf a_n} = {\\bf 0}\\]\ncan be written as\n\\[A{\\bf x} = {\\bf 0}.\\]\n\n\nSo each linear dependence relation among the columns of \\(A\\) corresponds to a nontrivial solution of \\(A{\\bf x} = {\\bf 0}.\\)\nPutting it another way: the columns of the matrix \\(A\\) are linearly independent if and only if the equation \\(A{\\bf x} = {\\bf 0}\\) has only the trivial solution \\({\\bf x} = {\\bf 0}\\).\n\nSo, let’s connect this to what we know about solutions of linear systems:\n\n\\(A{\\bf x} = {\\bf 0}\\) is always consistent; that is, it always has the solution \\({\\bf x} = {\\bf 0}\\) (at least).\nThe columns of \\(A\\) are linearly independent if and only if the only solution of \\(A{\\bf x} = {\\bf 0}\\) is \\({\\bf x} = {\\bf 0}\\).\nSo: The columns of \\(A\\) are linearly dependent if and only if \\(A{\\bf x} = {\\bf 0}\\) has an infinite solution set.\n\n\nSo we can also say: the columns of \\(A\\) are linearly dependent if and only if\n\nThe solution set of \\(A{\\bf x} = {\\bf 0}\\) has a free variable, or\nin other words, \\(A\\) does not have a pivot in every column.",
    "crumbs": [
      "Linear Independence"
    ]
  },
  {
    "objectID": "L06LinearIndependence.html#another-interpretation-of-linear-dependence",
    "href": "L06LinearIndependence.html#another-interpretation-of-linear-dependence",
    "title": "Geometric Algorithms",
    "section": "Another Interpretation of Linear Dependence",
    "text": "Another Interpretation of Linear Dependence\nHere is another way of thinking about linear dependence.\n\nTheorem. A set \\(S = \\{{\\bf v_1, ..., v_p}\\}\\) of two or more vectors is linearly dependent if and only if at least one of the vectors in \\(S\\) is a linear combination of the others.\n\n\nProof.\nFirst, let’s consider the “if” part:\nAssume \\({\\bf v_p} = c_1{\\bf v_1} + \\dots + c_{p-1} {\\bf v_{p-1}}.\\)\n\n\nThen clearly\n\\[c_1{\\bf v_1} + \\dots + c_{p-1} {\\bf v_{p-1}} - {\\bf v_p} = {\\bf 0},\\]\nand not all the coefficients are zero (the coefficient of \\({\\bf v_p}\\) is \\(-1\\)). Thus, the vectors are linearly dependent.\n\n\nNow, we consider the “only if” part:\nAssume \\(S\\) is linearly dependent. Then \\(c_1{\\bf v_1} + \\dots + c_p{\\bf v_p} = 0\\) and at least one of the \\(c_i\\) is nonzero.\n\n\nPick one of the nonzero \\(c_i,\\) and rearranging, we get:\n\\[{\\bf v_i} = -(c_1/c_i){\\bf v_1} + \\dots + -(c_p/c_i){\\bf v_p}\\]\nThus, there is at least one vector that is a linear combination of the others.",
    "crumbs": [
      "Linear Independence"
    ]
  },
  {
    "objectID": "L06LinearIndependence.html#spanning-sets-and-linear-dependence",
    "href": "L06LinearIndependence.html#spanning-sets-and-linear-dependence",
    "title": "Geometric Algorithms",
    "section": "Spanning Sets and Linear Dependence",
    "text": "Spanning Sets and Linear Dependence\nLet’s return to the motivation at the start of the lecture, and formalize the connection between spanning sets and linear dependence.\n\nWe’ll start with two linearly independent vectors \\({\\bf u} = \\left[\\begin{array}{r}3\\\\1\\\\0\\end{array}\\right]\\) and \\({\\bf v} = \\left[\\begin{array}{r}1\\\\6\\\\0\\end{array}\\right].\\)\nLet’s\n\ndescribe the set spanned by \\({\\bf u}\\) and \\({\\bf v}\\) and\nexplain why a vector \\({\\bf w}\\) is in Span\\(\\{\\bf u, v\\}\\) if and only if \\(\\{\\bf u, v, w\\}\\) is linearly dependent.\n\n\n\nSolution.\nThe vectors \\({\\bf u}\\) and \\({\\bf v}\\) are linearly independent so they span a plane in \\(\\mathbb{R}^3\\).\nIn fact, since \\(x_3 = 0\\) in both vectors, they span the \\(x_1x_2\\) plane.\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nNow, if \\({\\bf w}\\) is in Span\\(\\{\\bf u, v\\}\\), then \\({\\bf w}\\) is a linear combination of \\({\\bf u}\\) and \\({\\bf v}\\).\nSo then \\(\\{\\bf u, v, w\\}\\) is linearly dependent (by the Theorem we just proved).\n\n\nAnd conversely, if \\(\\{\\bf u, v, w\\}\\) is linearly dependent, then there exist \\(c_1, c_2,\\) and \\(c_3,\\) not all zero, such that\n\\[ c_1 {\\bf u} + c_2 {\\bf v} + c_3 {\\bf w} = {\\bf 0}.\\]\n\n\nWe know that \\({\\bf u}\\) and \\({\\bf v}\\) are linearly independent, so the only way for \\(c_1 {\\bf u} + c_2{\\bf v}\\) to be zero is if \\(c_1 = c_2 = 0\\).\nSo \\(c_1 {\\bf u} + c_2{\\bf v}\\) must be nonzero, and that means \\(c_3\\) must be different from zero.\n\n\nSo we can write:\n\\[ {\\bf w} = -(c_1/c_3) {\\bf u} - (c_2/c_3) {\\bf v},\\]\nthat is, \\({\\bf w}\\) is a linear combination of \\({\\bf u}\\) and \\({\\bf v}\\), and therefore lies in Span\\(\\{\\bf u, v\\}\\).\n\n\nSo we conclude:\n\nIf a set of vectors \\(\\{\\bf v_1, v_2, \\dots, v_p\\}\\) is linearly dependent, then at least one of them lies within the span of the others, and\nIf one vector in the set \\(\\{\\bf v_1, v_2, \\dots, v_p\\}\\) lies within the span of the others, then the set is linearly dependent.\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "Linear Independence"
    ]
  },
  {
    "objectID": "L06LinearIndependence.html#linear-dependence-geometrically-mathbbr2",
    "href": "L06LinearIndependence.html#linear-dependence-geometrically-mathbbr2",
    "title": "Geometric Algorithms",
    "section": "Linear Dependence Geometrically: \\(\\mathbb{R}^2\\)",
    "text": "Linear Dependence Geometrically: \\(\\mathbb{R}^2\\)\nNow let’s try to get a geometric sense of linear dependence.\nWe’ll use \\(\\mathbb{R}^2\\) for visualization.\nLet’s try to interpret linear dependence directly in terms of the definition, which involves vector sums.\n\n\n\n\n\n\n\n\n\n\n\n\nThe vectors \\(\\{{\\bf u},{\\bf v}\\}\\) are independent because there is no nozero combination of them that yields the origin.\nA nonzero combination of \\({\\bf u}\\) and \\({\\bf v}\\) geometrically means moving in the direction of \\({\\bf u}\\) or \\({\\bf v}\\) or both. There is no way to move in the direction of \\({\\bf u}\\) and then in the direction of \\({\\bf v}\\) and arrive back at the origin.\n\n\nNow let’s add another vector \\({\\bf w}\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow the situation is different. The set \\({\\bf u, v, w}\\) is linearly dependent.\nThere are nonzero moves along the three directions that can bring you back to the origin:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a geometric interpretation of the equation\n\\[ c_1{\\bf u} + c_2{\\bf v} + c_3{\\bf w} = 0.\\]\n\nThe last example suggested that three vectors in \\(\\mathbb{R}^2\\) are linearly dependent.\nThis is in fact always true, and furthermore, we can generalize to \\(\\mathbb{R}^n\\).\nTheorem. If a set contains more vectors than there are entries in each vector, then the set is linearly dependent.\nThat is, any set \\(\\{{\\bf v_1}, \\dots, {\\bf v_p}\\}\\) in \\(\\mathbb{R}^n\\) is linearly dependent if \\(p&gt;n.\\)\n\nProof. Let \\(A = [{\\bf v_1} \\; \\dots \\; {\\bf v_p}].\\) Then \\(A\\) is \\(n \\times p\\), and the equation \\(A{\\bf x} = {\\bf 0}\\) corresponds to a system of \\(n\\) equations in \\(p\\) unknowns.\nIf \\(p&gt;n,\\) there are more variables than equations, so there must be a free variable.\nHence \\(A{\\bf x} = {\\bf 0}\\) has a nontrivial solution, and the columns of \\(A\\) are linearly dependent.\n\n\nExample.\nThe vectors \\(\\left[\\begin{array}{r}2\\\\1\\end{array}\\right],\n\\left[\\begin{array}{r}4\\\\-1\\end{array}\\right],\\) and \\(\\left[\\begin{array}{r}-2\\\\2\\end{array}\\right]\\) are linearly dependent because these vectors live in \\(\\mathbb{R}^2\\) and there are 3 of them.\n\n\nHere is something else that we can say:\nTheorem. If a set \\(S = \\{{\\bf v_1, ..., v_p}\\}\\) in \\(\\mathbb{R}^n\\) contains the zero vector, then the set is linearly dependent.\n\n\nProof. Let’s say \\({\\bf v_1} = {\\bf 0}\\). Then \\(1{\\bf v_1} + 0{\\bf v_2}+ \\dots + 0{\\bf v_p} = {\\bf 0}.\\) The coefficients are not all zero, so the set is linearly dependent.",
    "crumbs": [
      "Linear Independence"
    ]
  },
  {
    "objectID": "L06LinearIndependence.html#application-example-network-flow",
    "href": "L06LinearIndependence.html#application-example-network-flow",
    "title": "Geometric Algorithms",
    "section": "Application Example: Network Flow",
    "text": "Application Example: Network Flow\n\nSystems of linear equations arise when considering flow through a network.\n\n\nA network is a set of nodes and links. Links connect nodes.\nBe aware that there is another set of terminology that is used more often in theoretical computer science and mathematics: A graph , which consists of vertices and edges. A network and a graph are exactly the same thing.\n\n\n\n\nImage Source\nHere are some examples of networks:\n\n\n\n\n\nMany times we are interested in flow through a network. Flows can represent movement from place to place. For example, consider this map of the MBTA:\n\n\n\nImage Source\n\n\n\n\n\nWe can think of T stops as nodes and rail connections as links. Flow corresponds to the movement of subway cars from station to station.\n\nHere is another example: this is a representation of the Abilene data network, which is used by universities to exchange data traffic (packets) within the US:\n\n\nImage Source\n\n\n\n\nNetworks usually obey the rule of “flow balance” or “conservation of flow.”\nThis simply means that the amount of flow going into a node equals the amount coming out.\nFor example, the number of packets that enter any node of the Abilene network equals the number that leave that node. This reflects the fact that packets are not created or destroyed within the network.\n\n\n\nImage Source: Lay, 4th edition.\n\n\n\nHere is another example: a simplified view of downtown Baltimore during rush hour.\nThe flows are vehicles per hour.\n\nNote that some of the traffic flows are measured, and some are not. The unmeasured flows are marked with symbols \\(x_1, x_2,\\) etc.\n\nWe’d like to understand the traffic pattern in the city, despite the presence of unmeasured flows. We can do that by using the principle of flow balance.\n\nThe key idea is: flow balance dictates that every node determines a linear equation. The equation is of the form:\n\\[ \\text{Flow in} = \\text{Flow out}.\\]\n\n\n\n\n\nIntersection\nFlow in\nFlow out\n\n\n\n\nA\n300 + 500\n\\(x_1 + x_2\\)\n\n\nB\n\\(x_2 + x_4\\)\n300 + \\(x_3\\)\n\n\nC\n100 + 400\n\\(x_4 + x_5\\)\n\n\nD\n\\(x_1 + x_5\\)\n600\n\n\nnetwork\n1300\n900 + \\(x_3\\)\n\n\n\n\n\n\n\n\nThe last line indicates that the total flow into the network equals the total flow out of the network, which means that \\(x_3 = 400.\\)\n\n\nThis yields the following system of equations:\n\\[\\begin{array}{rrrrrcl}\nx_1 & + x_2 & & & & = & 800\\\\\n& x_2 & -x_3 & + x_4 & & = &300\\\\\n&&&x_4 & +x_5 &=& 500\\\\\nx_1 &&&& + x_5 &=& 600\\\\\n&&x_3&&&=&400\\\\\n\\end{array}\\]\n\n\nWhen we row reduce the associated augmented matrix, the reduced echelon form yields these equations:\n\\[\\begin{array}{rrrrrcl}\nx_1 & & & & +x_5 & = & 600\\\\\n& x_2 & & & -x_5 & = &200\\\\\n&& x_3 & & &=& 400\\\\\n&&& x_4 & + x_5 &=& 500\\\\\n\\end{array}\\]\n\n\nThus the general flow pattern for the network is described by\n\\[\\left\\{\\begin{array}{l}\nx_1 = 600 - x_5\\\\\nx_2 = 200 + x_5\\\\\nx_3 = 400\\\\\nx_4 = 500 - x_5\\\\\nx_5 \\text{ is free}\\\\\n\\end{array}\\right.\\]\n\n\n\n\n\nHow can we interpret this result?\nA negative flow corresponds to a flow in the opposite direction assumed in the diagram. Since these streets are one-way, none of the variables here can be negative. So, for example, \\(x_5 \\leq 500\\) because \\(x_4\\) cannot be negative.",
    "crumbs": [
      "Linear Independence"
    ]
  },
  {
    "objectID": "L21OrthogonalSets.html",
    "href": "L21OrthogonalSets.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nToday we deepen our study of geometry.\nIn the last lecture we focused on points, lines, and angles.\nToday we take on more challenging geometric notions that bring in sets of vectors and subspaces.\nWithin this realm, we will focus on orthogonality and a new notion called projection.\n\nFirst of all, today we’ll study the properties of sets of orthogonal vectors.\nThese can be very useful.",
    "crumbs": [
      "Orthogonal Sets and Projection"
    ]
  },
  {
    "objectID": "L21OrthogonalSets.html#orthogonal-sets-and-projection",
    "href": "L21OrthogonalSets.html#orthogonal-sets-and-projection",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Many parts of this page are based on Linear Algebra and its Applications, by David C. Lay\nToday we deepen our study of geometry.\nIn the last lecture we focused on points, lines, and angles.\nToday we take on more challenging geometric notions that bring in sets of vectors and subspaces.\nWithin this realm, we will focus on orthogonality and a new notion called projection.\n\nFirst of all, today we’ll study the properties of sets of orthogonal vectors.\nThese can be very useful.",
    "crumbs": [
      "Orthogonal Sets and Projection"
    ]
  },
  {
    "objectID": "L21OrthogonalSets.html#orthogonal-sets",
    "href": "L21OrthogonalSets.html#orthogonal-sets",
    "title": "Geometric Algorithms",
    "section": "Orthogonal Sets",
    "text": "Orthogonal Sets\n\nA set of vectors \\(\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_p\\}\\) in \\(\\mathbb{R}^n\\) is said to be an orthogonal set if each pair of distinct vectors from the set is orthogonal, i.e.,\n\\[\\mathbf{u}_i^T\\mathbf{u}_j = 0\\;\\;\\text{whenever}\\;i\\neq j.\\]\n\n\nExample. Show that \\(\\{\\mathbf{u}_1,\\mathbf{u}_2,\\mathbf{u}_3\\}\\) is an orthogonal set, where\n\\[ \\mathbf{u}_1 = \\begin{bmatrix}3\\\\1\\\\1\\end{bmatrix},\\;\\;\\mathbf{u}_2=\\begin{bmatrix}-1\\\\2\\\\1\\end{bmatrix},\\;\\;\\mathbf{u}_3=\\begin{bmatrix}-1/2\\\\-2\\\\7/2\\end{bmatrix}.\\]\n\n\nSolution. Consider the three possible pairs of distinct vectors, namely, \\(\\{\\mathbf{u}_1,\\mathbf{u}_2\\}, \\{\\mathbf{u}_1,\\mathbf{u}_3\\},\\) and \\(\\{\\mathbf{u}_2,\\mathbf{u}_3\\}.\\)\n\n\n\\[ \\mathbf{u}_1^T\\mathbf{u}_2 = 3(-1) + 1(2) + 1(1) = 0\\]\n\\[ \\mathbf{u}_1^T\\mathbf{u}_3 = 3(-1/2) + 1(-2) + 1(7/2) = 0\\]\n\\[ \\mathbf{u}_2^T\\mathbf{u}_3 = -1(-1/2) + 2(-2) + 1(7/2) = 0\\]\n\nEach pair of distinct vectors is orthogonal, and so \\(\\{\\mathbf{u}_1,\\mathbf{u}_2, \\mathbf{u}_3\\}\\) is an orthogonal set.\nIn three-space they describe three lines that we say are mutually perpendicular.\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nOrthogonal Sets Must be Independent\nOrthogonal sets are very nice to work with.\nFirst of all, we will show that any orthogonal set must be linearly independent.\n\nTheorem. If \\(S = \\{\\mathbf{u}_1,\\dots,\\mathbf{u}_p\\}\\) is an orthogonal set of nonzero vectors in \\(\\mathbb{R}^n,\\) then \\(S\\) is linearly independent.\n\n\nProof. We will prove that there is no linear combination of the vectors in \\(S\\) with nonzero coefficients that yields the zero vector.\n\n\nOur proof strategy will be:\nwe will show that for any linear combination of the vectors in \\(S\\):\n\nif the combination is the zero vector,\nthen all coefficients of the combination must be zero.\n\n\n\nSpecifically:\nAssume \\({\\bf 0} = c_1\\mathbf{u}_1 + \\dots + c_p\\mathbf{u}_p\\) for some scalars \\(c_1,\\dots,c_p\\). Then:\n\\[{\\bf 0} = c_1\\mathbf{u}_1 + c_2\\mathbf{u}_2 + \\dots + c_p\\mathbf{u}_p\\]\n\n\n\\[{\\bf 0}^T\\mathbf{u}_1 = (c_1\\mathbf{u}_1 + c_2\\mathbf{u}_2 + \\dots + c_p\\mathbf{u}_p)^T\\mathbf{u}_1\\]\n\n\n\\[0 = (c_1\\mathbf{u}_1 + c_2\\mathbf{u}_2 + \\dots + c_p\\mathbf{u}_p)^T\\mathbf{u}_1\\]\n\n\n\\[ 0 = (c_1\\mathbf{u}_1)^T\\mathbf{u}_1 + (c_2\\mathbf{u}_2)^T\\mathbf{u}_1 \\dots + (c_p\\mathbf{u}_p)^T\\mathbf{u}_1\\]\n\n\n\\[ 0 = c_1(\\mathbf{u}_1^T\\mathbf{u}_1) + c_2(\\mathbf{u}_2^T\\mathbf{u}_1) + \\dots + c_p(\\mathbf{u}_p^T\\mathbf{u}_1)\\]\n\n\nBecause \\(\\mathbf{u}_1\\) is orthogonal to \\(\\mathbf{u}_2,\\dots,\\mathbf{u}_p\\):\n\\[ 0 = c_1(\\mathbf{u}_1^T\\mathbf{u}_1) \\]\n\n\nSince \\(\\mathbf{u}_1\\) is nonzero, \\(\\mathbf{u}_1^T\\mathbf{u}_1\\) is not zero and so \\(c_1 = 0\\).\n\nWe can use the same kind of reasoning to show that, \\(c_2,\\dots,c_p\\) must be zero.\nIn other words, there is no nonzero combination of \\(\\mathbf{u}_i\\)’s that yields the zero vector …\n… so \\(S\\) is linearly independent.\n\nNotice that since \\(S\\) is a linearly independent set, it is a basis for the subspace spanned by \\(S\\).\nThis leads us to a new kind of basis.\n\n\n\nOrthogonal Basis\n\nDefinition. An orthogonal basis for a subspace \\(W\\) of \\(\\mathbb{R}^n\\) is a basis for \\(W\\) that is also an orthogonal set.\n\n\nFor example, consider\n\\[\\mathbf{u} = \\begin{bmatrix} -1/2\\\\ 2\\\\ 1 \\end{bmatrix},\n\\mathbf{v} = \\begin{bmatrix} 8/3\\\\ 1/3\\\\ 2/3 \\end{bmatrix}.\\]\nNote that \\(\\mathbf{u}^T\\mathbf{v} = 0.\\) Hence they form an orthogonal basis for their span.\n\n\nHere is the subspace \\(W = \\text{Span}\\{\\mathbf{u},\\mathbf{v}\\}\\):\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\nFinding Coordinates in an Orthogonal Basis\n\nWe have seen that for any subspace \\(W\\), there may be many different sets of vectors that can serve as a basis for \\(W\\).\nFor example, let’s say we have a basis \\(\\mathcal{B} = \\{\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3\\}.\\)\nWe know that to compute the coordinates of \\(\\mathbf{y}\\) in this basis, we need to solve the linear system: ​ \\[c_1 \\mathbf{u}_1 + c_2\\mathbf{u}_2 + c_3\\mathbf{u}_3 = \\mathbf{y}\\] ​ or ​ \\[U\\mathbf{c} = \\mathbf{y}.\\]\n\n\nIn general, we’d need to perform Gaussian Elimination, or matrix inversion, or some other complex method to do this.\n\n\nHowever, an orthogonal basis is a particularly nice basis, because the weights (coordinates) of any point can be computed easily and simply.\n\nLet’s see how:\n\nTheorem. Let \\(\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_p\\}\\) be an orthogonal basis for a subspace \\(W\\) of \\(\\mathbb{R}^n\\). For each \\(\\mathbf{y}\\) in \\(W,\\) the weights of the linear combination\n\\[c_1\\mathbf{u}_1 + \\dots + c_p\\mathbf{u}_p = \\mathbf{y}\\]\nare given by\n\\[c_j = \\frac{\\mathbf{y}^T\\mathbf{u}_j}{\\mathbf{u}_j^T\\mathbf{u}_j}\\;\\;\\;j = 1,\\dots,p\\]\n\n\nProof.\nLet’s consider the inner product of \\(\\mathbf{y}\\) and one of the \\(\\mathbf{u}\\) vectors — say, \\(\\mathbf{u}_1\\).\nAs we saw in the last proof, the orthogonality of \\(\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_p\\}\\) means that\n\\[\\mathbf{y}^T\\mathbf{u}_1 = (c_1\\mathbf{u}_1 + c_2\\mathbf{u}_2 + \\dots + c_p\\mathbf{u}_p)^T\\mathbf{u}_1\\]\n\\[=c_1(\\mathbf{u}_1^T\\mathbf{u}_1)\\]\n\n\nSince \\(\\mathbf{u}_1^T\\mathbf{u}_1\\) is not zero (why?), the equation above can be solved for \\(c_1.\\)\nThus:\n\\[c_1 = \\frac{\\mathbf{y}^T\\mathbf{u}_1}{\\mathbf{u}_1^T\\mathbf{u}_1}\\]\nTo find any other \\(c_j,\\) compute \\(\\mathbf{y}^T\\mathbf{u}_j\\) and solve for \\(c_j\\).\n\nExample. The set \\(S\\) which we saw earlier, ie,\n\\[ \\mathbf{u}_1 = \\begin{bmatrix}3\\\\1\\\\1\\end{bmatrix},\\;\\;\\mathbf{u}_2=\\begin{bmatrix}-1\\\\2\\\\1\\end{bmatrix},\\;\\;\\mathbf{u}_3=\\begin{bmatrix}-1/2\\\\-2\\\\7/2\\end{bmatrix},\\]\nis an orthogonal basis for \\(\\mathbb{R}^3.\\)\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\nNow, let us express the vector \\(\\mathbf{y} = \\begin{bmatrix}6\\\\1\\\\-8\\end{bmatrix}\\) as a linear combination of the vectors in \\(S\\).\nThat is, find \\(\\mathbf{y}\\)’s coordinates in the basis \\(S\\) — i.e., in the coordinate system \\(S\\).\n\n\nSolution. Compute\n\\[\\mathbf{y}^T\\mathbf{u}_1 = 11,\\;\\;\\;\\mathbf{y}^T\\mathbf{u}_2 = -12,\\;\\;\\;\\mathbf{y}^T\\mathbf{u}_3 = -33,\\]\n\\[\\mathbf{u}_1^T\\mathbf{u}_1 = 11,\\;\\;\\;\\mathbf{u}_2^T\\mathbf{u}_2 = 6,\\;\\;\\;\\mathbf{u}_3^T\\mathbf{u}_3 = 33/2\\]\n\n\nSo\n\\[\\mathbf{y} = \\frac{\\mathbf{y}^T\\mathbf{u}_1}{\\mathbf{u}_1^T\\mathbf{u}_1}\\mathbf{u}_1 + \\frac{\\mathbf{y}^T\\mathbf{u}_2}{\\mathbf{u}_2^T\\mathbf{u}_2}\\mathbf{u}_2 + \\frac{\\mathbf{y}^T\\mathbf{u}_3}{\\mathbf{u}_3^T\\mathbf{u}_3}\\mathbf{u}_3\\]\n\n\n\\[ = \\frac{11}{11}\\mathbf{u}_1 + \\frac{-12}{6}\\mathbf{u}_2 + \\frac{-33}{33/2}\\mathbf{u}_3\\]\n\n\n\n\\[ = \\mathbf{u}_1 - 2\\mathbf{u}_2 - 2 \\mathbf{u}_3.\\]\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\nNote how much simpler it is finding the coordinates of \\(\\mathbf{y}\\) in the orthogonal basis,\nbecause each coefficient \\(c_1\\) can be found separately without matrix operations.",
    "crumbs": [
      "Orthogonal Sets and Projection"
    ]
  },
  {
    "objectID": "L21OrthogonalSets.html#orthogonal-projection",
    "href": "L21OrthogonalSets.html#orthogonal-projection",
    "title": "Geometric Algorithms",
    "section": "Orthogonal Projection",
    "text": "Orthogonal Projection\n\n\n\nNow let’s turn to the notion of projection.\nIn general, a projection happens when we decompose a vector into the sum of other vectors.\n\n\nHere is the central idea. We will use this a lot over the next couple lectures.\nGiven a nonzero vector \\(\\mathbf{u}\\) in \\(\\mathbb{R}^n,\\) consider the problem of decomposing a vector \\(\\mathbf{y}\\) in \\(\\mathbb{R}^n\\) into the sum of two vectors:\n\none that is a multiple of \\(\\mathbf{u}\\), and\none that is orthogonal to \\(\\mathbf{u}.\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn other words, we wish to write:\n\\[\\mathbf{y} = \\hat{\\mathbf{y}} + \\mathbf{z}\\]\nwhere \\(\\hat{\\mathbf{y}} = \\alpha\\mathbf{u}\\) for some scalar \\(\\alpha\\) and \\(\\mathbf{z}\\) is some vector orthogonal to \\(\\mathbf{u}.\\)\n\n\n\n\n\n\n\n\n\n\n\n\nThat is, we are given \\(\\mathbf{y}\\) and \\(\\mathbf{u}\\), and asked to compute \\(\\mathbf{z}\\) and \\(\\hat{\\mathbf{y}}.\\)\n\n\nTo solve this, assume that we have some \\(\\alpha\\), and with it we compute \\(\\mathbf{y} - \\alpha\\mathbf{u} = \\mathbf{y}-\\hat{\\mathbf{y}} = \\mathbf{z}.\\)\n\n\nWe want \\(\\mathbf{z}\\) to be orthogonal to \\(\\mathbf{u}.\\)\n\n\nNow \\(\\mathbf{z} = \\mathbf{y} - \\alpha{\\mathbf{u}}\\) is orthogonal to \\(\\mathbf{u}\\) if and only if\n\\[0 = (\\mathbf{y} - \\alpha\\mathbf{u})^T\\mathbf{u}\\]\n\n\n\\[ = \\mathbf{y}^T\\mathbf{u} - (\\alpha\\mathbf{u})^T\\mathbf{u}\\]\n\\[ = \\mathbf{y}^T\\mathbf{u} - \\alpha(\\mathbf{u}^T\\mathbf{u})\\]\n\n\nThat is, the solution in which \\(\\mathbf{z}\\) is orthogonal to \\(\\mathbf{u}\\) happens if and only if\n\\[\\alpha = \\frac{\\mathbf{y}^T\\mathbf{u}}{\\mathbf{u}^T\\mathbf{u}}\\]\n\n\nand since \\(\\hat{\\mathbf{y}} = \\alpha\\mathbf{u}\\),\n\\[\\hat{\\mathbf{y}} = \\frac{\\mathbf{y}^T\\mathbf{u}}{\\mathbf{u}^T\\mathbf{u}}\\mathbf{u}.\\]\n\n\nThe vector \\(\\hat{\\mathbf{y}}\\) is called the orthogonal projection of \\(\\mathbf{y}\\) onto \\(\\mathbf{u}\\), and the vector \\(\\mathbf{z}\\) is called the component of \\(\\mathbf{y}\\) orthogonal to \\(\\mathbf{u}.\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjections are onto Subspaces\n\n\n\n\n\n\n\n\n\n\n\n\nNow, note that if we had scaled \\(\\mathbf{u}\\) by any amount (ie, moved it to the right or the left), we would not have changed the location of \\(\\hat{\\mathbf{y}}.\\)\n\n\nThis can be seen as well by replacing \\(\\mathbf{u}\\) with \\(c\\mathbf{u}\\) and recomputing \\(\\hat{\\mathbf{y}}\\):\n\\[\\hat{\\mathbf{y}} = \\frac{\\mathbf{y}^Tc\\mathbf{u}}{c\\mathbf{u}^Tc\\mathbf{u}}c\\mathbf{u} = \\frac{\\mathbf{y}^T\\mathbf{u}}{\\mathbf{u}^T\\mathbf{u}}\\mathbf{u}.\\]\n\n\nThus, the projection of \\(\\mathbf{y}\\) is determined by the subspace \\(L\\) that is spanned by \\(\\mathbf{u}\\) – in other words, the line through \\(\\mathbf{u}\\) and the origin.\n\n\nHence sometimes \\(\\hat{\\mathbf{y}}\\) is denoted by \\(\\operatorname{proj}_L \\mathbf{y}\\) and is called the orthogonal projection of \\(\\mathbf{y}\\) onto \\(L\\).\nSpecifically:\n\\[\\hat{\\mathbf{y}} = \\operatorname{proj}_L \\mathbf{y} = \\frac{\\mathbf{y}^T\\mathbf{u}}{\\mathbf{u}^T\\mathbf{u}}\\mathbf{u}\\]\n\nExample. Let \\(\\mathbf{y} = \\begin{bmatrix}7\\\\6\\end{bmatrix}\\) and \\(\\mathbf{u} = \\begin{bmatrix}4\\\\2\\end{bmatrix}.\\)\nFind the orthogonal projection of \\(\\mathbf{y}\\) onto \\(\\mathbf{u}.\\) Then write \\(\\mathbf{y}\\) as the sum of two orthogonal vectors, one in Span\\(\\{\\mathbf{u}\\}\\), and one orthogonal to \\(\\mathbf{u}.\\)\n\nSolution. Compute\n\\[\\mathbf{y}^T\\mathbf{u} = \\begin{bmatrix}7&6\\end{bmatrix}\\begin{bmatrix}4\\\\2\\end{bmatrix} = 40\\]\n\\[\\mathbf{u}^T\\mathbf{u} = \\begin{bmatrix}4&2\\end{bmatrix}\\begin{bmatrix}4\\\\2\\end{bmatrix} = 20\\]\n\n\nThe orthogonal projection of \\(\\mathbf{y}\\) onto \\(\\mathbf{u}\\) is\n\\[\\hat{\\mathbf{y}} = \\frac{\\mathbf{y}^T\\mathbf{u}}{\\mathbf{u}^T\\mathbf{u}} \\mathbf{u}\\]\n\\[=\\frac{40}{20}\\mathbf{u} = 2\\begin{bmatrix}4\\\\2\\end{bmatrix} = \\begin{bmatrix}8\\\\4\\end{bmatrix}\\]\n\n\nAnd the component of \\(\\mathbf{y}\\) orthogonal to \\(\\mathbf{u}\\) is\n\\[\\mathbf{y}-\\hat{\\mathbf{y}} = \\begin{bmatrix}7\\\\6\\end{bmatrix} - \\begin{bmatrix}8\\\\4\\end{bmatrix} = \\begin{bmatrix}-1\\\\2\\end{bmatrix}.\\]\n\n\nSo\n\\[\\mathbf{y} = \\hat{\\mathbf{y}} + \\mathbf{z}\\]\n\\[\\begin{bmatrix}7\\\\6\\end{bmatrix} = \\begin{bmatrix}8\\\\4\\end{bmatrix} + \\begin{bmatrix}-1\\\\2\\end{bmatrix}.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant Properties of \\(\\hat{\\mathbf{y}}\\)\n\n\nHere is a proof that the closest point on the line to \\(\\mathbf{y}\\) is given by the projection of \\(\\mathbf{y}\\) onto the line.\nLet \\(\\mathbf{q} = \\alpha \\mathbf{u}\\). The squared distance from \\(\\mathbf{y}\\) to \\(\\mathbf{q}\\) is \\(\\Vert \\mathbf{y}-\\mathbf{q}\\Vert^2 = \\Vert \\mathbf{y}-\\alpha\\mathbf{u}\\Vert^2\\).\nExpanding this, \\(\\Vert \\mathbf{y}-\\alpha\\mathbf{u}\\Vert^2 =\\) \\(\\mathbf{y}^T\\mathbf{y} - 2\\alpha(\\mathbf{y}^T\\mathbf{u}) + \\alpha^2(\\mathbf{u}^T\\mathbf{u}).\\)\nThis is a quadratic function in \\(\\alpha\\). To minimize it, we take the derivative with respect to \\(\\alpha\\): \\(\\frac{d}{d\\alpha} [\\mathbf{y}^T\\mathbf{y} - 2\\alpha(\\mathbf{y}^T\\mathbf{u}) + \\alpha^2(\\mathbf{u}^T\\mathbf{u})]\\) \\(= -2(\\mathbf{y}^T\\mathbf{u}) + 2\\alpha(\\mathbf{u}^T\\mathbf{u})\\).\nSetting the derivative to zero gives \\(\\alpha = \\frac{\\mathbf{y}^T\\mathbf{u}}{\\mathbf{u}^T\\mathbf{u}}.\\)\nTherefore the closest point to \\(\\mathbf{y}\\) on the line \\(\\operatorname{Span}\\{\\mathbf{u}\\}\\) is \\(\\frac{\\mathbf{y}^T\\mathbf{u}}{\\mathbf{u}^T\\mathbf{u}} \\mathbf{u}\\), namely, the orthogonal projection of \\(\\mathbf{y}\\) onto \\(\\operatorname{Span}\\{\\mathbf{u}\\}\\).\nThe closest point.\nRecall from geometry that given a line and a point \\(\\mathbf{y}\\), the closest point on the line to \\(\\mathbf{y}\\) is given by the perpendicular from \\(\\mathbf{y}\\) to the line.\nThere is a short proof in the margin.\nSo this gives an important interpretation of \\(\\hat{\\mathbf{y}}\\): it is the closest point to \\(\\mathbf{y}\\) in the subspace \\(L\\).\n\nThe distance from \\(\\mathbf{y}\\) to \\(L\\).\nThe distance from \\(\\mathbf{y}\\) to \\(L\\) is the length of the perpendicular from \\(\\mathbf{y}\\) to its orthogonal projection on \\(L\\), namely \\(\\hat{\\mathbf{y}}\\).\nThis distance equals the length of \\(\\mathbf{y} - \\hat{\\mathbf{y}}\\).\n\n\nIn this example, the distance is\n\\[\\Vert\\mathbf{y}-\\hat{\\mathbf{y}}\\Vert = \\sqrt{(-1)^2 + 2^2} = \\sqrt{5}.\\]",
    "crumbs": [
      "Orthogonal Sets and Projection"
    ]
  },
  {
    "objectID": "L21OrthogonalSets.html#projections-find-coordinates-in-an-orthogonal-basis",
    "href": "L21OrthogonalSets.html#projections-find-coordinates-in-an-orthogonal-basis",
    "title": "Geometric Algorithms",
    "section": "Projections find Coordinates in an Orthogonal Basis",
    "text": "Projections find Coordinates in an Orthogonal Basis\n\nEarlier in this lectureopen, we saw that when we decompose a vector \\(\\mathbf{y}\\) into a linear combination of vectors \\(\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_p\\}\\) in a orthogonal basis, we have\n\\[\\mathbf{y} = c_1\\mathbf{u}_1 + \\dots + c_p\\mathbf{u}_p\\]\nwhere\n\\[c_j = \\frac{\\mathbf{y}^T\\mathbf{u}_j}{\\mathbf{u}_j^T\\mathbf{u}_j}\\]\n\n\nAnd just now we have seen that the projection of \\(\\mathbf{y}\\) onto the subspace spanned by any \\(\\mathbf{u}\\) is\n\\[\\operatorname{proj}_L \\mathbf{y} = \\frac{\\mathbf{y}^T\\mathbf{u}}{\\mathbf{u}^T\\mathbf{u}}\\mathbf{u}.\\]\n\n\nSo a decomposition like \\(\\mathbf{y} = c_1\\mathbf{u}_1 + \\dots + c_p\\mathbf{u}_p\\) is really decomposing \\(\\mathbf{y}\\) into a sum of orthogonal projections onto one-dimensional subspaces.\n\n\nFor example, let’s take the case where \\(\\mathbf{y} \\in \\mathbb{R}^2.\\)\nLet’s say we are given \\(\\mathbf{u}_1, \\mathbf{u}_2\\) such that \\(\\mathbf{u}_1\\) is orthogonal to \\(\\mathbf{u}_2\\), and so together they span \\(\\mathbb{R}^2.\\)\n\n\nThen \\(\\mathbf{y}\\) can be written in the form\n\\[\\mathbf{y} = \\frac{\\mathbf{y}^T\\mathbf{u}_1}{\\mathbf{u}_1^T\\mathbf{u}_1}\\mathbf{u}_1 +  \\frac{\\mathbf{y}^T\\mathbf{u}_2}{\\mathbf{u}_2^T\\mathbf{u}_2}\\mathbf{u}_2.\\]\n\n\nThe first term is the projection of \\(\\mathbf{y}\\) onto the subspace spanned by \\(\\mathbf{u}_1\\) and the second term is the projection of \\(\\mathbf{y}\\) onto the subspace spanned by \\(\\mathbf{u}_2.\\)\nSo this equation expresses \\(\\mathbf{y}\\) as the sum of its projections onto the (orthogonal) axes determined by \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\).\nThis is an useful way of thinking about coordinates in an orthogonal basis: coordinates are projections onto the axes!",
    "crumbs": [
      "Orthogonal Sets and Projection"
    ]
  },
  {
    "objectID": "L21OrthogonalSets.html#orthonormal-sets",
    "href": "L21OrthogonalSets.html#orthonormal-sets",
    "title": "Geometric Algorithms",
    "section": "Orthonormal Sets",
    "text": "Orthonormal Sets\nOrthogonal sets are therefore very useful. However, they become even more useful if we normalize the vectors in the set.\n\nA set \\(\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_p\\}\\) is an orthonormal set if it is an orthogonal set of unit vectors.\n\n\nIf \\(W\\) is the subspace spanned by such as a set, then \\(\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_p\\}\\) is an orthonormal basis for \\(W\\) since the set is automatically linearly independent.\n\n\nThe simplest example of an orthonormal set is the standard basis \\(\\{\\mathbf{e}_1, \\dots,\\mathbf{e}_n\\}\\) for \\(\\mathbb{R}^n\\).\n\n\nPro tip: keep the terms clear in your head:\n\northogonal is (just) perpendicular, while\northonormal is perpendicular and unit length.\n\n(You can see the word “normalized” inside “orthonormal”).\n\n\nOrthogonal Mappings\n\nMatrices with orthonormal columns are particularly important.\n\n\nTheorem. A \\(m\\times n\\) matrix \\(U\\) has orthonormal columns if and only if \\(U^TU = I\\).\n\n\nProof. Let us suppose that \\(U\\) has only three columns which are each vectors in \\(\\mathbb{R}^m\\) (but the proof will generalize to \\(n\\) columns).\n\n\nLet \\(U = [\\mathbf{u}_1\\;\\mathbf{u}_2\\;\\mathbf{u}_3].\\) Then:\n\\[U^TU = \\begin{bmatrix}\\mathbf{u}_1^T\\\\\\mathbf{u}_2^T\\\\\\mathbf{u}_3^T\\end{bmatrix}\\begin{bmatrix}\\mathbf{u}_1&\\mathbf{u}_2&\\mathbf{u}_3\\end{bmatrix}\\]\n\n\n\\[ = \\begin{bmatrix}\n\\mathbf{u}_1^T\\mathbf{u}_1&\\mathbf{u}_1^T\\mathbf{u}_2&\\mathbf{u}_1^T\\mathbf{u}_3\\\\\n\\mathbf{u}_2^T\\mathbf{u}_1&\\mathbf{u}_2^T\\mathbf{u}_2&\\mathbf{u}_2^T\\mathbf{u}_3\\\\\n\\mathbf{u}_3^T\\mathbf{u}_1&\\mathbf{u}_3^T\\mathbf{u}_2&\\mathbf{u}_3^T\\mathbf{u}_3\n\\end{bmatrix}\\]\n\n\nThe columns of \\(U\\) are orthogonal if and only if\n\\[\\mathbf{u}_1^T\\mathbf{u}_2 = \\mathbf{u}_2^T\\mathbf{u}_1 = 0,\\;\\; \\mathbf{u}_1^T\\mathbf{u}_3 = \\mathbf{u}_3^T\\mathbf{u}_1 = 0,\\;\\; \\mathbf{u}_2^T\\mathbf{u}_3 = \\mathbf{u}_3^T\\mathbf{u}_2 = 0\\]\nThe columns of \\(U\\) all have unit length if and only if\n\\[\\mathbf{u}_1^T\\mathbf{u}_1 = 1,\\;\\;\\mathbf{u}_2^T\\mathbf{u}_2 = 1,\\;\\;\\mathbf{u}_3^T\\mathbf{u}_3 = 1.\\]\nSo \\(U^TU = I.\\)\n\n\n\nOrthonormal Matrices Preserve Length and Orthogonality\n\nTheorem. Let \\(U\\) by an \\(m\\times n\\) matrix with orthonormal columns, and let \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) be in \\(\\mathbb{R}^n.\\) Then:\n  1. \\(\\Vert U\\mathbf{x}\\Vert = \\Vert\\mathbf{x}\\Vert.\\)   2. \\((U\\mathbf{x})^T(U\\mathbf{y}) = \\mathbf{x}^T\\mathbf{y}.\\)\n\n\nProof of (1):\n\\[\\Vert U\\mathbf{x}\\Vert = \\sqrt{(U\\mathbf{x})^T U\\mathbf{x}} = \\sqrt{\\mathbf{x}^T\\mathbf{x}} = \\Vert\\mathbf{x}\\Vert.\\]\n\n\nProof of (2):\n\\[(U\\mathbf{x})^T(U\\mathbf{y}) = \\mathbf{x}^TU^T(U\\mathbf{y}) = \\mathbf{x}^T(U^TU)\\mathbf{y} = \\mathbf{x}^T\\mathbf{y}.\\]\n\n\nThese make important statements:\n\nProperty (1) means that the mapping \\(\\mathbf{x}\\mapsto U\\mathbf{x}\\) preserves the lengths of vectors – and, therefore, also the distance between any two vectors.\nProperties (1) and (2) together mean that the mapping \\(\\mathbf{x}\\mapsto U\\mathbf{x}\\) preserves the angles between vectors – and, therefore, also any orthogonality between two vectors.\n\n\n\nSo, viewed as a linear operator, an orthonormal matrix is very special: the lengths of vectors, and therefore the distances between points is not changed by the action of \\(U\\).\n\n\nNotice as well that \\(U\\) is \\(m \\times n\\) – it may not be square.\nSo it may map vectors from one vector space to an entirely different vector space – but the distances between points will not be changed!\n\n\n\n\n\n\n\n… and the orthogonality of vectors will not be changed!\n\n\n\n\n\n\n\nNote however that we cannot in general construct an orthonormal map from a higher dimension to a lower one.\nFor example, three orthogonal vectors in \\(\\mathbb{R}^3\\) cannot be mapped to three orthogonal vectors in \\(\\mathbb{R}^2\\). Can you see why this is impossible? What is it about the definition of an orthonormal set that prevents this?\n\nExample.\nLet \\(U = \\begin{bmatrix}1/\\sqrt{2}&2/3\\\\1/\\sqrt{2}&-2/3\\\\0&1/3\\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix}\\sqrt{2}\\\\3\\end{bmatrix}.\\)\nNotice that \\(U\\) has orthonormal columns, so\n\\[U^TU = \\begin{bmatrix}1/\\sqrt{2}&1/\\sqrt{2}&0\\\\2/3&-2/3&1/3\\end{bmatrix}\\begin{bmatrix}1/\\sqrt{2}&2/3\\\\1/\\sqrt{2}&-2/3\\\\0&1/3\\end{bmatrix} = \\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}.\\]\n\nLet’s verify that \\(\\Vert Ux\\Vert = \\Vert x\\Vert.\\)\n\\[U\\mathbf{x} = \\begin{bmatrix}1/\\sqrt{2}&2/3\\\\1/\\sqrt{2}&-2/3\\\\0&1/3\\end{bmatrix}\\begin{bmatrix}\\sqrt{2}\\\\3\\end{bmatrix} = \\begin{bmatrix}3\\\\-1\\\\1\\end{bmatrix}\\]\n\n\n\\[\\Vert U\\mathbf{x}\\Vert = \\sqrt{9 + 1 + 1} = \\sqrt{11}.\\]\n\n\n\\[\\Vert\\mathbf{x}\\Vert = \\sqrt{2+9}= \\sqrt{11}.\\]\n\n\n\nWhen Orthonormal Matrices are Square\nFinally, one of the most useful transformation matrices is obtained when the columns of the matrix are orthonormal…\n… and the matrix is square.\n\nThese matrices map vectors in \\(\\mathbb{R}^n\\) to new locations in the same space, ie, \\(\\mathbb{R}^n\\).\n… in a way that preserves lengths, distances and orthogonality.\n\n\nNow, consider the case when \\(U\\) is square, and has orthonormal columns.\nThen the fact that \\(U^TU = I\\) implies that \\(U^{-1} = U^T.\\)\n\n\nThen \\(U\\) is called an orthogonal matrix.\n\n\nA good example of an orthogonal matrix is a rotation matrix:\n\\[{\\displaystyle R ={\\begin{bmatrix}\\cos \\theta &-\\sin \\theta \\\\\\sin \\theta &\\cos \\theta \\\\\\end{bmatrix}}.}\\]\n\n\nUsing trigonometric identities, you should be able to convince yourself that\n\\[R^TR =  I\\]\nand hopefully you can visualize how \\(R\\) preserves lengths and orthogonality.",
    "crumbs": [
      "Orthogonal Sets and Projection"
    ]
  },
  {
    "objectID": "L02Numerics.html",
    "href": "L02Numerics.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Photo Credit: George M. Bergman, CC BY-SA 4.0, via Wikimedia Commons\n\nWilliam Kahan, creator of the IEEE-754 standard. Turing Award Winner, 1989\n\n\nI have a number in my head  Though I don’t know why it’s there When numbers get serious You see their shape everywhere\nPaul Simon\n\nOne of the themes of this course will be shifting between mathematical and computational views of various concepts.\nToday we need to talk about why the answers we get from computers can be different from the answers we get mathematically\n– for the same question!\n\nThe root of the problem has to do with how numbers are manipulated in a computer.\nIn other words, how numbers are represented.",
    "crumbs": [
      "(Getting Serious About) Numbers"
    ]
  },
  {
    "objectID": "L02Numerics.html#getting-serious-about-numbers",
    "href": "L02Numerics.html#getting-serious-about-numbers",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Photo Credit: George M. Bergman, CC BY-SA 4.0, via Wikimedia Commons\n\nWilliam Kahan, creator of the IEEE-754 standard. Turing Award Winner, 1989\n\n\nI have a number in my head  Though I don’t know why it’s there When numbers get serious You see their shape everywhere\nPaul Simon\n\nOne of the themes of this course will be shifting between mathematical and computational views of various concepts.\nToday we need to talk about why the answers we get from computers can be different from the answers we get mathematically\n– for the same question!\n\nThe root of the problem has to do with how numbers are manipulated in a computer.\nIn other words, how numbers are represented.",
    "crumbs": [
      "(Getting Serious About) Numbers"
    ]
  },
  {
    "objectID": "L02Numerics.html#representations",
    "href": "L02Numerics.html#representations",
    "title": "Geometric Algorithms",
    "section": "Representations",
    "text": "Representations\nA number is a mathematical concept – an abstract idea.\n\n\nGod made the integers, all else is the work of man.\nLeopold Kronecker (1823 - 1891)\n\n\n\nIn a computer we assign bit patterns to correspond to certain numbers.\nWe say the bit pattern is the number’s representation.\n\n\nFor example the number ‘3.14’ might have the representation ‘01000000010010001111010111000011’.\nFor reasons of efficiency, we use a fixed number of bits for these representations. In most computers nowadays we use 64 bits to represent a number.\n\n\nLet’s look at some number representations and see what they imply about computations.",
    "crumbs": [
      "(Getting Serious About) Numbers"
    ]
  },
  {
    "objectID": "L02Numerics.html#integers",
    "href": "L02Numerics.html#integers",
    "title": "Geometric Algorithms",
    "section": "Integers",
    "text": "Integers\n\nKronecker believed that integers were the only ‘true’ numbers.\nAnd for the most part, using integers in a computer is not complicated.\nInteger representation is essentially the same as binary numerals.\nFor example, in a 64-bit computer, the representation of the concept of ‘seven’ would be ‘0..0111’ (with 61 zeros in the front).\nThere is a size limit on the largest value that can be stored as an integer, but it’s so big we don’t need to concern ourselves with it in this course.\n\n\nSo for our purposes, an integer can be stored exactly.\nIn other words, there is an 1-1 correspondence between every (computational) representation and the corresponding (mathematical) integer.\n\n\nSo, what happens when we compute with integers?\nFor (reasonably sized) integers, computation is exact …. as long as it only involves addition, subtraction, and multiplication.\nIn other words, there are no errors introduced when adding, subtracting or multiplying integers.\n\n\nHowever, it is a different story when we come to division, because the integers are not closed under division.\nFor example, 2/3 is not an integer. … It is, however, a real number.",
    "crumbs": [
      "(Getting Serious About) Numbers"
    ]
  },
  {
    "objectID": "L02Numerics.html#real-numbers-and-floating-point-representations",
    "href": "L02Numerics.html#real-numbers-and-floating-point-representations",
    "title": "Geometric Algorithms",
    "section": "Real Numbers and Floating-Point Representations",
    "text": "Real Numbers and Floating-Point Representations\n\nRepresenting a real number in a computer is a much more complicated matter.\nIn fact, for many decades after electronic computers were developed, there was no accepted “best” way to do this!\nEventually (in the 1980s) a widely accepted standard emerged, called IEEE-754. This is what almost all computers now use.\n\n\nThe style of representation used is called floating point.\n\n\nConceptually, it is similar to “scientific notation.”\n\\[\n123456 = \\underbrace{1.23456}_{\\text{significand}}\\times {\\underbrace{10}_{\\text{base}}}^{\\overbrace{5}^{exponent}}\n\\]\n\n\nExcept that it is encoded in binary:\n\\[17 = \\underbrace{1.0001}_{\\text{significand}}\\times {\\underbrace{2}_{\\text{base}}}^{\\overbrace{4}^{exponent}}\\]\n\n\nThe sign, significand, and exponent are all contained within the 64 bits.\n\n\n\n\n\n\n\n\n\nBy Codekaizen (Own work) [GFDL or CC BY-SA 4.0-3.0-2.5-2.0-1.0], via Wikimedia Commons\n\n\nBecause only a fixed number of bits are used, most real numbers cannot be represented exactly in a computer.\nAnother way of saying this is that, usually, a floating point number is an approximation of some particular real number.\n\n\nGenerally when we try to store a real number in a computer, what we wind up storing is the closest floating point number that the computer can represent.\n\n\nThe Relative Error of a Real Number stored in a Computer\n\n\nYou can experiment with floating point representations to see how errors arise using this interactive tool.\n\nThe way to think about working with floating point (in fact, how the hardware actually does it) is:\n\nRepresent each input as the nearest representable floating point number.\nCompute the result exactly from the floating point representations.\nReturn the nearest representable floating point number to the result.\n\n\n\nWhat does “nearest” mean? Long story short, it means “round to the nearest representable value.”\nLet’s say we have a particular real number \\(r\\) and we represent it as a floating point value \\(f\\).\nThen \\(r = f + \\epsilon\\) where \\(\\epsilon\\) is the amount that \\(r\\) was rounded when represented as \\(f\\).\n\n\nSo \\(\\epsilon\\) is the difference between the value we want, and the value we get.\nHow big can this difference be? Let’s say \\(f\\) is\n\\[f = \\underbrace{1.010...01}_\\text{53 bits}\\times 2^n\\]\n\n\nThen \\(|\\epsilon|\\) must be smaller than\n\\[|\\epsilon| &lt; \\underbrace{0.000...01}_\\text{53 bits}\\times 2^n.\\]\n\n\nSo as a relative error,\n\\[ \\text{relative error} = \\frac{|\\epsilon|}{f} &lt; \\frac{{0.000...01}\\times 2^n}{\\underbrace{1.000...00}_\\text{53 bits}\\times 2^n} = 2^{-52} \\approx 10^{-16}\\]\n\n\nThis value \\(10^{-16}\\) is an important one to remember.\nIt is approximately the relative error that can be introduced any time a real number is stored in a computer.\n\n\nAnother way of thinking about this is that you only have about 16 digits of accuracy in a floating point number.\n\n\n\nImplications of Representation Error\n\nProblems arise when we work with floating point numbers and confuse them with real numbers.\nIt is important to remember that most of the time we are not storing the real number exactly, but only a floating point number that is close to it.\n\n\nLet’s look at some examples. First:\n\\[ \\left( \\frac{1}{8} \\cdot 8 \\right) - 1 \\]\n\n\n\n# ((1/8)*8)-1\na = 1/8\nb = 8\nc = 1\n(a*b)-c\n\n0.0\n\n\n\n\nIt turns out that 1/8, 8, and 1 can all be stored exactly in IEEE-754 floating point format.\nSo, we are * storing the inputs exactly (1/8, 8 and 1) * computing the results exactly (by definition of IEEE-754), yielding \\((1/8) \\cdot 8 = 1\\) * and representing the result exactly (zero)\n\n\nOK, here is another example:\n\\[ \\left( \\frac{1}{7} \\cdot 7 \\right) - 1 \\]\n\n\n\n# ((1/7)*7)-1\na = 1/7\nb = 7\nc = 1\na * b - c\n\n0.0\n\n\n\n\nHere the situation is different.\n1/7 can not be stored exactly in IEEE-754 floating point format.\nIn binary, 1/7 is \\(0.001\\overline{001}\\), an infinitely repeating pattern that obviously cannot be represented as a finite sequence of bits.\n\n\nNonetheless, the computation \\((1/7) \\cdot 7\\) still yields exactly 1.0.\nWhy? Because the rounding of \\(0.001\\overline{001}\\) to its closest floating point representation, when multiplied by 7, yields a value whose closest floating point representation is 1.0.\n\n\nNow, let’s do something that seems very similar:\n\\[ \\left( \\frac{1}{70} \\cdot 7 \\right) - 0.1 \\]\n\n\n\na = 1/70\nb = 7\nc = 0.1\na * b - c\n\n-1.3877787807814457e-17\n\n\n\n\nIn this case, both 1/70 and 0.1 cannot be stored exactly.\nMore importantly, the process of rounding 1/70 to its closest floating point representation, then multiplying by 7, yields a number whose closest floating point representation is not 0.1\n\n\nHowever, that floating point representation is very close to 0.1.\nLet’s look at the difference: -1.3877787807814457e-17.\nThis is about \\(-1 \\cdot 10^{-17}\\).\n\n\nIn other words, about -0.00000000000000001\nCompared to 0.1, this is a very small number. The relative error is about:\n\\[ \\frac{|-0.00000000000000001|}{0.1} \\]\nwhich is about \\(10^{-16}.\\)\n\n\nThis suggests that when a floating point calculation is not exact, the error (in a relative sense) is usually very small.\n\n\nNotice also that in our example the size of the relative error is about \\(10^{-16}\\).\nRecall that the significand in IEEE-754 uses 52 bits.\nNow, note that \\(2^{-52} \\approx 10^{-16}\\).\nThere’s our “sixteen digits of accuracy” principle again.\n\n\n\nSpecial Values\n\nThere are three kinds of special values defined by IEEE-754:\n\nNaN, which means “Not a Number”\nInfinity – both positive and negative\nZero – both positive and negative.\n\n\n\nNaN and Inf behave about as you’d expect.\nIf you get one of these values in a computation you should be able to reason about how it happened. Note that these are values, and can be assigned to variables.\n\n\n\nnp.sqrt(-1)\n\nnp.float64(nan)\n\n\n\n\n\nnp.log(0)\n\nnp.float64(-inf)\n\n\n\n\n\n1/np.log(0)\n\nnp.float64(-0.0)\n\n\n\n\nAs far as we are concerned, there is no difference between positive and negative zero. You can ignore the minus sign in front of a negative zero. (If you are curious why there is a negative zero, see the online notes.)\n\n\n\nThe reason for having a negative and positive zero is the following.\nRemember that, due to the limitations of floating point representation, we can only store the nearest representable number to the one we’d like to store.\nSo, let’s say we try to store a number \\(x\\) that is very close to zero. To be specific, let \\(|x| &lt; 2.2 \\times 10^{-308}\\). Then the closest floating point representation is zero, so that is what is stored. This is known as “underflow”.\nBut … the number \\(x\\) that we were trying to store could have been positive or negative. So the standard defines a positive and negative zero. The sign of zero tells us when underflow occurred, “which direction” the underflow came from.\nThis can be useful in some numerical algorithms.\n\n\nvar = np.nan\nvar + 7\n\nnan\n\n\n\n\n\nvar = np.inf\nvar + 7\n\ninf",
    "crumbs": [
      "(Getting Serious About) Numbers"
    ]
  },
  {
    "objectID": "L02Numerics.html#mathematical-computation-vs.-mechanical-computation",
    "href": "L02Numerics.html#mathematical-computation-vs.-mechanical-computation",
    "title": "Geometric Algorithms",
    "section": "Mathematical Computation vs. Mechanical Computation",
    "text": "Mathematical Computation vs. Mechanical Computation\n\nIn a mathematical theorem, working with (idealized) numbers, it is always true that:\nIf\n\\[c = 1/a\\]\nthen\n\\[abc = b.\\]\nIn other words,\n\\[(ab)/a = b.\\]\n\n\nLet’s test whether this is always true in actual computation.\n\n\n\na = 7\nb = 1/10\nc = 1/a\na*c*b\n\n0.1\n\n\n\n\n\nb*c*a\n\n0.09999999999999999\n\n\n\n\n\na*c*b == b*c*a\n\nFalse\n\n\n\n\nHere is another example:\n\n0.1 + 0.1 + 0.1 \n\n0.30000000000000004\n\n\n\n\n\n3 * (0.1) - 0.3\n\n5.551115123125783e-17\n\n\n\nWhat does all this mean for us in practice?\nI will now give you three principles to keep in mind when computing with floating point numbers.\n\nPrinciple 1: Do not compare floating point numbers for equality\n\nTwo floating point computations that should yield the same result mathematically, may not do so due to rounding error.\nHowever, in general, if two numbers should be equal, the relative error of the difference in the floating point should be small.\nSo, instead of asking whether two floating numbers are equal, we should ask whether the relative error of their difference is small.\n\n\n\nr1 = a * b * c\nr2 = b * c * a\nnp.abs(r1-r2)/r1\n\nnp.float64(1.3877787807814457e-16)\n\n\n\n\n\nprint(r1 == r2)\n\nFalse\n\n\n\n\n\nprint(np.abs(r1 -  r2)/np.max([r1, r2]) &lt; np.finfo('float').resolution)\n\nTrue\n\n\n\n\nThis test is needed often enough that numpy has a function that implements it:\n\nnp.isclose(r1, r2)\n\nnp.True_\n\n\n\n\nSo another way to state Rule 1 for our purposes is:\n… always use np.isclose() to compare floating point numbers for equality!\n\n\nNext, we will generalize this idea a bit:\nbeyond the fact that numbers that should be equal, may not be in practice,\nwe can also observe that it can be hard to be accurate about the difference between two numbers that are nearly equal. This leads to the next two principles.\n\n\n\nPrinciple 2: Beware of ill-conditioned problems\n\nAn ill-conditioned problem is one in which the outputs depend in a very sensitive manner on the inputs.\nThat is, a small change in the inputs can yield a very large change in the outputs.\nThe simplest example is computing \\(1/(a-b)\\).\n\n\n\nprint(f'r1 is {r1}')\nprint(f'r2 is very close to r1')\nr3 = r1 + 0.0001\nprint(f'r3 is 0.1001')\n\nr1 is 0.1\nr2 is very close to r1\nr3 is 0.1001\n\n\n\n\nLet’s look at\n\\[ \\frac{1}{r1 - r2} \\text{ versus } \\frac{1}{r3-r2} \\]\n\n\n\nprint(f'1/(r1 - r2) = {1/(r1 - r2)}')\nprint(f'1/(r3 - r2) = {1/(r3 - r2)}')\n\n1/(r1 - r2) = 7.205759403792794e+16\n1/(r3 - r2) = 9999.999999998327\n\n\nIf \\(a\\) is close to \\(b\\), small changes in either make a big difference in the output.\n\n\nBecause the inputs to a problem may not be exact, if the problem is ill-conditioned, the outputs may be wrong by a large amount.\n\n\nLater on we will see that the notion of ill-conditioning applies to matrix problems too, and in particular comes up when we solve certain problems involving matrices.\n\n\n\nPrinciple 3: Relative error can be magnified during subtractions\n\nTwo numbers, each with small relative error, can yield a value with large relative error if subtracted.\n\n\nLet’s say we represent a = 1.2345 as 1.2345002 – the relative error is 0.0000002.\n\n\nLet’s say we represent b = 1.234 as 1.2340001 – the relative error is 0.0000001.\n\n\nNow, subtract a - b: the result is .0005001.\n\n\nWhat is the relative error? 0.005001 - 0.005 / 0.005 = 0.0002\n\n\nThe relative error of the result is 1000 times larger than the relative error of the inputs.\n\n\nHere’s an example in practice:\n\na = 1.23456789\nb = 1.2345678\nprint(0.00000009)\nprint(a-b)\n\n9e-08\n8.999999989711682e-08\n\n\n\n\n\nprint(np.abs(a-b-0.00000009)/ 0.00000009)\n\n1.1431464011915431e-09\n\n\n\n\nWe know the relative error in the inputs is on the order of \\(10^{-16}\\), but the relative error of the output is on the order of \\(10^{-9}\\) – i.e., a million times larger.\n\n\n\nFurther Reading\n\nFurther information about how Python handles issues around floating point is at Floating Point Arithmetic: Issues and Limitations.\nAn excellent, in-depth introduction to floating point is What Every Computer Scientist Should Know About Floating-Point Arithmetic.",
    "crumbs": [
      "(Getting Serious About) Numbers"
    ]
  },
  {
    "objectID": "L22LeastSquares.html",
    "href": "L22LeastSquares.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Ceres image credit: https://commons.wikimedia.org/wiki/File:Ceres_-RC3-Haulani_Crater(22381131691)_(cropped).jpg\nGood additional reading on Gauss’s use of Least Squares and determination of the orbit of Ceres is here.\n\n\n\n\n\n\n\n\nLet’s go back to week 1. A long time ago!\nRecall Gauss’s remarkable accomplishment in his early 20s. He took the set of measurements made by Piazzi of the dwarf planet Ceres and predicted where Ceres subsequently would appear in the sky (after it was lost behind the sun). This told Olbers exactly where to look, and lo and behold . . .\n\nWe can understand now a little better what Gauss had to do.\nKepler had discovered, and Newton had explained, that each planet orbits the sun following the path of an ellipse.\n\nTo describe the orbit of Ceres, Gauss had to construct the equation for its ellipse:\n\\[a_1 x_1^2 + a_2 x_2^2 + a_3 x_1x_2 + a_4 x_1 + a_5 x_2 + a_6 = 0.\\]\n\n\nHe had many measurements of \\((x_1, x_2)\\) pairs and had to find the \\(a_1, \\dots, a_6.\\)\n\n\nThis is actually a linear system:\n\\[\\begin{bmatrix}x_{11}^2 &x_{21}^2&x_{11}x_{21}&x_{11}&x_{21}&1\\\\x_{12}^2 &x_{22}^2&x_{12}x_{22}&x_{12}&x_{22}&1\\\\\n\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\x_{1n}^2 &x_{2n}^2&x_{1n}x_{2n}&x_{1n}&x_{2n}&1\\end{bmatrix} \\begin{bmatrix}a_1\\\\a_2\\\\a_3\\\\a_4\\\\a_5\\\\a_6\\end{bmatrix} = \\mathbf{0}\\]\n\n\nNow, according to Newton, this is a consistent linear system.\nThe equation for the ellipse is exactly correct and all we need is six \\((x_1, x_2)\\) sets of measurements to know the orbit of Ceres exactly.\n\n\nWhat could go wrong? :)\n\n\nObviously, there are going to be measurement errors in Piazzi’s observations.\nIf we just solve the system using six measurements, we will probably get incorrect values for the coefficients \\(a_1, \\dots, a_6.\\)",
    "crumbs": [
      "Least Squares"
    ]
  },
  {
    "objectID": "L22LeastSquares.html#least-squares",
    "href": "L22LeastSquares.html#least-squares",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Ceres image credit: https://commons.wikimedia.org/wiki/File:Ceres_-RC3-Haulani_Crater(22381131691)_(cropped).jpg\nGood additional reading on Gauss’s use of Least Squares and determination of the orbit of Ceres is here.\n\n\n\n\n\n\n\n\nLet’s go back to week 1. A long time ago!\nRecall Gauss’s remarkable accomplishment in his early 20s. He took the set of measurements made by Piazzi of the dwarf planet Ceres and predicted where Ceres subsequently would appear in the sky (after it was lost behind the sun). This told Olbers exactly where to look, and lo and behold . . .\n\nWe can understand now a little better what Gauss had to do.\nKepler had discovered, and Newton had explained, that each planet orbits the sun following the path of an ellipse.\n\nTo describe the orbit of Ceres, Gauss had to construct the equation for its ellipse:\n\\[a_1 x_1^2 + a_2 x_2^2 + a_3 x_1x_2 + a_4 x_1 + a_5 x_2 + a_6 = 0.\\]\n\n\nHe had many measurements of \\((x_1, x_2)\\) pairs and had to find the \\(a_1, \\dots, a_6.\\)\n\n\nThis is actually a linear system:\n\\[\\begin{bmatrix}x_{11}^2 &x_{21}^2&x_{11}x_{21}&x_{11}&x_{21}&1\\\\x_{12}^2 &x_{22}^2&x_{12}x_{22}&x_{12}&x_{22}&1\\\\\n\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\x_{1n}^2 &x_{2n}^2&x_{1n}x_{2n}&x_{1n}&x_{2n}&1\\end{bmatrix} \\begin{bmatrix}a_1\\\\a_2\\\\a_3\\\\a_4\\\\a_5\\\\a_6\\end{bmatrix} = \\mathbf{0}\\]\n\n\nNow, according to Newton, this is a consistent linear system.\nThe equation for the ellipse is exactly correct and all we need is six \\((x_1, x_2)\\) sets of measurements to know the orbit of Ceres exactly.\n\n\nWhat could go wrong? :)\n\n\nObviously, there are going to be measurement errors in Piazzi’s observations.\nIf we just solve the system using six measurements, we will probably get incorrect values for the coefficients \\(a_1, \\dots, a_6.\\)",
    "crumbs": [
      "Least Squares"
    ]
  },
  {
    "objectID": "L22LeastSquares.html#when-an-inconsistent-system-is-better-than-a-consistent-system",
    "href": "L22LeastSquares.html#when-an-inconsistent-system-is-better-than-a-consistent-system",
    "title": "Geometric Algorithms",
    "section": "When an Inconsistent System is Better than a Consistent System",
    "text": "When an Inconsistent System is Better than a Consistent System\n\nNotice that each time Piazzi takes a measurement of the position of Ceres, we add an additional equation to our linear system.\nJust using six measurements will certainly result in incorrect coefficients due to measurement error.\n\n\nA better idea is to use all of the \\(n\\) measurement data available, and try to find a way to cancel out errors.\nSo, using all the \\(n\\) data measurements available, we construct a linear system:\n\\[ X\\mathbf{a} = \\mathbf{b}\\]\nwhere \\(X\\) is \\(n\\times 6\\) and \\(\\mathbf{b} \\in \\mathbb{R}^n\\).\n\n\nBut now, due to measurement errors, we can’t expect \\(\\mathbf{b}\\) will lie in the column space of \\(X.\\) We have an inconsistent system.\nThis system has no solutions!\n\n\nWhat can we do if \\(A\\mathbf{x} = \\mathbf{b}\\) has no solutions?\n\n\nHere is the key idea: the fact that our measurements include errors does not make our measurements worthless!\nWe simply need a principled approach to doing the best job we can given the errors in our measurements.\nLet’s see how we can do that.\n\nWe now understand if \\(A\\) is \\(m\\times n\\) and \\(A\\mathbf{x} = \\mathbf{b}\\) has no solutions, that is because\n\nthe columns of \\(A\\) do not span \\(\\mathbb{R}^m\\), and\n\\(\\mathbf{b}\\) is not in the column space of \\(A\\).\n\n\nHere is an example we can visualize, in which \\(A\\) is \\(3 \\times 2\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the problem of Ceres’ orbit, the reason that \\(\\mathbf{b}\\) does not lie in \\(\\operatorname{Col} A\\) is due to measurement error.\n\n\nFinding a Good Approximate Solution\n\nIf we make the assumption that measurement errors are small, then we should be quite satisfied to find an \\(\\mathbf{x}\\) that makes \\(A\\mathbf{x}\\) as close as possible to \\(\\mathbf{b}.\\)\n\n\nIn other words, we are looking for an \\(\\mathbf{x}\\) such that \\(A\\mathbf{x}\\) makes a good approximation to \\(\\mathbf{b}.\\)\n\n\n\n\n\n&lt;&gt;:31: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:31: SyntaxWarning: invalid escape sequence '\\m'\n/var/folders/y0/f8_7klyx3sjbkw2pj03bnj040000gn/T/ipykernel_5591/1816056809.py:31: SyntaxWarning: invalid escape sequence '\\m'\n  'We should be happy with an $\\mathbf{x} such that AX is close to b', size = 16)\n\n\n\n\n\n\n\n\n\n\n\nWe can think of the error of the approximation of \\(A\\mathbf{x}\\) to \\(\\mathbf{b}\\) as the distance from \\(A\\mathbf{x}\\) to \\(\\mathbf{b},\\) which is\n\\[\\Vert A\\mathbf{x} - \\mathbf{b}\\Vert.\\]\n\n\n\n&lt;&gt;:31: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:31: SyntaxWarning: invalid escape sequence '\\m'\n/var/folders/y0/f8_7klyx3sjbkw2pj03bnj040000gn/T/ipykernel_5591/1816056809.py:31: SyntaxWarning: invalid escape sequence '\\m'\n  'We should be happy with an $\\mathbf{x} such that AX is close to b', size = 16)",
    "crumbs": [
      "Least Squares"
    ]
  },
  {
    "objectID": "L22LeastSquares.html#the-general-least-squares-problem",
    "href": "L22LeastSquares.html#the-general-least-squares-problem",
    "title": "Geometric Algorithms",
    "section": "The General Least-Squares Problem",
    "text": "The General Least-Squares Problem\nWe can now formally express what we are looking for when we seek a “good” solution to an inconsistent system:\nThe general least-squares problem is to find an \\(\\mathbf{x}\\) that makes \\(\\Vert A\\mathbf{x}-\\mathbf{b}\\Vert\\) as small as possible.\n\nThis is called “least squares” because it is equivalent to minimizing \\(\\Vert A\\mathbf{x}-\\mathbf{b}\\Vert^2,\\) which is the sum of squared differences.\n\nTo make this correspondence explicit: say that we denote \\(A\\mathbf{x}\\) by \\(\\mathbf{y}\\). Then\n\\[\\Vert A\\mathbf{x}-\\mathbf{b}\\Vert^2 = \\Vert \\mathbf{y}-\\mathbf{b}\\Vert^2 = \\sum_i (y_i-b_i)^2\\]\nWhere we interpret \\(y_i\\) as the estimated value and \\(b_i\\) as the measured value.\nSo this expression is the sum of squared error.\nThis is the most common measure of error used in statistics.\n\nThis is a key principle!\nMinimizing the length of \\(A\\mathbf{x} - \\mathbf{b}\\) is the same as minimizing the sum of squared error.\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition. If A is \\(m\\times n\\) and \\(\\mathbf{b}\\) is in \\(\\mathbb{R}^m,\\) a least squares solution of \\(A\\mathbf{x} =\\mathbf{b}\\) is an \\(\\mathbf{\\hat{x}}\\) in \\(\\mathbb{R}^n\\) such that\n\\[\\Vert A\\mathbf{\\hat{x}} - \\mathbf{b}\\Vert \\leq \\Vert A\\mathbf{x} - \\mathbf{b}\\Vert\\]\nfor all \\(\\mathbf{x}\\) in \\(\\mathbb{R}^n\\).\n\nAn equivalent (and more common) way to express this is:\n\\[\\hat{\\mathbf{x}} = \\arg\\min_\\mathbf{x} \\Vert A\\mathbf{x} - \\mathbf{b}\\Vert.\\]\nwhich emphasizes that this is a minimization problem, also called an optimization problem.\n\n\nInterpretation of the Least Squares Problem\n\nThe point here is that no matter what \\(\\mathbf{x}\\) is, \\(A\\mathbf{x}\\) will be in the column space of \\(A\\) — that is, \\(\\operatorname{Col}A\\).\nSo in our problem,\n\n\\(\\mathbf{b}\\) is outside \\(\\operatorname{Col}A\\), and\nwe are looking for \\(\\hat{\\mathbf{x}}\\),\nwhich specifies the closest point in \\(\\operatorname{Col}A\\) to \\(\\mathbf{b}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe vector \\(\\mathbf{b}\\) is closer to \\(A\\mathbf{\\hat{x}}\\) than it is to \\(A\\mathbf{x}\\) for any other \\(\\mathbf{x}\\).\nFor example, the red points in the figure are both further from \\(\\mathbf{b}\\) than is \\(A\\mathbf{\\hat{x}}\\).",
    "crumbs": [
      "Least Squares"
    ]
  },
  {
    "objectID": "L22LeastSquares.html#solving-the-general-least-squares-problem",
    "href": "L22LeastSquares.html#solving-the-general-least-squares-problem",
    "title": "Geometric Algorithms",
    "section": "Solving the General Least Squares Problem",
    "text": "Solving the General Least Squares Problem\n\nIn order to solve the Least Squares problem, we need to bring in a bit more theory.\nAll we need to do is to extend some of the ideas we developed in the last lecture.\nThe last lecture developed methods for finding the point in a 1D subspace that is closest to a given point.\n\n\nWe need to generalize the idea of “closest point” to the case of an arbitrary subspace.\nThis leads to two theorems: the Orthogonal Decomposition Theorem and the Best Approximation Theorem.\n\n\nThe Orthogonal Decomposition Theorem\n\nLet \\(W\\) be a subspace of \\(\\mathbb{R}^n\\). Then each \\(\\mathbf{y}\\) in \\(\\mathbb{R}^n\\) can be written uniquely in the form\n\\[ \\mathbf{y} = \\hat{\\mathbf{y}} + \\mathbf{z}\\]\nwhere \\(\\hat{\\mathbf{y}}\\) is in \\(W\\) and \\(\\mathbf{z}\\) is orthogonal to every vector in \\(W\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProof. (straightforward extension of the 1D case from last lecture.)\nJust as in the case of a 1D subspace (in the last lecture), we say that \\(\\hat{\\mathbf{y}}\\) is the orthogonal projection of \\(\\mathbf{y}\\) onto \\(W\\) and write \\(\\hat{\\mathbf{y}} = \\operatorname{proj}_W\\mathbf{y}.\\)\n\n\n\nThe Best Approximation Theorem\n\nLet \\(W\\) be a subspace of \\(\\mathbb{R}^n\\), let \\(\\mathbf{y}\\) be any vector in \\(\\mathbb{R}^n\\), and let \\(\\hat{\\mathbf{y}}\\) be the orthogonal projection of \\(\\mathbf{y}\\) onto \\(W\\). Then \\(\\hat{\\mathbf{y}}\\) is the closest point in \\(W\\) to \\(\\mathbf{y}\\), in the sense that\n\\[\\Vert \\mathbf{y}-\\hat{\\mathbf{y}} \\Vert &lt; \\Vert \\mathbf{y} - \\mathbf{v} \\Vert\\]\nfor all \\(\\mathbf{v}\\) in \\(W\\) distinct from \\(\\hat{\\mathbf{y}}\\).\n\n\nProof.\nTake \\(\\mathbf{v}\\) in \\(W\\) distinct from \\(\\hat{\\mathbf{y}}\\). Here is what the setup looks like:\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth \\(\\hat{\\mathbf{y}}\\) and \\(\\mathbf{v}\\) are in \\(W\\), so \\(\\hat{\\mathbf{y}} - \\mathbf{v}\\) is in \\(W\\).\n\n\nBy the orthogonal decomposition theorem, \\(\\mathbf{y} - \\hat{\\mathbf{y}}\\) is orthogonal to every vector in \\(W\\), so it is orthogonal to \\(\\hat{\\mathbf{y}} - \\mathbf{v}.\\)\n\n\nNow, these three points form a right triangle because\n\\[ \\mathbf{y} - \\mathbf{v} = (\\mathbf{y} - \\hat{\\mathbf{y}}) + (\\hat{\\mathbf{y}} - \\mathbf{v}). \\]\n\n\nSo the Pythagorean Theorem tells us that\n\\[ \\Vert\\mathbf{y} - \\mathbf{v}\\Vert^2 = \\Vert\\mathbf{y} - \\hat{\\mathbf{y}}\\Vert^2 + \\Vert\\hat{\\mathbf{y}} - \\mathbf{v}\\Vert^2. \\]\n\n\nNow \\(\\hat{\\mathbf{y}} - \\mathbf{v} \\neq {\\bf 0}\\) because \\(\\mathbf{y}\\) is distinct from \\(\\mathbf{v}\\).\n\n\nSo\n\\[\\Vert \\hat{\\mathbf{y}} - \\mathbf{v} \\Vert &gt; 0.\\]\n\n\nSo\n\\[ \\Vert\\mathbf{y} - \\mathbf{v}\\Vert^2 &gt; \\Vert\\mathbf{y} - \\hat{\\mathbf{y}}\\Vert^2. \\]\n\n\nSo we have shown a key fact:\n\\[ \\operatorname{Proj}_W \\mathbf{y} \\text{ is the closest point in } W \\text{ to } \\mathbf{y}.\\]",
    "crumbs": [
      "Least Squares"
    ]
  },
  {
    "objectID": "L22LeastSquares.html#orthogonal-projection-solves-least-squares",
    "href": "L22LeastSquares.html#orthogonal-projection-solves-least-squares",
    "title": "Geometric Algorithms",
    "section": "Orthogonal Projection Solves Least Squares",
    "text": "Orthogonal Projection Solves Least Squares\nLet’s apply these ideas to solving the least squares problem.\n\nHere is what we want to achieve:\n\\[\\hat{\\mathbf{x}} = \\arg\\min_\\mathbf{x} \\Vert A\\mathbf{x} - \\mathbf{b}\\Vert.\\]\nThat is, we want \\(A\\hat{\\mathbf{x}}\\) to be the closest point in \\(\\operatorname{Col}A\\) to \\(\\mathbf{b}\\).\n\n\n… and we now know that the closest point to \\(\\mathbf{b}\\) in a subspace \\(W\\) is the projection of \\(\\mathbf{b}\\) onto \\(W.\\)\n\n\nSo the point we are looking for, which we’ll call \\(\\hat{\\mathbf{b}},\\) is:\n\\[\\hat{\\mathbf{b}} = \\operatorname{proj}_{\\operatorname{Col}A} \\mathbf{b}\\]\n\n\nThe key is that \\(\\hat{\\mathbf{b}}\\) is in the column space of \\(A\\). So this equation is consistent, and we can solve it:\n\\[A\\mathbf{\\hat{x}} = \\hat{\\mathbf{b}}.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince \\(\\hat{\\mathbf{b}}\\) is the closest point in \\(\\operatorname{Col}A\\) to \\(\\mathbf{b},\\) a vector \\(\\hat{\\mathbf{x}}\\) is a least-squares solution of \\(A\\mathbf{x}=\\mathbf{b}\\) if and only if \\(\\mathbf{\\hat{x}}\\) satisfies \\(A\\mathbf{\\hat{x}} = \\hat{\\mathbf{b}}.\\)\n\n\nLet’s go back to the case that we can visualize.\n\n\\(A\\) is \\(3 \\times 2.\\)\nWe have only two columns \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2\\) so they cannot span \\(\\mathbb{R}^3\\).\nSo \\(\\mathbf{b}\\) may not lie in \\(\\operatorname{Col}A\\), and in our example it does not:\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd what we want to find is the projection of \\(\\mathbf{b}\\) onto the column space of \\(A\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Normal Equations\nSo: how are we going to find this projection \\(\\hat{\\mathbf{b}}\\)?\n\nHere is the key idea:\nWe know that the projection \\(\\hat{\\mathbf{b}}\\) has the property that \\(\\hat{\\mathbf{b}}-\\mathbf{b}\\) is orthogonal to \\(\\operatorname{Col}A.\\)\n\n\nSuppose \\(\\hat{\\mathbf{b}}\\) is \\(\\operatorname{proj}_{\\operatorname{Col}A}\\mathbf{b},\\) and that \\(\\mathbf{\\hat{x}}\\) satisfies \\(A\\mathbf{\\hat{x}} = \\hat{\\mathbf{b}}\\).\nSo \\(A\\mathbf{\\hat{x}} - \\mathbf{b}\\) is orthogonal to each column of \\(A\\).\n\n\nIf \\(\\mathbf{a}_j\\) is any column of \\(A\\), then\n\\[\\mathbf{a}_j^T(A\\mathbf{\\hat{x}} - \\mathbf{b}) = 0.\\]\n\n\nNow, each \\(\\mathbf{a}_j^T\\) is a row of \\(A^T\\).\nWe can collect all of the equations for all the \\(\\mathbf{a}_j\\) as:\n\\[A^T(A\\mathbf{\\hat{x}} - \\mathbf{b}) = {\\bf 0}.\\]\n\n\nSo\n\\[A^TA\\mathbf{\\hat{x}} - A^T\\mathbf{b} = {\\bf 0}\\]\n\n\nSo\n\\[A^TA\\mathbf{\\hat{x}} = A^T\\mathbf{b}\\]\n\n\nLooking at this, we see that \\(A^T\\mathbf{b}\\) is a vector, and \\(A^TA\\) is a matrix, so this is a standard linear system.\n\n\nThis linear system is called the normal equations for \\(A\\mathbf{x} = \\mathbf{b}.\\)\nIts solution is usually denoted \\(\\mathbf{\\hat{x}}\\).\n\nTheorem. The set of least-squares solutions of \\(A\\mathbf{x} = \\mathbf{b}\\) is equal to the (nonempty) set of solutions of the normal equations \\(A^TA\\mathbf{x} = A^T\\mathbf{b}.\\)\n\nProof.\n  (1) The set of solutions is nonempty. The matrix on the left has the same column space as \\(A^T\\) and the vector on the right is a vector in the column space of \\(A^T.\\)\nAnd, by the arguments above, any least-squares solution of \\(A\\mathbf{x} = \\mathbf{b}\\) must satisfy the normal equations \\(A^TA\\mathbf{x} = A^T\\mathbf{b}.\\)\n\n\n  (2) Now let’s show that any solution of \\(A^TA\\mathbf{x} = A^T\\mathbf{b}\\) is a least squares solution of \\(A\\mathbf{x} = \\mathbf{b}\\).\nIf \\(\\mathbf{\\hat{x}}\\) satisfies \\(A^TA\\mathbf{x} = A^T\\mathbf{b},\\) then \\(A^T(A\\mathbf{\\hat{x}} -\\mathbf{b}) = {\\bf 0},\\)\n\n\nwhich shows that \\(A\\mathbf{\\hat{x}} - \\mathbf{b}\\) is orthogonal to the rows of \\(A^T,\\) and so is orthogonal to the columns of \\(A\\).\n\n\nSo the vector \\(A\\mathbf{\\hat{x}} - \\mathbf{b}\\) is orthogonal to \\(\\operatorname{Col}A\\).\n\n\nSo the equation\n\\[\\mathbf{b} = A\\mathbf{\\hat{x}} + (\\mathbf{b} - A\\mathbf{\\hat{x}})\\]\nis a decomposition of \\(\\mathbf{b}\\) into the sum of a vector in \\(\\operatorname{Col}A\\) and a vector orthogonal to \\(\\operatorname{Col}A\\).\n\n\nSince the orthogonal decomposition is unique, \\(A\\mathbf{\\hat{x}}\\) must be the orthogonal projection of \\(\\mathbf{b}\\) onto the column space of \\(A\\).\nSo \\(A\\mathbf{\\hat{x}} = \\hat{\\mathbf{b}}\\) and \\(\\mathbf{\\hat{x}}\\) is a least-squares solution.\n\nExample. Find the least squares solution of the inconsistent system \\(A\\mathbf{x} = \\mathbf{b}\\) for\n\\[A = \\begin{bmatrix}4&0\\\\0&2\\\\1&1\\end{bmatrix}, \\;\\;\\; \\mathbf{b} = \\begin{bmatrix}2\\\\0\\\\11\\end{bmatrix}.\\]\n\nSolution.\nWe will use the normal equations \\(A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}.\\)\n\\[A^TA = \\begin{bmatrix}4&0&1\\\\0&2&1\\end{bmatrix} \\begin{bmatrix}4&0\\\\0&2\\\\1&1\\end{bmatrix} = \\begin{bmatrix}17&1\\\\1&5\\end{bmatrix}\\]\n\\[A^T\\mathbf{b} =  \\begin{bmatrix}4&0&1\\\\0&2&1\\end{bmatrix} \\begin{bmatrix}2\\\\0\\\\11\\end{bmatrix} = \\begin{bmatrix}19\\\\11\\end{bmatrix}\\]\n\n\nSo the normal equations are:\n\\[ \\begin{bmatrix}17&1\\\\1&5\\end{bmatrix}\\begin{bmatrix}\\hat{x}_1\\\\\\hat{x}_2\\end{bmatrix} = \\begin{bmatrix}19\\\\11\\end{bmatrix}\\]\n\n\nWe can solve this using row operations, or by inverting \\(A^TA\\) (if it is invertible).\n\\[(A^TA)^{-1} = \\frac{1}{84}\\begin{bmatrix}5&-1\\\\-1&17\\end{bmatrix}\\]\n\n\nSince \\(A^TA\\) is invertible, we can then solve \\(A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}\\) as\n\\[\\mathbf{\\hat{x}} = (A^TA)^{-1}A^T\\mathbf{b}\\]\n\n\n\\[ = \\frac{1}{84}\\begin{bmatrix}5&-1\\\\-1&17\\end{bmatrix}\\begin{bmatrix}19\\\\11\\end{bmatrix} = \\frac{1}{84}\\begin{bmatrix}84\\\\168\\end{bmatrix} = \\begin{bmatrix}1\\\\2\\end{bmatrix}.\\]\nSo we conclude that \\(\\mathbf{\\hat{x}} = \\begin{bmatrix}1\\\\2\\end{bmatrix}\\) is the vector that minimizes \\(\\Vert A\\mathbf{x} -\\mathbf{b}\\Vert.\\)\n\n\nMore formally,\n\\[\\mathbf{\\hat{x}} = \\arg\\min_{\\mathbf{x}} \\Vert A\\mathbf{x} - \\mathbf{b}\\Vert.\\]\nThat is, \\(\\mathbf{\\hat{x}}\\) is the least-squares solution of \\(A\\mathbf{x} = \\mathbf{b}\\).\n\n\n\nWhen the Normal Equations have Multiple Solutions\nWe have seen that the normal equations always have a solution.\nIs there always a unique solution?\n\nNo, there can be multiple solutions that all minimize \\(\\Vert A\\mathbf{x} - \\mathbf{b}\\Vert.\\)\n\n\nLet’s remind ourselves of what is going on when a linear system has multiple solutions.\nWe know that a linear system has multiple solutions when there are columns that are not pivot columns.\n\n\nEquivalently, when \\(A\\hat{\\mathbf{x}} = \\hat{\\mathbf{b}}\\) has multiple solutions, the columns of \\(A\\) are linearly dependent.\nHere is a picture of what is going on. In this case, \\(A\\) is \\(3 \\times 3\\).\nBut note, that \\(\\operatorname{Col}A\\) is only two-dimensional because the three columns are linearly dependent.\n\n\n\n\n\n\n\n\n\n\n\n\nExample.\nFind a least-squares solution for \\(A\\mathbf{x} = \\mathbf{b}\\) for\n\\[A = \\begin{bmatrix}1&1&0&0\\\\1&1&0&0\\\\1&0&1&0\\\\1&0&1&0\\\\1&0&0&1\\\\1&0&0&1\\end{bmatrix},\\;\\;\\; \\mathbf{b} = \\begin{bmatrix}-3\\\\-1\\\\0\\\\2\\\\5\\\\1\\end{bmatrix}.\\]\n\nSolution. Compute\n\\[A^TA = \\begin{bmatrix}1&1&1&1&1&1\\\\1&1&0&0&0&0\\\\0&0&1&1&0&0\\\\0&0&0&0&1&1\\end{bmatrix}\\begin{bmatrix}1&1&0&0\\\\1&1&0&0\\\\1&0&1&0\\\\1&0&1&0\\\\1&0&0&1\\\\1&0&0&1\\end{bmatrix} = \\begin{bmatrix}6&2&2&2\\\\2&2&0&0\\\\2&0&2&0\\\\2&0&0&2\\end{bmatrix}\\]\n\\[A^T\\mathbf{b} = \\begin{bmatrix}1&1&1&1&1&1\\\\1&1&0&0&0&0\\\\0&0&1&1&0&0\\\\0&0&0&0&1&1\\end{bmatrix}\\begin{bmatrix}-3\\\\-1\\\\0\\\\2\\\\5\\\\1\\end{bmatrix} = \\begin{bmatrix}4\\\\-4\\\\2\\\\6\\end{bmatrix}\\]\n\n\nTo solve \\(A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b},\\) we’ll use row reduction. The augmented matrix \\([A^TA\\; A^T\\mathbf{b}]\\) is:\n\\[\\begin{bmatrix}6&2&2&2&4\\\\2&2&0&0&-4\\\\2&0&2&0&2\\\\2&0&0&2&6\\end{bmatrix} \\sim \\begin{bmatrix}1&0&0&1&3\\\\0&1&0&-1&-5\\\\0&0&1&-1&-2\\\\0&0&0&0&0\\end{bmatrix}\\]\n\n\nSince there is a column without a pivot, we know the columns of \\(A^TA\\) are linearly dependent.\nThis happens because the columns of \\(A\\) are linearly dependent.\nYou can see this as follows: if \\(A\\) has a non-trivial null space, then \\(A^TA\\) also has a nontrival null space.\n\n\nSo there is a free variable.\nThe general solution is then \\(x_1 = 3-x_4\\), \\(x_2 = -5+x_4\\), \\(x_3 = -2 + x_4\\), and \\(x_4\\) is free.\nSo the general least-squares solution of \\(A\\hat{\\mathbf{x}} = \\mathbf{b}\\) has the form\n\\[\\mathbf{\\hat{x}} = \\begin{bmatrix}3\\\\-5\\\\-2\\\\0\\end{bmatrix} + x_4\\begin{bmatrix}-1\\\\1\\\\1\\\\1\\end{bmatrix}\\]\n\n\nKeep in mind that the orthogonal projection \\(\\hat{\\mathbf{b}}\\) is always unique.\nThe reason that there are multiple solutions to this least squares problem is that there are multiple ways to construct \\(\\hat{\\mathbf{b}}\\).\nThe reason that there are multiple ways to construct \\(\\hat{\\mathbf{b}}\\) is that the columns of \\(A\\) are linearly dependent, so any vector in the column space of \\(A\\) can be constructed in multiple ways.\n\n\nHere is a theorem that allows use to identify when there are multiple least-squares solutions.\n\nTheorem. Let \\(A\\) be an \\(m\\times n\\) matrix. The following statements are equivalent:\n  1. The equation \\(A\\mathbf{x} = \\mathbf{b}\\) has a unique least-squares solution for each \\(\\mathbf{b}\\) in \\(\\mathbb{R}^m.\\)\n  2. The columns of \\(A\\) are linearly independent.\n  3. The matrix \\(A^TA\\) is invertible.\nWhen these statements are true, the least-squares solution \\(\\mathbf{\\hat{x}}\\) is given by:\n\\[\\mathbf{\\hat{x}} = (A^TA)^{-1}A^T\\mathbf{b}\\]\n\n\nProjection onto an Abitrary Basis\n\nWhen \\(A^TA\\) is invertible, and \\(\\hat{\\mathbf{b}}\\) is unique, we can put together the two equations\n\\[\\mathbf{\\hat{x}} = (A^TA)^{-1}A^T\\mathbf{b}\\]\nand\n\\[A\\mathbf{\\hat{x}} = \\hat{\\mathbf{b}}\\]\nto get:\n\\[\\hat{\\mathbf{b}} = A(A^TA)^{-1}A^T\\mathbf{b}\\]\n\n\nLet’s stop and look at this from a very general standpoint.\nConsider \\(\\mathbf{b}\\) to be an arbitrary point, and \\(A\\) to be a matrix whose columns are a basis for a subspace (ie, \\(\\operatorname{Col} A\\)).\nThen \\(\\hat{\\mathbf{b}}\\) is the projection of \\(\\mathbf{b}\\) onto \\(\\operatorname{Col} A\\).\n\n\nUp until now we have seen how to project a point onto a line, or on to a subspace with an orthogonal basis.\nBut now we see that\n\\[ \\operatorname{proj}_{\\operatorname{Col} A} \\mathbf{b} = A(A^TA)^{-1}A^T\\mathbf{b} \\]\nSo we now have an expression for projection onto a subspace given an arbitrary basis. This is a general formula that can be very useful!",
    "crumbs": [
      "Least Squares"
    ]
  },
  {
    "objectID": "L26ApplicationsOfSVD.html",
    "href": "L26ApplicationsOfSVD.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": ":::: {.column width = “50%”} In “Nude Descending a Staircase” Marcel Duchamp captures a four-dimensional object on a two-dimensional canvas. Accomplishing this without losing essential information is called dimensionality reduction.\n\n:::::\n\n\nNude Descending a Staircase by Marcel Duchamp (1887-1968) - Philadelphia Museum of Art, PD-US\n\n\n\n\n\nImage from Wikipedia\n\nThe Singular Value Decomposition is the “Swiss Army Knife” and the “Rolls Royce” of matrix decompositions.\n\n– Diane O’Leary\nToday we will concern ourselves with the “Swiss Army Knife” aspect of the SVD.\nOur focus today will be on applications to data analysis.\nSo today we will be thinking of matrices as data.\n(Rather than thinking of matrices as linear operators.)\n\n\n\n\n\nImage Credit: xkcd\nAs a specific example, here is a typical data matrix. This matrix could be the result of measuring a collection of data objects, and noting a set of features for each object.\n\\[{\\text{$m$ data objects}}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{ccccc}\n\\begin{array}{c}a_{11}\\\\\\vdots\\\\a_{i1}\\\\\\vdots\\\\a_{m1}\\end{array}&\n\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n\\begin{array}{c}a_{1j}\\\\\\vdots\\\\a_{ij}\\\\\\vdots\\\\a_{mj}\\end{array}&\n\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n\\begin{array}{c}a_{1n}\\\\\\vdots\\\\a_{in}\\\\\\vdots\\\\a_{mn}\\end{array}\n\\end{array}\\right]}^{n \\text{~features}}\\]\n\nFor example, rows could be people, and columns could be movie ratings.\nOr rows could be documents, and columns could be words within the documents.",
    "crumbs": [
      "Applications of the SVD"
    ]
  },
  {
    "objectID": "L26ApplicationsOfSVD.html#applications-of-the-svd",
    "href": "L26ApplicationsOfSVD.html#applications-of-the-svd",
    "title": "Geometric Algorithms",
    "section": "",
    "text": ":::: {.column width = “50%”} In “Nude Descending a Staircase” Marcel Duchamp captures a four-dimensional object on a two-dimensional canvas. Accomplishing this without losing essential information is called dimensionality reduction.\n\n:::::\n\n\nNude Descending a Staircase by Marcel Duchamp (1887-1968) - Philadelphia Museum of Art, PD-US\n\n\n\n\n\nImage from Wikipedia\n\nThe Singular Value Decomposition is the “Swiss Army Knife” and the “Rolls Royce” of matrix decompositions.\n\n– Diane O’Leary\nToday we will concern ourselves with the “Swiss Army Knife” aspect of the SVD.\nOur focus today will be on applications to data analysis.\nSo today we will be thinking of matrices as data.\n(Rather than thinking of matrices as linear operators.)\n\n\n\n\n\nImage Credit: xkcd\nAs a specific example, here is a typical data matrix. This matrix could be the result of measuring a collection of data objects, and noting a set of features for each object.\n\\[{\\text{$m$ data objects}}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{ccccc}\n\\begin{array}{c}a_{11}\\\\\\vdots\\\\a_{i1}\\\\\\vdots\\\\a_{m1}\\end{array}&\n\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n\\begin{array}{c}a_{1j}\\\\\\vdots\\\\a_{ij}\\\\\\vdots\\\\a_{mj}\\end{array}&\n\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n\\begin{array}{c}a_{1n}\\\\\\vdots\\\\a_{in}\\\\\\vdots\\\\a_{mn}\\end{array}\n\\end{array}\\right]}^{n \\text{~features}}\\]\n\nFor example, rows could be people, and columns could be movie ratings.\nOr rows could be documents, and columns could be words within the documents.",
    "crumbs": [
      "Applications of the SVD"
    ]
  },
  {
    "objectID": "L26ApplicationsOfSVD.html#recap-of-svd",
    "href": "L26ApplicationsOfSVD.html#recap-of-svd",
    "title": "Geometric Algorithms",
    "section": "Recap of SVD",
    "text": "Recap of SVD\n\nTo start discussing the set of tools that SVD provides for analyzing data, let’s remind ourselves what the SVD is.\n\n\nToday we’ll work exclusively with the reduced SVD.\nHere it is again, for the case where \\(A\\) is \\(m \\times n\\), and \\(A\\) has rank \\(r\\).\nIn that case, the reduced SVD looks like this, with singular values on the diagonal of \\(\\Sigma\\):\n\\(\\tiny {\\small m}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{cccc}\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_1}\\\\\\vdots\\\\\\vdots\\end{array}&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_2}\\\\\\vdots\\\\\\vdots\\end{array}&\\dots&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_n}\\\\\\vdots\\\\\\vdots\\end{array}\\\\\\end{array}\\right]}^{\\small n} =\n\\overbrace{\\left[\\begin{array}{ccc}\\vdots&&\\vdots\\\\\\vdots&&\\vdots\\\\\\mathbf{u}_1&\\cdots&\\mathbf{u}_r\\\\\\vdots&&\\vdots\\\\\\vdots&&\\vdots\\end{array}\\right]}^{\\small r}\n\\times\n\\overbrace{\\left[\\begin{array}{ccc}\\sigma_1& &\\\\&\\ddots&\\\\&&\\sigma_r\\end{array}\\right]}^{\\small r}\n\\times\n\\overbrace{\\left[\\begin{array}{ccccc}\\dots&\\dots&\\mathbf{v}_1&\\dots&\\dots\\\\&&\\vdots&&\\\\\\dots&\\dots&\\mathbf{v}_r&\\dots&\\dots\\end{array}\\right]}^{\\small n}\\)\n\n\n\\[\\Large\\overset{m\\,\\times\\, n}{A^{\\vphantom{T}}} = \\overset{m\\,\\times\\, r}{U^{\\vphantom{T}}}\\;\\;\\overset{r\\,\\times\\, r}{\\Sigma^{\\vphantom{T}}}\\;\\;\\overset{r\\,\\times\\, n}{V^T}\\]",
    "crumbs": [
      "Applications of the SVD"
    ]
  },
  {
    "objectID": "L26ApplicationsOfSVD.html#approximating-a-matrix",
    "href": "L26ApplicationsOfSVD.html#approximating-a-matrix",
    "title": "Geometric Algorithms",
    "section": "Approximating a Matrix",
    "text": "Approximating a Matrix\n\nTo understand the power of SVD for analyzing data, it helps to think of it as a tool for approximating one matrix by another, simpler, matrix.\n\n\nTo talk about when one matrix approximates another, we need a “length” for matrices.\n\n\nWe will use the Frobenius norm.\nThe Frobenius norm is just the usual vector norm, treating the matrix as if it were a vector.\nIn other words, the definition of the Frobenius norm of \\(A\\), denoted \\(\\Vert A\\Vert_F\\), is:\n\\[\\Vert A\\Vert_F = \\sqrt{\\sum a_{ij}^2}.\\]\n\nThe approximations we’ll discuss are low-rank approximations.\nRecall that the rank of a matrix \\(A\\) is the largest number of linearly independent columns of \\(A\\).\nOr, equivalently, the dimension of \\(\\operatorname{Col} A\\).\n\nLet’s define the rank-\\(k\\) approximation to \\(A\\):\nWhen \\(k &lt; \\operatorname{Rank}A\\), the rank-\\(k\\) approximation to \\(A\\) is the closest rank-\\(k\\) matrix to \\(A\\), i.e.,\n\\[A^{(k)} =\\arg \\min_{\\operatorname{Rank}B = k} \\Vert A-B\\Vert_F.\\]\n\nWhy is a rank-\\(k\\) approximation valuable?\n\nThe reason is that a rank-\\(k\\) matrix may take up much less space than the original \\(A\\).\n\\(\\small {\\normalsize m}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{cccc}\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_1}\\\\\\vdots\\\\\\vdots\\end{array}&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_2}\\\\\\vdots\\\\\\vdots\\end{array}&\\dots&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_n}\\\\\\vdots\\\\\\vdots\\end{array}\\\\\\end{array}\\right]}^{\\normalsize n} =\n\\overbrace{\\left[\\begin{array}{cc}\\vdots&\\vdots\\\\\\vdots&\\vdots\\\\\\sigma_1\\mathbf{u}_1&\\sigma_k\\mathbf{u}_k\\\\\\vdots&\\vdots\\\\\\vdots&\\vdots\\end{array}\\right]}^{\\normalsize k}\n\\times\n\\overbrace{\\left[\\begin{array}{ccc}\\dots&\\mathbf{v}_1&\\dots\\\\\\dots&\\mathbf{v}_k&\\dots\\end{array}\\right]}^{\\normalsize n}\\)\nNotice that:\n\n\\(A\\) takes space \\(mn\\)\nbut the rank-\\(k\\) approximation only takes space \\((m+n)k\\).\n\n\n\nFor example, if A is \\(1000 \\times 1000\\), and \\(k=10\\), then\n\nA takes space \\(1000000\\)\nwhile the rank-\\(k\\) approximation takes space \\(20000\\),\n\nSo the rank-\\(k\\) approximation is just 2% the size of \\(A\\).\n\n\n\nThe fact that the SVD finds the best rank-\\(k\\) approximation to any matrix is called the Eckart-Young-Mirsky Theorem. You can find a proof of the theorem here. In fact it is true for the Frobenius norm (the norm we are using here) as well as another matrix norm, the spectral norm.\nMore good resources on how to understand SVD as a data approximation method are here and here.\nThe key to using the SVD for matrix approximation is as follows:\n\nThe best rank-k approximation to any matrix can be found via the SVD.\n\n\nIn fact, for an \\(m\\times n\\) matrix \\(A\\), the SVD does two things:\n  1. It gives the best rank-\\(k\\) approximation to \\(A\\) for every \\(k\\) up to the rank of \\(A\\).\n  2. It gives the distance of the best rank-\\(k\\) approximation \\(A^{(k)}\\) from \\(A\\) for each \\(k\\).\n\n\nWhen we say “best”, we mean in terms of Frobenius norm \\(\\Vert A-A^{(k)}\\Vert_F\\),\nand by distance we mean the same quantity, \\(\\Vert A-A^{(k)}\\Vert_F\\).\n\nHow do we use SVD to find the best rank-\\(k\\) approximation to \\(A\\)?\n\nConceptually, we “throw away” the portions of the SVD having the smallest singular values.\n\n\nMore specifically: in terms of the singular value decomposition,\n\\[ A = U\\Sigma V^T, \\]\nthe best rank-\\(k\\) approximation to \\(A\\) is formed by taking\n\n\\(U' =\\) the \\(k\\) leftmost columns of \\(U\\),\n\\(\\Sigma ' =\\) the \\(k\\times k\\) upper left submatrix of \\(\\Sigma\\), and\n\\((V')^T=\\) the \\(k\\) upper rows of \\(V^T\\),\n\nand constructing\n\\[A^{(k)} = U'\\Sigma'(V')^T.\\]\n\n\nThe distance (in Frobenius norm) of the best rank-\\(k\\) approximation \\(A^{(k)}\\) from \\(A\\) is equal to\n\\[\\sqrt{\\sum_{i=k+1}^r\\sigma^2_i}.\\]\n\n\nNotice that this quantity is summing over the singular values beyond \\(k\\).\n\n\nWhat this means is that if, beyond some \\(k\\), all of the singular values are small, then \\(A\\) can be closely approximated by a rank-\\(k\\) matrix.",
    "crumbs": [
      "Applications of the SVD"
    ]
  },
  {
    "objectID": "L26ApplicationsOfSVD.html#signal-compression",
    "href": "L26ApplicationsOfSVD.html#signal-compression",
    "title": "Geometric Algorithms",
    "section": "Signal Compression",
    "text": "Signal Compression\n\nWhen working with measurement data, ie measurements of real-world objects, we find that data is often approximately low-rank.\nIn other words, a matrix of measurements can often be well approximated by a low-rank matrix.\n\n\nClassic examples include\n\nmeasurements of human abilities - eg, psychology\nmeasurements of human preferences – eg, movie ratings, social networks\nimages, movies, sound recordings\ngenomics, biological data\nmedical records\ntext documents\n\n\nFor example, here is a photo.\nWe can think of this as a \\(512\\times 512\\) matrix \\(A\\) whose entries are grayscale values (numbers between 0 and 1).\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s look at the singular values of this matrix.\nWe compute \\(A = U\\Sigma V^T\\) and look at the values on the diagonal of \\(\\Sigma\\).\nThis is often called the matrix’s “spectrum.”\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is this telling us?\nMost of the singular values of \\(A\\) are quite small.\nOnly the first few singular values are large – up to, say, \\(k\\) = 40.\n\n\nRemember that the error we get when we use a rank-\\(k\\) approximation is\n\\[\\Vert A-A^{(k)}\\Vert_F = \\sqrt{\\sum_{i=k+1}^r\\sigma^2_i}.\\]\nSo we can use the singular values of \\(A\\) to compute the relative error over a range of possible approximations \\(A^{(k)}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis matrix \\(A\\) has rank of 512.\nBut the error when we approximate \\(A\\) by a rank 40 matrix is only around 10%.\nWe say that the effective rank of \\(A\\) is low (perhaps 40).\n\n\nLet’s find the closest rank-40 matrix to \\(A\\) and view it.\nWe can do this quite easily using the SVD.\nWe simply construct our approximation of \\(A\\) using only the first 40 columns of \\(U\\) and top 40 rows of \\(V^T\\).\n\n\n\\[U\\Sigma V^T = A\\]\n\\[A^{(k)} = U'\\Sigma'(V')^T.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the rank-40 boat takes up only 40/512 = 8% of the space of the original image!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis general principle is what makes image, video, and sound compression effective.\nWhen you\n\nwatch HDTV, or\nlisten to an MP3, or\nlook at a JPEG image,\n\nthese signals have been compressed using the fact that they are effectively low-rank matrices.\n\nAs you can see from the example of the boat image, it is often possible to compress such signals enormously, leading to an immense savings of storage space and transmission bandwidth.\nIn fact the entire premise of the show “Silicon Valley” is based on this fact :)",
    "crumbs": [
      "Applications of the SVD"
    ]
  },
  {
    "objectID": "L26ApplicationsOfSVD.html#dimensionality-reduction",
    "href": "L26ApplicationsOfSVD.html#dimensionality-reduction",
    "title": "Geometric Algorithms",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nAnother way to think about what we just did is “dimensionality reduction”.\n\n\nConsider this common situation:\n\\({\\text{m objects}}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{ccccc}\n\\begin{array}{c}a_{11}\\\\\\vdots\\\\a_{i1}\\\\\\vdots\\\\a_{m1}\\end{array}&\n\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n\\begin{array}{c}a_{1j}\\\\\\vdots\\\\a_{ij}\\\\\\vdots\\\\a_{mj}\\end{array}&\n\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n\\begin{array}{c}a_{1n}\\\\\\vdots\\\\a_{in}\\\\\\vdots\\\\a_{mn}\\end{array}\n\\end{array}\\right]}^{\\text{n features}} =\n\\overbrace{\\left[\\begin{array}{ccc}\\vdots&&\\vdots\\\\\\vdots&&\\vdots\\\\\\mathbf{u}_1&\\cdots&\\mathbf{u}_k\\\\\\vdots&&\\vdots\\\\\\vdots&&\\vdots\\end{array}\\right]}^{\\large k}\n\\times\n\\left[\\begin{array}{ccc}\\sigma_1& &\\\\&\\ddots&\\\\&&\\sigma_k\\end{array}\\right]\n\\times\n\\left[\\begin{array}{ccccc}\\dots&\\dots&\\mathbf{v}_1&\\dots&\\dots\\\\&&\\vdots&&\\\\\\dots&\\dots&\\mathbf{v}_k&\\dots&\\dots\\end{array}\\right]\\)\n\n\nThe \\(U\\) matrix has a row for each data object.\nNotice that the original data objects had \\(n\\) features, but each row of \\(U\\) only has \\(k\\) entries.\n\n\nDespite that, a row of \\(U\\) can still provide most of the information in the corresponding row of \\(A\\)\n(To see that, note that we can approximately recover the original row by simply multiplying the row of \\(U\\) by \\(\\Sigma V^T\\)).\nSo we have reduced the dimension of our data objects – from \\(n\\) down to \\(k\\) – without losing much of the information they contain.",
    "crumbs": [
      "Applications of the SVD"
    ]
  },
  {
    "objectID": "L26ApplicationsOfSVD.html#principal-component-analysis",
    "href": "L26ApplicationsOfSVD.html#principal-component-analysis",
    "title": "Geometric Algorithms",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nThis kind of dimensionality reduction can be done in an optimal way.\nThe method for doing it is called Principal Component Analysis (or PCA).\n\n\nWhat does optimal mean in this context?\nHere we use a statistical criterion: a dimensionality reduction that captures the maximum variance in the data.\n\n\nHere is a simple example.\nConsider the points below, which live in \\(\\mathbb{R}^2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, although the points are in \\(\\mathbb{R}^2\\), they seem to show effective low-rank.\nThat is, it might not be a bad approximation to replace each point by a point in a 1-D dimensional space, that is, along a line.\n\n\nWhat line should we choose? We will choose the line such that the sum of the distances of the points to the line is minimized.\nThe points, projected on this line, will capture the maximum variance in the data (because the remaining errors are minimized).\n\n\nWhat would happen if we used SVD at this point, and kept only rank-1 approximation to the data?\nThis would be the 1-D subspace that approximates the data best in Frobenius norm.\n\n\nHowever the variance in the data is defined with respect to the data mean, so we need to mean-center the data first, before using SVD.\nThat is, without mean centering, SVD finds the best 1-D subspace, not the best line though the data (which might not pass through the origin).\n\n\nSo to capture the best line through the data, we first move the data points to the origin:\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we use SVD to construct the best 1-D approximation of the mean-centered data:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis method is called Principal Component Analysis.\nIn summary, PCA consists of:\n  1. Mean center the data, and\n  2. Reduce the dimension of the mean-centered data via SVD.\n\n\nIt winds up constructing the best low dimensional approximation of the data in terms of variance.\n\n\nThis is equivalent to projecting the data onto the subspace that captures the maximum variance in the data.\nThat is, each point is replaced by a point in \\(k\\) dimensional space such that the total error (distances between points and their replacements) is minimized.",
    "crumbs": [
      "Applications of the SVD"
    ]
  },
  {
    "objectID": "L26ApplicationsOfSVD.html#visualization-using-pca",
    "href": "L26ApplicationsOfSVD.html#visualization-using-pca",
    "title": "Geometric Algorithms",
    "section": "Visualization using PCA",
    "text": "Visualization using PCA\n\nI’ll now show an extended example to give you a sense of the power of PCA.\n\n\nLet’s analyze some really high-dimensional data: documents.\nA common way to represent documents is using the bag-of-words model.\nIn this matrix, rows are documents, columns are words, and entries count how many time a word appears in a document.\nThis is called a document-term matrix.\n\n\n\\[{\\text{$m$ documents}}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{ccccc}\n\\begin{array}{c}a_{11}\\\\\\vdots\\\\a_{i1}\\\\\\vdots\\\\a_{m1}\\end{array}&\n\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n\\begin{array}{c}a_{1j}\\\\\\vdots\\\\a_{ij}\\\\\\vdots\\\\a_{mj}\\end{array}&\n\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n\\begin{array}{c}a_{1n}\\\\\\vdots\\\\a_{in}\\\\\\vdots\\\\a_{mn}\\end{array}\n\\end{array}\\right]}^{\\text{$n$ terms}}\\]\n\n\nWe are touching on a broad topic, called Latent Semantic Analysis, which is essentially the application of linear algebra to document analysis.\nYou can learn about Latent Semantic Analysis in other courses in data science or natural language processing.\n\nOur text documents are going to be posts from certain discussion forums called “newsgroups”.\nWe will collect posts from three groups: comp.os.ms-windows.misc, sci.space, and rec.sport.baseball.\nI am going to skip over some details. However, all the code is here, so you can explore it on your own if you like.\n\n\nfrom sklearn.datasets import fetch_20newsgroups\ncategories = ['comp.os.ms-windows.misc', 'sci.space', 'rec.sport.baseball']\nnews_data = fetch_20newsgroups(subset='train', categories=categories)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words='english', min_df=4, max_df=0.8)\ndtm = vectorizer.fit_transform(news_data.data).todense()\n\n\n\n\nprint('The size of our document-term matrix is {}'.format(dtm.shape))\n\nThe size of our document-term matrix is (1781, 9409)\n\n\n\n\nSo we have 1781 documents, and there are 9409 different words that are contained in the documents.\nWe can think of each document as a vector in 9409-dimensional space.\n\n\nLet us apply PCA to the document-term matrix.\n\n\nFirst, we mean center the data.\n\ncentered_dtm = dtm - np.mean(dtm, axis=0)\n\n\n\nNow we compute the SVD of the mean-centered data:\n\nu, s, vt = np.linalg.svd(centered_dtm)\n\n\n\nNow, we use PCA to visualize the set of documents.\nOur visualization will be in two dimensions.\nThis is pretty extreme …\n– we are taking points in 9409-dimensional space and projecting them into a subspace of only two dimensions!\n\n\n\n\nXk = u @ np.diag(s)\nfig, ax = plt.subplots(1,1,figsize=(7,6))\nplt.scatter(np.ravel(Xk[:,0]), np.ravel(Xk[:,1]), \n                    s=20, alpha=0.5, marker='D')\nplt.title('2D Visualization of Documents via PCA', size=20);\n\n\n\n\n\n\n\n\n\n\n\nThis visualization shows that our collection of documents has considerable internal structure.\nIn particular, based on word frequency, it appears that there are three general groups of documents.\n\n\nAs you might guess, this is because the discussion topics of the document sets are different:",
    "crumbs": [
      "Applications of the SVD"
    ]
  },
  {
    "objectID": "L26ApplicationsOfSVD.html#section-14",
    "href": "L26ApplicationsOfSVD.html#section-14",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "In summary:\n\nData often arrives in high dimension\n\nfor example, the documents above are points in \\(\\mathbb{R}^{9049}\\)\n\nHowever, the structure in data can be relatively low-dimensional\n\nin our example, we can see structure in just two dimensions!\n\nPCA allows us to find the low-dimensional structure in data\n\nin a way that is optimal in some sense",
    "crumbs": [
      "Applications of the SVD"
    ]
  },
  {
    "objectID": "L23LinearModels.html",
    "href": "L23LinearModels.html",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Sir Francis Galton by Charles Wellington Furse by Charles Wellington Furse (died 1904) - National Portrait Gallery: NPG 3916\n\nMany parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\n\n\n\n\n \n\n\n\nIn 1886 Francis Galton published his observations about how random factors affect outliers.\nThis notion has come to be called “regression to the mean” because unusually large or small phenomena, after the influence of random events, become closer to their mean values (less extreme).\n\nOne of the most fundamental kinds of machine learning is the construction of a model that can be used to summarize a set of data.\nA model is a concise description of a dataset or a real-world phenomenon.\nFor example, an equation can be a model if we use the equation to describe something in the real world.\n\nThe most common form of modeling is regression, which means constructing an equation that describes the relationships among variables.",
    "crumbs": [
      "Linear Models"
    ]
  },
  {
    "objectID": "L23LinearModels.html#linear-models",
    "href": "L23LinearModels.html#linear-models",
    "title": "Geometric Algorithms",
    "section": "",
    "text": "Sir Francis Galton by Charles Wellington Furse by Charles Wellington Furse (died 1904) - National Portrait Gallery: NPG 3916\n\nMany parts of this page are based on Linear Algebra and its Applications, by David C. Lay\n\n\n\n\n\n \n\n\n\nIn 1886 Francis Galton published his observations about how random factors affect outliers.\nThis notion has come to be called “regression to the mean” because unusually large or small phenomena, after the influence of random events, become closer to their mean values (less extreme).\n\nOne of the most fundamental kinds of machine learning is the construction of a model that can be used to summarize a set of data.\nA model is a concise description of a dataset or a real-world phenomenon.\nFor example, an equation can be a model if we use the equation to describe something in the real world.\n\nThe most common form of modeling is regression, which means constructing an equation that describes the relationships among variables.",
    "crumbs": [
      "Linear Models"
    ]
  },
  {
    "objectID": "L23LinearModels.html#regression-problems",
    "href": "L23LinearModels.html#regression-problems",
    "title": "Geometric Algorithms",
    "section": "Regression Problems",
    "text": "Regression Problems\n\nFor example, we may look at these points and observe that they approximately lie on a line.\nSo we could decide to model this data using a line.\n\n\n\n\n\n\n\n\n\n\n\n\nWe may look at these points and decide to model them using a quadratic function.\n\n\n\n\n\n\n\n\n\n\n\nAnd we may look at these points and decide to model them using a logarithmic function.\n\n\n\n\n\n\n\n\n\n\n\nClearly, none of these datasets agrees perfectly with the proposed model. So the question arises:\nHow do we find the best linear function (or quadratic function, or logarithmic function) given the data?",
    "crumbs": [
      "Linear Models"
    ]
  },
  {
    "objectID": "L23LinearModels.html#the-framework-of-linear-models",
    "href": "L23LinearModels.html#the-framework-of-linear-models",
    "title": "Geometric Algorithms",
    "section": "The Framework of Linear Models",
    "text": "The Framework of Linear Models\n\nThe regression problem has been studied extensively in the field of statistics and machine learning.\nCertain terminology is used:\n\nSome values are referred to as “independent,” and\nSome values are referred to as “dependent.”\n\n\n\nThe basic regression task is:\n\ngiven a set of independent variables\nand the associated dependent variables,\nestimate the parameters of a model (such as a line, parabola, etc) that describes how the dependent variables are related to the independent variables.\n\n\n\nThe independent variables are collected into a matrix \\(X,\\) which is called the design matrix.\nThe dependent variables are collected into an observation vector \\(\\mathbf{y}.\\)\nThe parameters of the model (for any kind of model) are collected into a parameter vector \\(\\beta.\\)",
    "crumbs": [
      "Linear Models"
    ]
  },
  {
    "objectID": "L23LinearModels.html#fitting-a-line-to-data",
    "href": "L23LinearModels.html#fitting-a-line-to-data",
    "title": "Geometric Algorithms",
    "section": "Fitting a Line to Data",
    "text": "Fitting a Line to Data\n\nThe first kind of model we’ll study is a linear equation:\n\\[y = \\beta_0 + \\beta_1 x.\\]\nThis is the most commonly used type of model, particularly in fields like economics, psychology, biology, etc.\nThe reason it is so commonly used is that, like Galton’s data, experimental data often produce points \\((x_1, y_1), \\dots, (x_n,y_n)\\) that seem to lie close to a line.\n\n\nThe question we must confront is: given a set of data, how should we “fit” the equation of the line to the data?\nOur intuition is this: we want to determine the parameters \\(\\beta_0, \\beta_1\\) that define a line that is as “close” to the points as possible.\n\nLet’s develop some terminology for evaluating a model.\nSuppose we have a line \\(y = \\beta_0 + \\beta_1 x\\). For each data point \\((x_j, y_j),\\) there is a point \\((x_j, \\beta_0 + \\beta_1 x_j)\\) that is the point on the line with the same \\(x\\)-coordinate.\n\n\nImage credit: Linear Algebra and its Applications, by David C. Lay, 4th ed.\n\n\n\n\n\n\nWe call \\(y_j\\) the observed value of \\(y\\)\nand we call \\(\\beta_0 + \\beta_1 x_j\\) the predicted \\(y\\)-value.\nThe difference between an observed \\(y\\)-value and a predicted \\(y\\)-value is called a residual.\n\n\nThere are several ways to measure how “close” the line is to the data.\nThe usual choice is to sum the squares of the residuals.\n(Note that the residuals themselves may be positive or negative – by squaring them, we ensure that our error measures don’t cancel out.)\n\n\nThe least-squares line is the line \\(y = \\beta_0 + \\beta_1x\\) that minimizes the sum of squares of the residuals.\nThe coefficients \\(\\beta_0, \\beta_1\\) of the line are called regression coefficients.",
    "crumbs": [
      "Linear Models"
    ]
  },
  {
    "objectID": "L23LinearModels.html#a-least-squares-problem",
    "href": "L23LinearModels.html#a-least-squares-problem",
    "title": "Geometric Algorithms",
    "section": "A Least-Squares Problem",
    "text": "A Least-Squares Problem\n\nLet’s imagine for a moment that the data fit a line perfectly.\nThen, if each of the data points happened to fall exactly on the line, the parameters \\(\\beta_0\\) and \\(\\beta_1\\) would satisfy the equations\n\\[\\beta_0 + \\beta_1 x_1 = y_1 \\] \\[\\beta_0 + \\beta_1 x_2 = y_2 \\] \\[\\beta_0 + \\beta_1 x_3 = y_3 \\] \\[ \\vdots\\] \\[\\beta_0 + \\beta_1 x_n = y_n \\]\n\n\nWe can write this system as\n\\[X\\mathbf{\\beta} = \\mathbf{y}\\]\nwhere\n\\[X=\\begin{bmatrix}1&x_1\\\\1&x_2\\\\\\vdots&\\vdots\\\\1&x_n\\end{bmatrix},\\;\\;\\mathbf{\\beta} = \\begin{bmatrix}\\beta_0\\\\\\beta_1\\end{bmatrix},\\;\\;\\mathbf{y}=\\begin{bmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_n\\end{bmatrix}\\]\n\n\nOf course, if the data points don’t actually lie exactly on a line,\n… then there are no parameters \\(\\beta_0, \\beta_1\\) for which the predicted \\(y\\)-values in \\(X\\mathbf{\\beta}\\) equal the observed \\(y\\)-values in \\(\\mathbf{y}\\),\n… and \\(X\\mathbf{\\beta}=\\mathbf{y}\\) has no solution.\n\nNow, since the data doesn’t fall exactly on a line, we have decided to seek the \\(\\beta\\) that minimizes the sum of squared residuals, ie,\n\\[\\sum_i (\\beta_0 + \\beta_1 x_i - y_i)^2\\]\n\n\\[=\\Vert X\\beta -\\mathbf{y}\\Vert^2\\]\n\n\nThis is key: the sum of squares of the residuals is exactly the square of the distance between the vectors \\(X\\mathbf{\\beta}\\) and \\(\\mathbf{y}.\\)\n\n\nThis is a least-squares problem, \\(A\\mathbf{x} = \\mathbf{b},\\) with different notation.\n\n\nComputing the least-squares solution of \\(X\\mathbf{\\beta} = \\mathbf{y}\\) is equivalent to finding the \\(\\mathbf{\\beta}\\) that determines the least-squares line.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 1. Find the equation \\(y = \\beta_0 + \\beta_1 x\\) of the least-squares line that best fits the data points\n\n\n\nx\ny\n\n\n\n\n2\n1\n\n\n5\n2\n\n\n7\n3\n\n\n8\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution. Use the \\(x\\)-coordinates of the data to build the design matrix \\(X\\), and the \\(y\\)-coordinates to build the observation vector \\(\\mathbf{y}\\):\n\\[X = \\begin{bmatrix}1&2\\\\1&5\\\\1&7\\\\1&8\\end{bmatrix},\\;\\;\\;\\mathbf{y}=\\begin{bmatrix}1\\\\2\\\\3\\\\3\\end{bmatrix}\\]\n\n\nNow, to obtain the least-squares line, find the least-squares solution to \\(X\\mathbf{\\beta} = \\mathbf{y}.\\)\n\n\nWe do this via the method we learned last lecture (just with new notation):\n\\[X^TX\\mathbf{\\beta} = X^T\\mathbf{y}\\]\n\n\nSo, we compute:\n\\[X^TX = \\begin{bmatrix}1&1&1&1\\\\2&5&7&8\\end{bmatrix}\\begin{bmatrix}1&2\\\\1&5\\\\1&7\\\\1&8\\end{bmatrix} = \\begin{bmatrix}4&22\\\\22&142\\end{bmatrix}\\]\n\\[X^T\\mathbf{y} =  \\begin{bmatrix}1&1&1&1\\\\2&5&7&8\\end{bmatrix}\\begin{bmatrix}1\\\\2\\\\3\\\\3\\end{bmatrix} = \\begin{bmatrix}9\\\\57\\end{bmatrix}\\]\n\n\nSo the normal equations are:\n\\[\\begin{bmatrix}4&22\\\\22&142\\end{bmatrix}\\begin{bmatrix}\\beta_0\\\\\\beta_1\\end{bmatrix} = \\begin{bmatrix}9\\\\57\\end{bmatrix}\\]\n\n\nSolving, we get:\n\\[\\begin{bmatrix}\\beta_0\\\\\\beta_1\\end{bmatrix}=\\begin{bmatrix}4&22\\\\22&142\\end{bmatrix}^{-1}\\begin{bmatrix}9\\\\57\\end{bmatrix}\\]\n\\[ = \\frac{1}{84}\\begin{bmatrix}142&-22\\\\-22&4\\end{bmatrix}\\begin{bmatrix}9\\\\57\\end{bmatrix}\\]\n\\[ = \\begin{bmatrix}2/7\\\\5/14\\end{bmatrix}\\]\n\n\nSo the least-squares line has the equation\n\\[y = \\frac{2}{7} + \\frac{5}{14}x.\\]",
    "crumbs": [
      "Linear Models"
    ]
  },
  {
    "objectID": "L23LinearModels.html#the-general-linear-model",
    "href": "L23LinearModels.html#the-general-linear-model",
    "title": "Geometric Algorithms",
    "section": "The General Linear Model",
    "text": "The General Linear Model\n\nAnother way that the inconsistent linear system is often written is to collect all the residuals into a residual vector.\nThen an exact equation is\n\\[\\mathbf{y} = X\\mathbf{\\beta} + {\\mathbf\\epsilon}\\]\n\n\nAny equation of this form is referred to as a linear model.\nIn this formulation, the goal is to minimize the length of \\(\\epsilon\\), ie, \\(\\Vert\\epsilon\\Vert.\\)\n\n\nIn some cases, one would like to fit data points with something other than a straight line.\nFor example, think of Gauss trying to find the equation for the orbit of Ceres.\nIn cases like this, the matrix equation is still \\(X\\mathbf{\\beta} = \\mathbf{y}\\), but the specific form of \\(X\\) changes from one problem to the next.\n\n\nThe least-squares solution \\(\\hat{\\mathbf{\\beta}}\\) is a solution of the normal equations\n\\[X^TX\\mathbf{\\beta} = X^T\\mathbf{y}.\\]",
    "crumbs": [
      "Linear Models"
    ]
  },
  {
    "objectID": "L23LinearModels.html#least-squares-fitting-of-other-models",
    "href": "L23LinearModels.html#least-squares-fitting-of-other-models",
    "title": "Geometric Algorithms",
    "section": "Least-Squares Fitting of Other Models",
    "text": "Least-Squares Fitting of Other Models\n\nMost models have parameters, and the object of model fitting is to to fix those parameters. Let’s talk about model parameters.\n\n\nIn model fitting, the parameters are the unknown. A central question for us is whether the model is linear in its parameters.\n\n\nFor example, the model \\(y = \\beta_0 e^{-\\beta_1 x}\\) is not linear in its parameters.\nThe model \\(y = \\beta_0 e^{-2 x}\\) is linear in its parameters.\n\n\nFor a model that is linear in its parameters, an observation is a linear combination of (arbitrary) known functions.\n\n\nIn other words, a model that is linear in its parameters is\n\\[y = \\beta_0f_0(x) + \\beta_1f_1(x) + \\dots + \\beta_nf_n(x)\\]\nwhere \\(f_0, \\dots, f_n\\) are known functions and \\(\\beta_0,\\dots,\\beta_k\\) are parameters.\n\nExample. Suppose data points \\((x_1, y_1), \\dots, (x_n, y_n)\\) appear to lie along some sort of parabola instead of a straight line. Suppose we wish to approximate the data by an equation of the form\n\\[y = \\beta_0 + \\beta_1x + \\beta_2x^2.\\]\nDescribe the linear model that produces a “least squares fit” of the data by the equation.\n\nSolution. The ideal relationship is \\(y = \\beta_0 + \\beta_1x + \\beta_2x^2.\\)\n\n\nSuppose the actual values of the parameters are \\(\\beta_0, \\beta_1, \\beta_2.\\) Then the coordinates of the first data point satisfy the equation\n\\[y_1 = \\beta_0 + \\beta_1x_1 + \\beta_2x_1^2 + \\epsilon_1\\]\nwhere \\(\\epsilon_1\\) is the residual error between the observed value \\(y_1\\) and the predicted \\(y\\)-value.\n\n\nEach data point determines a similar equation:\n\\[y_1 = \\beta_0 + \\beta_1x_1 + \\beta_2x_1^2 + \\epsilon_1\\] \\[y_2 = \\beta_0 + \\beta_1x_2 + \\beta_2x_2^2 + \\epsilon_2\\] \\[\\vdots\\] \\[y_n = \\beta_0 + \\beta_1x_n + \\beta_2x_n^2 + \\epsilon_n\\]\n\n\nClearly, this system can be written as \\(\\mathbf{y} = X\\mathbf{\\beta} + \\mathbf{\\epsilon}.\\)\n\\[\\begin{bmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_n\\end{bmatrix} = \\begin{bmatrix}1&x_1&x_1^2\\\\1&x_2&x_2^2\\\\\\vdots&\\vdots&\\vdots\\\\1&x_n&x_n^2\\end{bmatrix} \\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\end{bmatrix} + \\begin{bmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\vdots\\\\\\epsilon_n\\end{bmatrix}\\]",
    "crumbs": [
      "Linear Models"
    ]
  },
  {
    "objectID": "L23LinearModels.html#multiple-regression",
    "href": "L23LinearModels.html#multiple-regression",
    "title": "Geometric Algorithms",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\nSuppose an experiment involves two independent variables – say, \\(u\\) and \\(v\\), – and one dependent variable, \\(y\\).\nA linear equation for predicting \\(y\\) from \\(u\\) and \\(v\\) has the form\n\\[y = \\beta_0 + \\beta_1 u + \\beta_2 v\\]\n\n\nSince there is more than one independent variable, this is called multiple regression.\n\n\nA more general prediction equation might have the form\n\\[y = \\beta_0 + \\beta_1 u + \\beta_2 v + \\beta_3u^2 + \\beta_4 uv + \\beta_5 v^2\\]\nA least squares fit to equations like this is called a trend surface.\n\n\nIn general, a linear model will arise whenever \\(y\\) is to be predicted by an equation of the form\n\\[y = \\beta_0f_0(u,v) + \\beta_1f_1(u,v) + \\cdots + \\beta_kf_k(u,v)\\]\nwith \\(f_0,\\dots,f_k\\) any sort of known functions and \\(\\beta_0,...,\\beta_k\\) unknown weights.\n\n\nExample. In geography, local models of terrain are constructed from data \\((u_1, v_1, y_1), \\dots, (u_n, v_n, y_n)\\) where \\(u_j, v_j\\), and \\(y_j\\) are latitude, longitude, and altitude, respectively.\n\n\nLet’s take an example. Here are a set of points in \\(\\mathbb{R}^3\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s describe the linear models that gives a least-squares fit to such data. The solution is called the least-squares plane.\n\n\nSolution. We expect the data to satisfy these equations:\n\\[y_1 = \\beta_0 + \\beta_1 u_1 + \\beta_2 v_1 + \\epsilon_1\\] \\[y_1 = \\beta_0 + \\beta_1 u_2 + \\beta_2 v_2 + \\epsilon_2\\] \\[\\vdots\\] \\[y_1 = \\beta_0 + \\beta_1 u_n + \\beta_2 v_n + \\epsilon_n\\]\n\n\nThis system has the matrix for \\(\\mathbf{y} = X\\mathbf{\\beta} + \\epsilon,\\) where\n\\[\\mathbf{y} = \\begin{bmatrix}y_1\\\\y_1\\\\\\vdots\\\\y_n\\end{bmatrix},\\;\\;X = \\begin{bmatrix}1&u_1&v_1\\\\1&u_2&v_2\\\\\\vdots&\\vdots&\\vdots\\\\1&u_n&v_n\\end{bmatrix},\\;\\;\\mathbf{\\beta}=\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\end{bmatrix},\\;\\;\\epsilon = \\begin{bmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\vdots\\\\\\epsilon_n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis example shows that the linear model for multiple regression has the same abstract form as the model for the simple regression in the earlier examples.\n\n\nWe can see that there the general principle is the same across all the different kinds of linear models.\n\n\nOnce \\(X\\) is defined properly, the normal equations for \\(\\mathbf{\\beta}\\) have the same matrix form, no matter how many variables are involved.\nThus, for any linear model where \\(X^TX\\) is invertible, the least squares \\(\\hat{\\mathbf{\\beta}}\\) is given by \\((X^TX)^{-1}X^T\\mathbf{y}\\).",
    "crumbs": [
      "Linear Models"
    ]
  },
  {
    "objectID": "L23LinearModels.html#multiple-regression-in-practice",
    "href": "L23LinearModels.html#multiple-regression-in-practice",
    "title": "Geometric Algorithms",
    "section": "Multiple Regression in Practice",
    "text": "Multiple Regression in Practice\n\nLet’s see how powerful multiple regression can be on a real-world example.\nA typical application of linear models is predicting house prices. Linear models have been used for this problem for decades, and when a municipality does a value assessment on your house, they typically use a linear model.\n\n\nWe can consider various measurable attributes of a house (its “features”) as the independent variables, and the most recent sale price of the house as the dependent variable.\n\n\nFor our case study, we will use the features:\n\nLot Area (sq ft),\nGross Living Area (sq ft),\nNumber of Fireplaces,\nNumber of Full Baths,\nNumber of Half Baths,\nGarage Area (sq ft),\nBasement Area (sq ft)\n\n\n\nSo our design matrix will have 8 columns (including the constant for the intercept):\n\\[ X\\beta = \\mathbf{y}\\]\nand it will have one row for each house in the data set, with \\(y\\) the sale price of the house.\n\n\nWe will use data from housing sales in Ames, Iowa from 2006 to 2009:\n\n\n\ndf = pd.read_csv('data/ames-housing-data/train.csv')\n\n\n\n\ndf[['LotArea', 'GrLivArea', 'Fireplaces', 'FullBath', 'HalfBath', 'GarageArea', 'TotalBsmtSF', 'SalePrice']].head()\n\n\n\n\n\n\n\n\nLotArea\nGrLivArea\nFireplaces\nFullBath\nHalfBath\nGarageArea\nTotalBsmtSF\nSalePrice\n\n\n\n\n0\n8450\n1710\n0\n2\n1\n548\n856\n208500\n\n\n1\n9600\n1262\n1\n2\n0\n460\n1262\n181500\n\n\n2\n11250\n1786\n1\n2\n1\n608\n920\n223500\n\n\n3\n9550\n1717\n1\n1\n0\n642\n756\n140000\n\n\n4\n14260\n2198\n1\n2\n1\n836\n1145\n250000\n\n\n\n\n\n\n\n\n\n\nX_no_intercept = df[['LotArea', 'GrLivArea', 'Fireplaces', 'FullBath', 'HalfBath', 'GarageArea', 'TotalBsmtSF']].values\ny = df['SalePrice'].values\n\n\n\nNext we add a column of 1s to the design matrix, which adds a constant intercept to the model:\n\n\n\nX = np.column_stack([np.ones(X_no_intercept.shape[0], dtype = 'int'), X_no_intercept])\n\n\n\n\nX\n\narray([[    1,  8450,  1710, ...,     1,   548,   856],\n       [    1,  9600,  1262, ...,     0,   460,  1262],\n       [    1, 11250,  1786, ...,     1,   608,   920],\n       ...,\n       [    1,  9042,  2340, ...,     0,   252,  1152],\n       [    1,  9717,  1078, ...,     0,   240,  1078],\n       [    1,  9937,  1256, ...,     1,   276,  1256]])\n\n\n\n\nNow let’s peform the least-squares regression:\n\\[\\hat{\\beta} = (X^TX)^{-1}X^T \\mathbf{y}\\]\n\n\n\nbeta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n\n\n\nWhat does our model tell us?\n\n\n\nbeta_hat\n\narray([-2.92338280e+04,  1.87444579e-01,  3.94185205e+01,  1.45698657e+04,\n        2.29695596e+04,  1.62834807e+04,  9.14770980e+01,  5.11282216e+01])\n\n\n\n\nWe see that we have:\n\n\\(\\beta_0\\): Intercept of -$29,233\n\\(\\beta_1\\): Marginal value of one square foot of Lot Area: $18\n\\(\\beta_2\\): Marginal value of one square foot of Gross Living Area: $39\n\\(\\beta_3\\): Marginal value of one additional fireplace: $14,570\n\\(\\beta_4\\): Marginal value of one additional full bath: $22,970\n\\(\\beta_5\\): Marginal value of one additional half bath: $16,283\n\\(\\beta_6\\): Marginal value of one square foot of Garage Area: $91\n\\(\\beta_7\\): Marginal value of one square foot of Basement Area: $51\n\n\n\nIs our model doing a good job?\nThere are many statistics for testing this question, but we’ll just look at the predictions versus the ground truth.\nFor each house we compute its predicted sale value according to our model:\n\\[\\hat{\\mathbf{y}} = X\\hat{\\beta}\\]\n\n\n\ny_hat = X @ beta_hat\n\n\n\nAnd for each house, we’ll plot its predicted versus actual sale value:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that the model does a reasonable job for house values less than about $250,000.\nFor a better model, we’d want to consider more features of each house, and perhaps some additional functions such as polynomials as components of our model.",
    "crumbs": [
      "Linear Models"
    ]
  }
]